{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "import networkx as nx; \n",
    "from networkx.readwrite import json_graph;\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "nlp = spacy.load('en_core_web_sm') #you can use other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/headlines_cl_sent.csv')\n",
    "freq = pd.read_csv('../data/processed/words_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47554, 19)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset = ['clean_hl_words'], inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['bloody', 'phone', 'thief', 'm', 'move']\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_hl_words'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pregnant woman, baby survive shooting at Philadelphia homeNew York Post"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(df[\"headline\"][150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['waarheid', 'moet', 'uit', 'oor', 'diana', 'onderhoud', 's', 'william']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(df['clean_hl_words'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloody\n",
      "phone\n",
      "thief\n",
      "m\n",
      "move\n"
     ]
    }
   ],
   "source": [
    "for idx in nlp(' '.join(word for word in ast.literal_eval(df['clean_hl_words'][100]))):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['virginia', 'find', 'dead', 'disappear', 'night', 'friend']\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_hl_words'][101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150                  [pregnant, survive]\n",
       "151               [reveal, huge, famous]\n",
       "152                          [go, viral]\n",
       "153                          [underwear]\n",
       "154                              [worth]\n",
       "155                           [say, try]\n",
       "156                               [play]\n",
       "157                                [say]\n",
       "158      [grohl, invite, old, co, write]\n",
       "159                   [allegedly, shoot]\n",
       "160                                   []\n",
       "161                               [link]\n",
       "162             [reportedly, heal, find]\n",
       "163                                   []\n",
       "164                         [dead, c-19]\n",
       "165        [teen, bust, fatal, stabbing]\n",
       "166                                   []\n",
       "167                                   []\n",
       "168             [allegedly, beat, ex, -]\n",
       "169                                   []\n",
       "170                    [give, agonizing]\n",
       "171                   [order, hear, new]\n",
       "172                                   []\n",
       "173                   [pregnant, unborn]\n",
       "174                             [reveal]\n",
       "175                                [get]\n",
       "176                    [learn, pregnant]\n",
       "177    [remain, suggest, ancient, human]\n",
       "178                                   []\n",
       "179                     [admit, fatally]\n",
       "180                          [soon, hit]\n",
       "181                                   []\n",
       "182                           [fugitive]\n",
       "183                        [charge, say]\n",
       "184                     [beat, cheating]\n",
       "185                   [say, hairdresser]\n",
       "186                          [brazilian]\n",
       "187                                   []\n",
       "188                          [depressed]\n",
       "189                                   []\n",
       "190                                   []\n",
       "191                      [incoming, say]\n",
       "192                          [notorious]\n",
       "193                                   []\n",
       "194                        [watch, save]\n",
       "195                               [hurt]\n",
       "196                                   []\n",
       "197                        [spend, lose]\n",
       "198                                [fat]\n",
       "199                          [look, new]\n",
       "Name: clean_hl_words, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# excluded_tags = {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"ADP\", \"PROPN\"}\n",
    "included_tags = {\"VERB\", \"ADJ\", \"ADV\"}\n",
    "# df[\"clean_h1_verbs_adjs\"] = \n",
    "# df['clean_hl_words'].apply(lambda x: [idx for idx in ast.literal_eval(x)])[0]\n",
    "df['clean_hl_words'][150:200].apply(lambda x: [idx for idx in nlp(' '.join(word for word in ast.literal_eval(x))) if idx.pos_ in included_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "weight              weight              weight              \n",
      "baby                babi                baby                \n",
      "beauty              beauti              beauty              \n",
      "polite              polit               polit               \n",
      "gentle              gentl               gentl               \n",
      "new born            new born            new born            \n",
      "cooking             cook                cook                \n",
      "sympathy            sympathi            sympathy            \n",
      "beautiful           beauti              beauty              \n",
      "lose                lose                los                 \n",
      "share               share               shar                \n",
      "empathetic          empathet            empathet            \n",
      "model               model               model               \n",
      "caress              caress              caress              \n",
      "communal            commun              commun              \n",
      "whin                whin                whin                \n",
      "bossy               bossi               bossy               \n",
      "dance               danc                dant                \n",
      "care                care                car                 \n",
      "empath              empath              empa                \n",
      "interdependent      interdepend         interdepend         \n",
      "depend              depend              depend              \n",
      "hair                hair                hair                \n",
      "understand          understand          understand          \n",
      "kissing             kiss                kiss                \n",
      "yield               yield               yield               \n",
      "fail                fail                fail                \n",
      "naked               nake                nak                 \n",
      "angel               angel               angel               \n",
      "together            togeth              togeth              \n",
      "kiss                kiss                kiss                \n",
      "cook                cook                cook                \n",
      "cheer               cheer               che                 \n",
      "marry               marri               marry               \n",
      "quiet               quiet               quiet               \n",
      "seductive           seduct              seduc               \n",
      "modesty             modesti             modesty             \n",
      "nurse               nurs                nurs                \n",
      "single              singl               singl               \n",
      "seduce              seduc               seduc               \n",
      "support             support             support             \n",
      "domestic            domest              domest              \n",
      "sexually            sexual              sex                 \n",
      "commitment          commit              commit              \n",
      "birth               birth               bir                 \n",
      "emotions            emot                emot                \n",
      "trust               trust               trust               \n",
      "sensitive           sensit              sensit              \n",
      "feminine            feminin             feminin             \n",
      "body                bodi                body                \n",
      "nurtur              nurtur              nurt                \n",
      "marriage            marriag             marry               \n",
      "frail               frail               frail               \n",
      "submissive          submiss             submit              \n",
      "kid                 kid                 kid                 \n",
      "dress               dress               dress               \n",
      "lover               lover               lov                 \n",
      "whore               whore               whor                \n",
      "divorce             divorc              divorc              \n",
      "sharin              sharin              sharin              \n",
      "inter-dependence    inter-depend        inter-dependence    \n",
      "breast              breast              breast              \n",
      "emotiona            emotiona            emotion             \n",
      "adorable            ador                ad                  \n",
      "affection           affect              affect              \n",
      "pretty              pretti              pretty              \n",
      "pleasant            pleasant            pleas               \n",
      "nag                 nag                 nag                 \n",
      "fear                fear                fear                \n",
      "prostitute          prostitut           prostitut           \n",
      "makeup              makeup              makeup              \n",
      "love                love                lov                 \n",
      "affair              affair              affair              \n",
      "crying              cri                 cry                 \n",
      "shopping            shop                shop                \n",
      "sexual              sexual              sex                 \n",
      "teenage             teenag              teen                \n",
      "child               child               child               \n",
      "respon              respon              respon              \n",
      "married             marri               marry               \n",
      "teen                teen                teen                \n",
      "powerless           powerless           powerless           \n",
      "inter-dependent     inter-depend        inter-dependent     \n",
      "accuse              accus               accus               \n",
      "couple              coupl               coupl               \n",
      "weak                weak                weak                \n",
      "inclusive           inclus              includ              \n",
      "new-born            new-born            new-born            \n",
      "sympath             sympath             sympa               \n",
      "kinship             kinship             kin                 \n",
      "flatterable         flatter             flat                \n",
      "bitch               bitch               bitch               \n",
      "inter-dependen      inter-dependen      inter-dependen      \n",
      "sale                sale                sal                 \n",
      "warm                warm                warm                \n",
      "kind                kind                kind                \n",
      "obedient            obedi               obedy               \n",
      "nurture             nurtur              nurt                \n",
      "soft                soft                soft                \n",
      "virgin              virgin              virgin              \n",
      "cry                 cri                 cry                 \n",
      "slut                slut                slut                \n",
      "nurtures            nurtur              nurt                \n",
      "family              famili              famy                \n",
      "agree               agre                agr                 \n",
      "lie                 lie                 lie                 \n",
      "sex                 sex                 sex                 \n",
      "victim              victim              victim              \n",
      "cries               cri                 cri                 \n",
      "collab              collab              collab              \n",
      "sensitiv            sensitiv            sensit              \n",
      "breasts             breast              breast              \n",
      "affectionate        affection           affect              \n",
      "shop                shop                shop                \n",
      "newborn             newborn             newborn             \n",
      "secretary           secretari           secret              \n",
      "cheat               cheat               che                 \n",
      "maid                maid                maid                \n",
      "good                good                good                \n",
      "committed           commit              commit              \n",
      "considerate         consider            consid              \n",
      "empathize           empath              empath              \n",
      "commit              commit              commit              \n",
      "emotional           emot                emot                \n",
      "cooperate           cooper              coop                \n",
      "question            question            quest               \n",
      "flatter             flatter             flat                \n",
      "tender              tender              tend                \n",
      "co-operate          co-oper             co-operate          \n",
      "bikini              bikini              bikin               \n",
      "interdependence     interdepend         interdepend         \n",
      "afraid              afraid              afraid              \n",
      "feeling             feel                feel                \n",
      "nurturing           nurtur              nurt                \n",
      "date                date                dat                 \n",
      "compassion          compass             compass             \n",
      "interpersonal       interperson         interperson         \n",
      "irrational          irrat               ir                  \n",
      "honest              honest              honest              \n",
      "feel                feel                feel                \n",
      "young               young               young               \n",
      "enthusias           enthusia            enthusia            \n",
      "home                home                hom                 \n",
      "wedding             wed                 wed                 \n",
      "modest              modest              modest              \n",
      "pregnancy           pregnanc            pregn               \n",
      "loyal               loyal               loy                 \n",
      "mistress            mistress            mistress            \n",
      "struggle            struggl             struggle            \n",
      "relationship        relationship        rel                 \n",
      "inter-personal      inter-person        inter-personal      \n",
      "emotion             emot                emot                \n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "for word in female_bias_words:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'obedi'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem(\"obedience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources\n",
    "# http://gender-decoder.katmatfield.com/about\n",
    "# http://gender-decoder.katmatfield.com/static/documents/Gaucher-Friesen-Kay-JPSP-Gendered-Wording-in-Job-ads.pdf\n",
    "# https://github.com/DanielleSucher/Jailbreak-the-Patriarchy\n",
    "\n",
    "# this list of words is used to check for female-gendered language\n",
    "female_words=set(['heroine','spokeswoman','chairwoman',\"women's\",'actress','women',\"she's\",'her','aunt',\n",
    "                  'aunts','bride','daughter','daughters','female','fiancee','girl','girlfriend','girlfriends',\n",
    "                  'girls','granddaughter','grandma','grandmother',\"her\", 'herself','ladies','lady',\n",
    "                  'mom','moms','mother','mothers','mrs','ms','niece','nieces','priestess','princess',\n",
    "                  'she','sister','sisters','waitress','widow','widows','wife','wives','woman', \n",
    "                  'housewife', 'pregnant', 'madam', 'mum', 'mummy', \n",
    "                  'mums', 'mistress', \"miss\", \"queen\"])\n",
    "\n",
    "# this list of words\n",
    "female_bias_words = set(['weight', 'beauty', \"beautiful\", 'frail', 'weak', 'modest', 'virgin', \"slut\", \"whore\", \"prostitute\",\n",
    "                  'sex', 'feminine', 'sensitive', 'emotional', 'gentle', 'soft', \n",
    "                  'pretty', 'bitch', 'sexual', 'shopping','shop', 'cook', 'cooking', 'baby', 'child', 'breast',\n",
    "                  'affectionate', 'affection', 'cheer', 'emotion', 'emotions', 'feel', 'feeling',\n",
    "                  'kind', 'modest', 'bossy', 'family', 'new born', 'newborn', 'new-born', \"irrational\",\n",
    "                 'nurture', 'nurtures', 'nurturing', 'pleasant', 'submissive', 'tender', 'cry', 'cries',\n",
    "                 'crying', 'obedient', \"obey\", \"love\", \"domestic\", \"victim\", \"mistress\", \"powerless\",\n",
    "                \"kid\", \"fear\", \"afraid\", \"marriage\", \"marrigeable\", \"marry\", \"married\", \"wedding\", \"body\", \"caress\", \n",
    "                \"seduce\", \"seductive\", \"angel\", \"single\", \"pregnancy\", \"makeup\", \"dress\", \"adorable\", \n",
    "                \"birth\", \"sexually\", \"relationship\", \"divorce\", \"lover\", \"care\", \"secretary\", \"lie\", \"bikini\", \"naked\", \"cheat\", \n",
    "                \"affair\", \"nurse\", \"maid\", \"agree\", \"affectionate\", \"affection\", \"child\", \"gossip\", \"dramatic\", \"drama\",\n",
    "                \"cheer\", \"collab\", \"commit\", \"commitment\", \"committed\", \"communal\", \"compassion\", \"considerate\", \"cooperate\",\n",
    "                \"co-operate\", \"depend\", \"emotiona\", \"emotional\", \"empath\", \"empathize\", \"empathetic\", \"flatterable\", \"flatter\",\n",
    "                \"gentle\", \"honest\", \"interpersonal\", \"interdependent\", \"interdependence\", \"inter-personal\", \n",
    "                \"inter-dependen\", \"inter-dependent\", \"inter-dependence\", \"inter-personal\", \"kind\", \"kinship\", \"loyal\", \"modesty\",\n",
    "                \"modest\", \"nag\", \"nurtur\", \"pleasant\", \"polite\", \"quiet\", \"respon\", \"sensitiv\", \"submissive\", \"support\", \"sympath\", \n",
    "                \"sympathy\", \"tender\", \"together\", \"trust\", \"understand\", \"warm\", \"whin\", \"enthusias\", \"inclusive\", \"yield\", \"share\", \"sharin\"])\n",
    "\n",
    "# words we are doubtful about: \"model\", \"accuse\", \"teen\", \"young\", \"home\",\"question\", \"date\", \"good\", \"hair\", \"couple\", \"teenage\", \"fail\", \"struggle\",\n",
    "\n",
    "\n",
    "\n",
    "male_words = set(['hero', 'man',  'men', 'his', 'him', 'he', 'husband', 'father', 'male', 'son', 'god',\n",
    "                  'prince', 'king', 'mr', 'sir', 'brother', 'grandfather', 'uncle', 'nephew', 'master', 'patriarch',\n",
    "                 'chairman', 'chairmen', \"boy\",\"boyfriend\", \"save\", \"guy\", \"dad\"])\n",
    "\n",
    "discrimination_words = set(['race', 'caste', 'casteless', 'black', 'SC', 'ST', 'african american', 'white', 'colour', \n",
    "                            'color', \"brown\", \"asian\", \"native\", \"racial\", \"minority\", \"ethnic\", \"ethnicity\", \"indian\",\n",
    "                            'hindu', \"muslim\", \"chinese\", \"indian\"])\n",
    "\n",
    "male_bias_words = set([\"active\", \"adventurous\", \"aggression\",'aggressive', \"ambition\", \"assert\", 'assertive',\n",
    "                       \"athlete\", 'athletic', \"battle\", \"champion\", \"decisive\", 'head', 'dominate', 'dominant',\n",
    "                        \"driven\", 'confident', 'strong', 'force', 'master', 'superior', 'strength', 'bold', \n",
    "                       'ambitious', 'power', 'intelligent', 'greedy', 'hostile', 'uncaring', 'logic', 'logical', 'rational',\n",
    "                       \"fearless\",'stubborn', 'independent', 'objective', \"charismatic\"])\n",
    "\n",
    "empowerement_words = set(['chairperson', 'leader','leadership',  'chairwoman', 'minister', 'power','powerful', 'authority', \n",
    "                          'queen', 'manager', 'success', 'successful', 'successes', 'career', 'job',\n",
    "                         'CEO','CFO', 'chief', 'officer', 'employment', 'employed', 'millionaire', \n",
    "                          'wealth', 'wealthy', 'strong', 'strength', 'courage','achievement', 'achievements', \n",
    "                          'achieve', 'goal', 'ambition', 'ambitious', 'passionate','passion', 'badass', \n",
    "                          'confident', 'confidence', 'breakthrough', \"inspirational\", \"educated\"\n",
    "                         'inspiring', 'inspiration', 'inspire', 'empower', 'empowered', 'empowerement',\n",
    "                         'genius', 'expert', 'mastery', 'owner', 'businesswoman', 'intelligent', 'smart', \n",
    "                          'clever', 'wise', 'worth', 'role model', 'role-model', 'activist', \"pay\", \"work\", \n",
    "                          \"business\", \"win\", \"award\", \"appoint\", \"lead\", \"star\", \"boss\", \"dream\",'goddess', \n",
    "                          \"actor\", 'queen', \"launch\", \"worker\", \"lawyer\", \"education\", \"director\", \"protester\", \n",
    "                          \"protest\", \"governor\", \"survive\", \"stallion\", \"doctor\", \"voice\", \"perfect\", \"author\",\n",
    "                          \"mayor\", \"founder\", \"abortion\", \"rise\", \"1st\", \"winner\", \"artist\", \"graduate\", \n",
    "                          \"employee\", \"earning\", \"survivor\", \"scientist\", \"equality\", \"equal\", \"deputy\", \n",
    "                          \"entrepreneur\", \"survive\", \"parent\"])\n",
    "\n",
    "politics_words = set([\"trump\", \"biden\", \"kamala\", \"harris\", \"joe\", \"vote\", \"election\", \"president\", \"elect\", \n",
    "                      \"state\", \"government\", \"obama\", \"office\", \"campaign\", \"melania\", \"vice\", \"govt\", \"donald\", \n",
    "                      \"voter\", \"congress\", \"candidate\", \"political\", \"breonna\", \"taylor\", \"pelosi\", \"democratic\",\n",
    "                      \"politic\", \"democrats\", \"republican\", \"ivanka\", \"republicans\", \"hillary\", \"clinton\", \"susan\", \n",
    "                      \"collins\", \"warren\", \"rep\", \"rally\", \"debate\", \"senate\", \"washington\", \"speech\", \"presidential\",\n",
    "                      \"boris\", \"mandela\"])\n",
    "\n",
    "violence_words = set([\"find\", \"allegedly\", \"fire\", \"life\", 'violent', 'violence', 'crime', 'rape','rapist', 'raped', 'murder','kill', 'killed','killer',\n",
    "                     'murdered', 'murderer', 'attack', 'alleged', 'criminal', 'stab', 'knife', 'gun', 'guns', 'knives',\n",
    "                     'blood', 'bloodshed', 'court', 'rage', 'outrage', 'rob', 'steal', 'robber', 'stealer', 'beater', \"beaten\",\n",
    "                     'domestic violence', 'aggression', 'aggressor', 'war', 'battle', 'abduction', 'assault', 'assaulted',\n",
    "                     'drug', 'abuse', 'child abuse', 'prison', 'fraud', 'human traffic', 'homicide', 'organised crime',\n",
    "                     'organized crime', 'genocide', 'fight', 'manslaughter', 'terrorist', 'weapon', 'smuggl', 'shoplift',\n",
    "                     'vandalism', 'crime', 'theft', 'penalty', 'prison sentence', 'detained', 'guilty', 'trial',\n",
    "                     'defense', 'defend', 'armed', 'jail', 'illegal', 'accomplice',\n",
    "                     'alcohol', 'allegation', 'arson', 'bail', 'battery', 'dead', 'death', 'deadly', 'corrupt', 'killer', \n",
    "                     'sex crime', 'wanted', \"arrest\", \"police\", \"die\", \"charge\", \"suspect\", \"shoot\", \"sentence\", \"cop\", \"hit\", \n",
    "                     \"break\", \"beat\", \"judge\", \"kidnap\", \"law\", \"corruption\", \"gang\", \"suicide\", \"critically injured\", 'harassment',\n",
    "                     \"run\", \"crash\", \"hospital\", 'security', \"report\", \"risk\", \"fall\", \"burn\", \"escape\", \"threaten\", \"slam\",\n",
    "                     \"gangrape\", \"harass\", \"brutally\", \"drown\", \"justice\", \"hate\", \"racist\", \"allege\", \"lawsuit\", \"injure\", \"racism\", \n",
    "                      \"thug\", \"suffer\", \"injury\", \"horror\", \"killing\", \"robbery\", \"plead\", \"wound\", \"kidnapping\", \"convict\", \"shooting\"])\n",
    "\n",
    "female_bias = female_words.union(female_bias_words)\n",
    "male_bias = male_words.union(male_bias_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_hl_words =[]\n",
    "# all_marked_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_marked_words =  female_words.union(female_bias_words).union(male_words).union(discrimination_words).union(male_bias_words).union(empowerement_words).union(violence_words)\n",
    "# #all_marked_words = ast.literal_eval(all_marked_words)\n",
    "# #list(female_bias)\n",
    "# all_marked_words = list(all_marked_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def addedges(list):\n",
    "#     edges = []\n",
    "#     for i in combinations(list,2):\n",
    "#         edges.append(i)\n",
    "#     G.add_edges_from(edges) \n",
    "#     print('added edge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def usefulwordassociations(list):\n",
    "#     new_list = [x for x in list if x in all_marked_words]\n",
    "#     return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hl_words(all_hl_words):\n",
    "    for index,value in enumerate(all_hl_words):\n",
    "        if len(value) == 1:\n",
    "            all_hl_words.remove(value)\n",
    "\n",
    "    for index,value in enumerate(all_hl_words):\n",
    "        if value.isalpha() == False:\n",
    "            all_hl_words.remove(value)\n",
    "    return all_hl_words       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleanwordassociations(list):\n",
    "#     new_list = [x for x in list if x.isalpha == False]\n",
    "#     new_list = [x for x in new_list if len(x)>1]\n",
    "#     return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createfullnetwork(df):\n",
    "    G = nx.MultiGraph()\n",
    "    all_hl_words = []\n",
    "    for i in range(len(df)):\n",
    "        word_associations = []\n",
    "        all_hl_words += clean_hl_words(ast.literal_eval(df['clean_hl_words'][i]))\n",
    "        \n",
    "        #all_hl_words = clean_hl_words(all_hl_words)\n",
    "        #word_associations = cleanwordassociations(ast.literal_eval(df['clean_hl_words'][i]))\n",
    "        \n",
    "        word_associations = ast.literal_eval(df['clean_hl_words'][i])\n",
    "        \n",
    "        #print(type(word_associations))\n",
    "        #addedges(word_associations)\n",
    "        \n",
    "        edges = []\n",
    "        for i in combinations(word_associations,2):\n",
    "            edges.append(i)\n",
    "        G.add_edges_from(edges) \n",
    "        \n",
    "    #print('added edge')\n",
    "    G.add_nodes_from(all_hl_words)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWeightedGraph(g_test):\n",
    "    G = nx.Graph()\n",
    "    for u,v,data in g_test.edges(data=True):\n",
    "        w = data['weight'] if 'weight' in data else 1.0\n",
    "        if G.has_edge(u,v):\n",
    "            G[u][v]['weight'] += int(w)\n",
    "        else:\n",
    "            G.add_edge(u, v, weight=w)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNodeFreq(g, freq):\n",
    "    nx.set_node_attributes(g, 0, \"frequency\")\n",
    "    nx.set_node_attributes(g, 0, \"perc_freq\")\n",
    "    for u in g.nodes(data=True):\n",
    "        node = u[0]\n",
    "        if len(freq[freq['word']== str(u[0])])>0:\n",
    "            g.nodes[str(node)]['frequency'] = int(freq[freq['word']== str(u[0])]['frequency'].values[0])\n",
    "            g.nodes[str(node)]['perc_freq'] = int(freq[freq['word']== str(u[0])]['perc_freq'].values[0]*100000)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_themes(g):\n",
    "    nx.set_node_attributes(g, \"\", \"theme\")\n",
    "    for u in g.nodes(data=True):\n",
    "        node = u[0]\n",
    "        if node in female_bias:\n",
    "             g.nodes[str(node)]['theme'] = \"female_bias\"\n",
    "        elif node in male_bias:\n",
    "             g.nodes[str(node)]['theme'] = \"male_bias\"\n",
    "        elif node in empowerement_words:\n",
    "             g.nodes[str(node)]['theme'] = \"empowerment\"\n",
    "        elif node in violence_words:\n",
    "             g.nodes[str(node)]['theme'] = \"violence\"\n",
    "        elif node in politics_words:\n",
    "             g.nodes[str(node)]['theme'] = \"politics\"\n",
    "        elif node in discrimination_words:\n",
    "             g.nodes[str(node)]['theme'] = \"race\"\n",
    "        else:\n",
    "            g.nodes[str(node)]['theme'] = \"other\"\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_themes(g):\n",
    "    nx.set_edge_attributes(g, \"\", \"theme\")\n",
    "    for u,v,data in g.edges(data=True):\n",
    "    #     print(data['theme'])\n",
    "    #     print(g[u][v]['weight'])\n",
    "        if u in female_bias:\n",
    "             g[u][v]['theme'] = \"female_bias\"\n",
    "        elif u in male_bias:\n",
    "             g[u][v]['theme'] = \"male_bias\"\n",
    "        elif u in empowerement_words:\n",
    "             g[u][v]['theme'] = \"empowerment\"\n",
    "        elif u in violence_words:\n",
    "             g[u][v]['theme'] = \"violence\"\n",
    "        elif u in politics_words:\n",
    "             g[u][v]['theme'] = \"politics\"\n",
    "        elif u in discrimination_words:\n",
    "             g[u][v]['theme'] = \"race\"\n",
    "        else:\n",
    "            g[u][v]['theme'] = \"other\"\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFinalNetwork(df, freq, edge_filter=2, node_filter=50):\n",
    "    # create network\n",
    "    g = createfullnetwork(df)\n",
    "    # get edge weights\n",
    "    g = createWeightedGraph(g)\n",
    "    # get node themes\n",
    "    g = get_node_themes(g)\n",
    "    # get edge themes\n",
    "    g = get_edge_themes(g)\n",
    "    # get node frequency\n",
    "    g = getNodeFreq(g, freq)\n",
    "    print(len(g.nodes), \"nodes\", \"and\", len(g.edges), \"edges before reducing\") \n",
    "    # remove irrelevant noise\n",
    "    ## add irrelevant words here\n",
    "    words = ['say', 'new', 'time', \"south\", \"africa\", \"india\", \"england\", \"united states\", \"american\", \"americans\", \n",
    "             \"UK\", \"get\", \"video\", \"tell\", \"cape\", \"makesefakese\", \"not\", \"no\", \"ask\", \"november\"]\n",
    "    remove_edges = [(u,v) for u, v, data in g.edges(data=True) if data[\"weight\"] <= edge_filter]\n",
    "    remove_nodes = [node[0] for node in g.nodes(data=True) if g.nodes[node[0]][\"perc_freq\"] <= node_filter]\n",
    "    remove_nodes.extend(words)\n",
    "    g.remove_edges_from(remove_edges)\n",
    "    g.remove_nodes_from(remove_nodes)\n",
    "    print(len(g.nodes), \"nodes\", \"and\", len(g.edges), \"edges after reducing\")     \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_data(freq, df, country):\n",
    "    country_freqs = freq[freq['country']==country].reset_index()\n",
    "    country_df = df[df['country']==country].reset_index()\n",
    "    return country_df, country_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveNetwork(g, country):\n",
    "    connections = json_graph.node_link_data(g)\n",
    "    connections_json = json.dumps(connections, indent = 2) \n",
    "    with open(f\"../data/processed/word_connections_{country}.json\", \"w\") as outfile: \n",
    "        outfile.write(connections_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13181 nodes and 215872 edges before reducing\n",
      "589 nodes and 4622 edges after reducing\n",
      "15094 nodes and 294534 edges before reducing\n",
      "633 nodes and 6153 edges after reducing\n",
      "16674 nodes and 282702 edges before reducing\n",
      "602 nodes and 5373 edges after reducing\n",
      "10476 nodes and 104227 edges before reducing\n",
      "574 nodes and 1805 edges after reducing\n"
     ]
    }
   ],
   "source": [
    "freq = pd.read_csv(\"../data/processed/countries_freq.csv\")\n",
    "# country = \"India\" #South Africa, UK, USA, India\n",
    "# # words = list(freq[freq['country']!=country].word.values)\n",
    "# country_freqs = freq[freq['country']!=country].reset_index()\n",
    "# country_df = df[df['country']==country].reset_index()\n",
    "g_IN = getFinalNetwork(df = get_country_data(freq=freq, df=df, country = \"India\")[0], freq = get_country_data(freq=freq, df=df, country = \"India\")[1])\n",
    "saveNetwork(g_IN, \"IN\")\n",
    "g_USA = getFinalNetwork(df = get_country_data(freq=freq, df=df, country = \"USA\")[0], freq = get_country_data(freq=freq, df=df, country = \"USA\")[1])\n",
    "saveNetwork(g_USA, \"USA\")\n",
    "g_UK = getFinalNetwork(get_country_data(freq, df, \"UK\")[0], get_country_data(freq, df, \"UK\")[1])\n",
    "saveNetwork(g_UK, \"UK\")\n",
    "g_SA = getFinalNetwork(get_country_data(freq, df, \"South Africa\")[0], get_country_data(freq, df, \"South Africa\")[1])\n",
    "saveNetwork(g_SA, \"SA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13181 nodes and 215872 edges before reducing\n",
      "91 nodes and 175 edges after reducing\n",
      "15094 nodes and 294534 edges before reducing\n",
      "92 nodes and 157 edges after reducing\n",
      "16674 nodes and 282702 edges before reducing\n",
      "87 nodes and 142 edges after reducing\n",
      "10476 nodes and 104227 edges before reducing\n",
      "83 nodes and 83 edges after reducing\n"
     ]
    }
   ],
   "source": [
    "freq = pd.read_csv(\"../data/processed/countries_freq.csv\")\n",
    "# country = \"India\" #South Africa, UK, USA, India\n",
    "# # words = list(freq[freq['country']!=country].word.values)\n",
    "# country_freqs = freq[freq['country']!=country].reset_index()\n",
    "# country_df = df[df['country']==country].reset_index()\n",
    "g_IN = getFinalNetwork(df = get_country_data(freq=freq, df=df, country = \"India\")[0], freq = get_country_data(freq=freq, df=df, country = \"India\")[1], edge_filter=10, node_filter=200)\n",
    "saveNetwork(g_IN, \"IN_small\")\n",
    "g_USA = getFinalNetwork(df = get_country_data(freq=freq, df=df, country = \"USA\")[0], freq = get_country_data(freq=freq, df=df, country = \"USA\")[1], edge_filter=10, node_filter=200)\n",
    "saveNetwork(g_USA, \"USA_small\")\n",
    "g_UK = getFinalNetwork(get_country_data(freq, df, \"UK\")[0], get_country_data(freq, df, \"UK\")[1], edge_filter=10, node_filter=200)\n",
    "saveNetwork(g_UK, \"UK_small\")\n",
    "g_SA = getFinalNetwork(get_country_data(freq, df, \"South Africa\")[0], get_country_data(freq, df, \"South Africa\")[1], edge_filter=10, node_filter=200)\n",
    "saveNetwork(g_SA, \"SA_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_nodes_IN = [node[0] for node in g_IN.nodes(data=True) if g_IN.nodes[node[0]][\"theme\"] == \"other\"]\n",
    "grey_nodes_USA = [node[0] for node in g_USA.nodes(data=True) if g_USA.nodes[node[0]][\"theme\"] == \"other\"]\n",
    "grey_nodes_UK = [node[0] for node in g_UK.nodes(data=True) if g_UK.nodes[node[0]][\"theme\"] == \"other\"]\n",
    "grey_nodes_SA = [node[0] for node in g_SA.nodes(data=True) if g_SA.nodes[node[0]][\"theme\"] == \"other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reveal',\n",
       " 'real',\n",
       " 'indian',\n",
       " 'covid',\n",
       " 'treatment',\n",
       " 'security',\n",
       " 'come',\n",
       " 'claim',\n",
       " 'study',\n",
       " 'hand',\n",
       " 'question',\n",
       " 'diwali',\n",
       " 'post',\n",
       " 'report',\n",
       " 'hathras',\n",
       " 'not',\n",
       " 'kerala',\n",
       " 'act',\n",
       " 'state',\n",
       " 'government',\n",
       " 'eat',\n",
       " 'right',\n",
       " 'food',\n",
       " 'school',\n",
       " 'grow',\n",
       " 'bengaluru',\n",
       " 'share',\n",
       " 'tip',\n",
       " 'great',\n",
       " 'risk',\n",
       " 'long',\n",
       " 'set',\n",
       " 'way',\n",
       " 'tell',\n",
       " 'official',\n",
       " 'face',\n",
       " 'day',\n",
       " 'obama',\n",
       " 'leave',\n",
       " 'office',\n",
       " 'go',\n",
       " 'troll',\n",
       " 'online',\n",
       " 'date',\n",
       " 'app',\n",
       " 'give',\n",
       " 'struggle',\n",
       " 'like',\n",
       " 'fall',\n",
       " 'single',\n",
       " 'stay',\n",
       " 'style',\n",
       " 'room',\n",
       " 'pregnancy',\n",
       " 'shilpa',\n",
       " 'shetty',\n",
       " 'happy',\n",
       " 'hour',\n",
       " 'singh',\n",
       " 'character',\n",
       " 'look',\n",
       " 'rajput',\n",
       " 'kapoor',\n",
       " 'hair',\n",
       " 'health',\n",
       " 'sell',\n",
       " 'late',\n",
       " 'malaika',\n",
       " 'arora',\n",
       " 'help',\n",
       " 'loss',\n",
       " 'mask',\n",
       " 'deepika',\n",
       " 'padukone',\n",
       " 'makeup',\n",
       " 'hang',\n",
       " 'eye',\n",
       " 'want',\n",
       " 'fact',\n",
       " 'taapsee',\n",
       " 'pannu',\n",
       " 'film',\n",
       " 'ape',\n",
       " 'priyanka',\n",
       " 'chopra',\n",
       " 'world',\n",
       " 'moment',\n",
       " 'try',\n",
       " 'fashion',\n",
       " 'story',\n",
       " 'talk',\n",
       " 'special',\n",
       " 'sonam',\n",
       " 'dog',\n",
       " 'drop',\n",
       " 'dress',\n",
       " 'week',\n",
       " 'adorable',\n",
       " 'keep',\n",
       " 'khan',\n",
       " 'quit',\n",
       " 'friend',\n",
       " 'amid',\n",
       " 'pandemic',\n",
       " 'crush',\n",
       " 'kumar',\n",
       " 'campaign',\n",
       " 'throw',\n",
       " 'party',\n",
       " 'year',\n",
       " 'melania',\n",
       " 'end',\n",
       " 'australia',\n",
       " 'thing',\n",
       " 'birth',\n",
       " 'toilet',\n",
       " 'catch',\n",
       " 'couple',\n",
       " 'constable',\n",
       " 'china',\n",
       " 'test',\n",
       " 'coronavirus',\n",
       " 'attempt',\n",
       " 'well',\n",
       " 'think',\n",
       " 'age',\n",
       " 'burn',\n",
       " 'alive',\n",
       " 'people',\n",
       " 'cancer',\n",
       " 'spend',\n",
       " 'night',\n",
       " 'change',\n",
       " 'selfie',\n",
       " 'learn',\n",
       " 'write',\n",
       " 'raise',\n",
       " 'picture',\n",
       " 'viral',\n",
       " 'netflix',\n",
       " 'sexually',\n",
       " 'person',\n",
       " 'doctor',\n",
       " 'parent',\n",
       " 'hold',\n",
       " 'update',\n",
       " 'november',\n",
       " 'make',\n",
       " 'plan',\n",
       " 'vice',\n",
       " 'disha',\n",
       " 'twitter',\n",
       " 'fan',\n",
       " 'stop',\n",
       " 'move',\n",
       " 'mumbai',\n",
       " 'probe',\n",
       " 'diana',\n",
       " 'big',\n",
       " 'watch',\n",
       " 'ali',\n",
       " 'read',\n",
       " 'minor',\n",
       " 'hindu',\n",
       " 'muslim',\n",
       " 'bengal',\n",
       " 'member',\n",
       " 'ask',\n",
       " 'kolkata',\n",
       " 'student',\n",
       " 'start',\n",
       " 'trend',\n",
       " 'old',\n",
       " 'crorepati',\n",
       " 'take',\n",
       " 'cut',\n",
       " 'news',\n",
       " 'social',\n",
       " 'medium',\n",
       " 'air',\n",
       " 'instagram',\n",
       " 'escape',\n",
       " 'know',\n",
       " 'sushmita',\n",
       " 'sen',\n",
       " 'open',\n",
       " 'threaten',\n",
       " 'pilot',\n",
       " 'saxena',\n",
       " 'village',\n",
       " 'rule',\n",
       " 'board',\n",
       " 'discover',\n",
       " 'meet',\n",
       " 'show',\n",
       " 'vs',\n",
       " 'name',\n",
       " 'pass',\n",
       " 'relationship',\n",
       " 'kashmir',\n",
       " 'delhi',\n",
       " 'support',\n",
       " 'self',\n",
       " 'passenger',\n",
       " 'flight',\n",
       " 'rise',\n",
       " 'bihar',\n",
       " 'pakistan',\n",
       " 'gandhi',\n",
       " 'safety',\n",
       " 'uttar',\n",
       " 'pradesh',\n",
       " 'modi',\n",
       " 'case',\n",
       " 'medical',\n",
       " 'injure',\n",
       " 'bear',\n",
       " 'play',\n",
       " 'slam',\n",
       " 't20',\n",
       " 'challenge',\n",
       " 'pak',\n",
       " 'inside',\n",
       " 'tribute',\n",
       " 'bid',\n",
       " 'airport',\n",
       " 'debut',\n",
       " 'release',\n",
       " 'trailer',\n",
       " 'role',\n",
       " 'reason',\n",
       " 'movie',\n",
       " 'away',\n",
       " 'today',\n",
       " 'announce',\n",
       " 'group',\n",
       " 'pakistani',\n",
       " 'bollywood',\n",
       " 'tamil',\n",
       " 'team',\n",
       " 'no',\n",
       " 'call',\n",
       " 'turn',\n",
       " 'book',\n",
       " 'chakraborty',\n",
       " 'cricket',\n",
       " 'cup',\n",
       " 'ago',\n",
       " 'worker',\n",
       " 'demand',\n",
       " 'rhea',\n",
       " 'lawyer',\n",
       " 'record',\n",
       " 'wear',\n",
       " 'kangana',\n",
       " 'rajasthan',\n",
       " 'period',\n",
       " 'need',\n",
       " 'allow',\n",
       " 'pic',\n",
       " 'divorce',\n",
       " 'neha',\n",
       " 'govt',\n",
       " 'donald',\n",
       " 'bigg',\n",
       " 'create',\n",
       " 'host',\n",
       " 'stunning',\n",
       " 'photo',\n",
       " 'rahul',\n",
       " 'let',\n",
       " 'use',\n",
       " 'bachchan',\n",
       " 'anti',\n",
       " 'human',\n",
       " 'gujarat',\n",
       " 'prove',\n",
       " 'assam',\n",
       " 'refuse',\n",
       " 'college',\n",
       " 'class',\n",
       " 'vaccine',\n",
       " 'travel',\n",
       " 'celebrate',\n",
       " 'dad',\n",
       " 'allege',\n",
       " 'issue',\n",
       " 'stand',\n",
       " 'speak',\n",
       " 'see',\n",
       " 'teach',\n",
       " 'karnataka',\n",
       " 'seek',\n",
       " 'money',\n",
       " 'sharma',\n",
       " 'education',\n",
       " 'hot',\n",
       " 'check',\n",
       " 'jihad',\n",
       " 'kashmiri',\n",
       " 'sushant',\n",
       " 'tweet',\n",
       " 'fake',\n",
       " 'account',\n",
       " 'teacher',\n",
       " 'heart',\n",
       " 'house',\n",
       " 'return',\n",
       " 'telangana',\n",
       " 'hospital',\n",
       " 'patient',\n",
       " 'lakh',\n",
       " 'ranaut',\n",
       " 'internet',\n",
       " 'explain',\n",
       " 'army',\n",
       " 'tara',\n",
       " 'maldives',\n",
       " 'birthday',\n",
       " '1st',\n",
       " 'ld',\n",
       " 'second',\n",
       " 'traffic',\n",
       " 'positive',\n",
       " 'voter',\n",
       " 'gangrape',\n",
       " 'dalit',\n",
       " 'journalist',\n",
       " 'rs',\n",
       " 'bank',\n",
       " 'yogi',\n",
       " 'protest',\n",
       " 'file',\n",
       " 'car',\n",
       " 'high',\n",
       " 'centre',\n",
       " 'month',\n",
       " 'acid',\n",
       " 'national',\n",
       " 'commission',\n",
       " 'speech',\n",
       " 'thrash',\n",
       " 'outside',\n",
       " 'haryana',\n",
       " 'live',\n",
       " 'trailblazer',\n",
       " 'send',\n",
       " 'order',\n",
       " 'gift',\n",
       " 'note',\n",
       " 'west',\n",
       " 'visit',\n",
       " 'recover',\n",
       " 'entrepreneur',\n",
       " 'poll',\n",
       " 'exam',\n",
       " 'chinese',\n",
       " 'earn',\n",
       " 'list',\n",
       " 'wish',\n",
       " 'reply',\n",
       " 'complaint',\n",
       " 'goa',\n",
       " 'chennai',\n",
       " 'pick',\n",
       " 'punjab',\n",
       " 'nadu',\n",
       " 'song',\n",
       " 'mehbooba',\n",
       " 'c19',\n",
       " 'soon',\n",
       " 'likely',\n",
       " 'walk',\n",
       " 'netizen',\n",
       " 'rescue',\n",
       " 'road',\n",
       " 'review',\n",
       " 'action',\n",
       " 'future',\n",
       " 'youth',\n",
       " 'lockdown',\n",
       " 'free',\n",
       " 'choose',\n",
       " 'safe',\n",
       " 'congress',\n",
       " 'candidate',\n",
       " 'indira',\n",
       " 'anniversary',\n",
       " 'bus',\n",
       " 'little',\n",
       " 'proud',\n",
       " 'red',\n",
       " 'international',\n",
       " 'train',\n",
       " 'railway',\n",
       " 'step',\n",
       " 'exclusive',\n",
       " 'message',\n",
       " 'gold',\n",
       " 'celebration',\n",
       " 'driver',\n",
       " 'commit',\n",
       " 'teenage',\n",
       " 'experience',\n",
       " 'pune',\n",
       " 'elderly',\n",
       " 'bring',\n",
       " 'station',\n",
       " 'quarantine',\n",
       " 'field',\n",
       " 'shah',\n",
       " 'tribal',\n",
       " 'join',\n",
       " 'treat',\n",
       " 'water',\n",
       " 'convert',\n",
       " 'street',\n",
       " 'lover',\n",
       " 'affair',\n",
       " 'shocker',\n",
       " 'bag',\n",
       " 'kareena',\n",
       " 'bhai',\n",
       " 'remember',\n",
       " 'clothe',\n",
       " 'sara',\n",
       " 'tree',\n",
       " 'accident',\n",
       " 'saree',\n",
       " 'maharashtra',\n",
       " 'dance',\n",
       " 'tie',\n",
       " 'pet',\n",
       " 'pose',\n",
       " 'hyderabad',\n",
       " 'harass',\n",
       " 'andhra',\n",
       " 'noida',\n",
       " 'molest',\n",
       " 'kanpur',\n",
       " 'brutally',\n",
       " 'gurugram',\n",
       " 'resist',\n",
       " 'near',\n",
       " 'drown',\n",
       " 'blackmail',\n",
       " 'odisha',\n",
       " 'ablaze',\n",
       " 'drive',\n",
       " 'justice',\n",
       " 'district',\n",
       " 'madhya',\n",
       " 'srinagar',\n",
       " 'local',\n",
       " 'carry',\n",
       " 'puja',\n",
       " 'enjoy',\n",
       " 'katrina',\n",
       " 'deny',\n",
       " 'alia',\n",
       " 'bhatt',\n",
       " 'km',\n",
       " 'neighbour',\n",
       " 'deliver',\n",
       " 'welcome',\n",
       " 'director',\n",
       " 'nitish',\n",
       " 'kaif',\n",
       " 'cycle',\n",
       " 'journey',\n",
       " 'toll',\n",
       " 'anushka',\n",
       " 'sari',\n",
       " 'jk']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'theme': 'violence', 'frequency': 250, 'perc_freq': 946}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_SA.nodes['rape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009463244757362405"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq[freq['word']== \"rape\"]['perc_freq'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0005*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10415    1431.440078\n",
       "10416    1363.009165\n",
       "10417    1312.527344\n",
       "10418    1020.854601\n",
       "10419     701.136402\n",
       "            ...     \n",
       "29855       1.121818\n",
       "29856       1.121818\n",
       "29857       1.121818\n",
       "29858       1.121818\n",
       "29859       1.121818\n",
       "Name: perc_freq, Length: 19445, dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq[freq['country']==\"USA\"]['perc_freq']*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'perc_freq'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-865dbd65cf79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mg_USA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg_USA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"perc_freq\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-91-865dbd65cf79>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mg_USA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg_USA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"perc_freq\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'perc_freq'"
     ]
    }
   ],
   "source": [
    "len([node[0] for node in g_USA.nodes(data=True) if g_USA.nodes[node[0]][\"perc_freq\"] <= 0.0005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([(u,v) for u, v, data in g_IN.edges(data=True) if data[\"weight\"] <= 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_edges = [(u,v) for u, v, data in g.edges(data=True) if data[\"weight\"] <= 3]\n",
    "remove_nodes = [node[0] for node in g.nodes(data=True) if g.nodes[node[0]][\"frequency\"] <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_SA = getNodeFreq(g_SA, country_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_SA.edges['man', \"rape\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_SA.nodes['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1331"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len([node[0]  for node in g_SA.nodes(data=True) if g_SA.nodes[node[0]][\"frequency\"] <= 5])\n",
    "len([(u,v) for u, v, data in g_IN.edges(data=True) if data[\"weight\"] <= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_edges = [(u,v) for u, v, data in g_IN.edges(data=True) if data[\"weight\"] <= 3]\n",
    "remove_nodes = [node[0] for node in g_IN.nodes(data=True) if g_IN.nodes[node[0]][\"frequency\"] <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_SA.remove_edges_from(remove_edges)\n",
    "g_SA.remove_nodes_from(remove_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209850"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_IN.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34311\n",
      "1173780\n"
     ]
    }
   ],
   "source": [
    "g = createfullnetwork(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34311\n",
      "1173780\n"
     ]
    }
   ],
   "source": [
    "g = createfullnetwork(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_weighted = createWeightedGraph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>say</td>\n",
       "      <td>2146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>new</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>man</td>\n",
       "      <td>1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>trump</td>\n",
       "      <td>1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>black</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   word  frequency\n",
       "0           0    say       2146\n",
       "1           1    new       1940\n",
       "2           2    man       1774\n",
       "3           3  trump       1573\n",
       "4           4  black       1235"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = []\n",
    "nx.set_node_attributes(G, frequency, \"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in g_weighted.nodes(data=True):\n",
    "    node = u[0]\n",
    "    if len(freq[freq['word']== str(u[0])])>0:\n",
    "        g_weighted.nodes[str(node)]['frequency'] = int(freq[freq['word']== str(u[0])]['frequency'].values[0])\n",
    "   # print(u[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frequency': 937}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_weighted.nodes['die']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>crown</td>\n",
       "      <td>v</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>crown</td>\n",
       "      <td>di</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>crown</td>\n",
       "      <td>diana</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>le</td>\n",
       "      <td>di</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>le</td>\n",
       "      <td>diana</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12603</th>\n",
       "      <td>supply</td>\n",
       "      <td>problem</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12606</th>\n",
       "      <td>problem</td>\n",
       "      <td>reckon</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12868</th>\n",
       "      <td>monthly</td>\n",
       "      <td>archives</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12871</th>\n",
       "      <td>monthly</td>\n",
       "      <td>archive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13092</th>\n",
       "      <td>archives</td>\n",
       "      <td>tag</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         source    target  weight\n",
       "0         crown         v       2\n",
       "6         crown        di       2\n",
       "7         crown     diana       3\n",
       "65           le        di       2\n",
       "66           le     diana       2\n",
       "...         ...       ...     ...\n",
       "12603    supply   problem       2\n",
       "12606   problem    reckon       2\n",
       "12868   monthly  archives       4\n",
       "12871   monthly   archive       3\n",
       "13092  archives       tag       2\n",
       "\n",
       "[795 rows x 3 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_test = nx.to_pandas_edgelist(G)\n",
    "edges_test[edges_test['weight']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "g = createfullnetwork(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "70\n",
      "357\n"
     ]
    }
   ],
   "source": [
    "G = createfullnetwork(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_test.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node attributes to add -\n",
    "1. which dictionary it belongs to\n",
    "2. frequency of node\n",
    "3. frequency of edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = json_graph.node_link_data(g_weighted)\n",
    "connections_json = json.dumps(connections, indent = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/word_connections.json\", \"w\") as outfile: \n",
    "    outfile.write(connections_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231     ['crown', 'v', 'realt', 'le', 'foto', 'dei', '...\n",
       "738     ['crown', 'riveler', 'nuovi', 'segreti', 'anch...\n",
       "2798    ['play', 'princess', 'diana', 'netflix', 'crown']\n",
       "3885    ['fact', 'crown', 'star', 'emma', 'corrin', 'l...\n",
       "3970    ['princess', 'diana', 'camilla', 'parker', 'bo...\n",
       "Name: clean_hl_words, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[(df['headline'].str.lower().str.contains('diana')) & (df['headline'].str.lower().str.contains('crown'))].head()['clean_hl_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.number_of_edges('diana', 'crown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32592"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_filtered.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['useful_word_associations'] = np.empty((len(df), 0)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# time\n",
    "# def calcfilterednetwork(df):\n",
    "#     all_hl_words = []\n",
    "#     all_useful_hl_words = []\n",
    "#     for i in range(len(df)):\n",
    "#         all_hl_words += getusefulnodes(ast.literal_eval(df['clean_hl_words'][i]))\n",
    "#         df['useful_word_associations'].iloc[i] = usefulwordassociations(ast.literal_eval(df['clean_hl_words'][i]))\n",
    "        \n",
    "#         if len(df['useful_word_associations'][i])>=1:\n",
    "#             addedges(df['useful_word_associations'][i])\n",
    "            \n",
    "            \n",
    "#     G.add_nodes_from(all_hl_words)\n",
    "    \n",
    "#     print(len(all_hl_words))  \n",
    "#     return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_filtered = calcfilterednetwork(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Themes addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(filename):\n",
    "    with open(filename) as f:\n",
    "        js_graph = json.load(f)\n",
    "    return json_graph.node_link_graph(js_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = read_json_file(\"../data/processed/word_connections_4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme = \"\"\n",
    "nx.set_node_attributes(g, theme, \"theme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x1ad8a9116a0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_node_themes(g):\n",
    "    for u in g.nodes(data=True):\n",
    "        node = u[0]\n",
    "        if node in female_bias:\n",
    "             g.nodes[str(node)]['theme'] = \"female_bias\"\n",
    "        elif node in male_bias:\n",
    "             g.nodes[str(node)]['theme'] = \"male_bias\"\n",
    "        elif node in empowerement_words:\n",
    "             g.nodes[str(node)]['theme'] = \"empowerment\"\n",
    "        elif node in violence_words:\n",
    "             g.nodes[str(node)]['theme'] = \"violence\"\n",
    "        elif node in politics_words:\n",
    "             g.nodes[str(node)]['theme'] = \"politics\"\n",
    "        elif node in discrimination_words:\n",
    "             g.nodes[str(node)]['theme'] = \"race\"\n",
    "        else:\n",
    "            g.nodes[str(node)]['theme'] = \"other\"\n",
    "            \n",
    "    return g\n",
    "\n",
    "get_node_themes(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frequency': 954, 'theme': 'violence'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes['police']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.remove_node(\"say\")\n",
    "g.remove_node(\"new\")\n",
    "g.remove_node(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_edge_attributes(g, theme, \"theme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_themes(g):\n",
    "    for u,v,data in g.edges(data=True):\n",
    "    #     print(data['theme'])\n",
    "    #     print(g[u][v]['weight'])\n",
    "        if u in female_bias:\n",
    "             g[u][v]['theme'] = \"female_bias\"\n",
    "        elif u in male_bias:\n",
    "             g[u][v]['theme'] = \"male_bias\"\n",
    "        elif u in empowerement_words:\n",
    "             g[u][v]['theme'] = \"empowerment\"\n",
    "        elif u in violence_words:\n",
    "             g[u][v]['theme'] = \"violence\"\n",
    "        elif u in politics_words:\n",
    "             g[u][v]['theme'] = \"politics\"\n",
    "        elif u in discrimination_words:\n",
    "             g[u][v]['theme'] = \"race\"\n",
    "        else:\n",
    "            g[u][v]['theme'] = \"other\"\n",
    "            \n",
    "get_edge_themes(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': 173.0, 'theme': 'politics'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edges[\"joe\", \"biden\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = json_graph.node_link_data(g)\n",
    "connections_json = json.dumps(connections, indent = 2) \n",
    "with open(\"../data/processed/word_connections_4_themes.json\", \"w\") as outfile: \n",
    "    outfile.write(connections_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [(u,v) for u, v, data in g.edges(data=True) if data[\"weight\"] <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83723"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93642"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.remove_edges_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9919"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = json_graph.node_link_data(g)\n",
    "connections_json = json.dumps(connections, indent = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/word_connections_4_themes_filtered.json\", \"w\") as outfile: \n",
    "    outfile.write(connections_json) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countries Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = read_json_file(\"../data/processed/word_connections_4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.read_csv(\"../data/processed/countries_freq.csv\")\n",
    "country = \"South Africa\"\n",
    "words = list(freq[freq['country']!=country].word.values)\n",
    "country_freqs = freq[freq['country']!=country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2032"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.remove_nodes_from(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(g.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(g, 0, \"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frequency': 0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes['die']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get specific country frequencies\n",
    "for u in g.nodes(data=True):\n",
    "    node = u[0]\n",
    "    if len(country_freqs[country_freqs['word']== str(u[0])])>0:\n",
    "        g.nodes[str(node)]['frequency'] = int(freq[freq['word']== str(u[0])]['frequency'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frequency': 143}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes['die']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
