{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "import networkx as nx; \n",
    "from networkx.readwrite import json_graph;\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "import time\n",
    "import json\n",
    "nlp = spacy.load('en_core_web_md') #you can use other methods"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "datatype = \"rapi\"\n",
    "df = pd.read_csv(f'../data/processed/headlines_cl_sent_pol_{datatype}.csv')\n",
    "freq = pd.read_csv(f'../data/processed/words_freq_{datatype}.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "freq.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(65846, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "df.drop_duplicates(subset = ['clean_hl_words'], inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "for idx in nlp(' '.join(word for word in ast.literal_eval(df['clean_hl_words'][1541]))):\n",
    "    print(idx.pos_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NOUN\n",
      "NOUN\n",
      "NOUN\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "for w in nlp(\"laugh\"):\n",
    "    print(w.pos_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PROPN\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "df = df[df['clean_hl_words']!=\"set()\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "%%time\n",
    "# exclude nouns from word list\n",
    "# excluded_tags = {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"ADP\", \"PROPN\"}\n",
    "included_tags = {\"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "# df[\"clean_h1_verbs_adjs\"] = df['clean_hl_words'].apply(lambda x: [str(idx) for idx in nlp(' '.join(word for word in ast.literal_eval(x))) if idx.pos_ in included_tags])\n",
    "df[\"clean_h1_verbs_adjs\"] = df['clean_hl_words'].apply(lambda x: [str(idx) for idx in nlp(' '.join(word for word in ast.literal_eval(x))) if idx.pos_ in included_tags])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 50min\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "df.to_csv(f'../data/processed/headlines_cl_sent_pol_vbs_{datatype}.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "# porter = PorterStemmer()\n",
    "# lancaster=LancasterStemmer()\n",
    "\n",
    "# print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "# for word in female_bias_words:\n",
    "#     print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "# sources\r\n",
    "# http://gender-decoder.katmatfield.com/about\r\n",
    "# http://gender-decoder.katmatfield.com/static/documents/Gaucher-Friesen-Kay-JPSP-Gendered-Wording-in-Job-ads.pdf\r\n",
    "# https://github.com/DanielleSucher/Jailbreak-the-Patriarchy\r\n",
    "\r\n",
    "# this list of words is used to check for female-gendered language\r\n",
    "female_words=set(['actress',  'actresses',  'airwomen',  'alderwoman',  'alderwomen',  'anchorwoman',  'anchorwomen',  'archduchess',  'archduchesses',  \r\n",
    "                  'assemblywoman',  'assemblywomen',  'aunt',  'aunts',  'baroness',  'baronesses',  'baronetess',  'baronetesses',  'bogeywoman',  \r\n",
    "                  'bogeywomen',  'bondswoman',  'bondswomen',  'bride',  'businesswoman',  'businesswomen',  'camerawoman',  'camerawomen',  \r\n",
    "                  'cavewoman',  'cavewomen',  'chairwoman',  'chairwomen',  'clergywoman',  'clergywomen',  'comtesse',  'congresswoman',  \r\n",
    "                  'congresswomen',  'councilwoman',  'councilwomen',  'countess',  'countesses',  'countrywoman',  'countrywomen',  \r\n",
    "                  'craftswoman',  'craftswomen',  'damsel',  'daughter',  'daughters',  'deaconess',  'deaconesses',  'diva',  'donna', \r\n",
    "                  'housewife',  'doorwoman',  'doorwomen',  'duchess',  'duchesses',  'empress',  'empresses',  'fem',  'female',  'females',  \r\n",
    "                  'fiancee',  'firewoman',  'firewomen',  'fisherwoman',  'fisherwomen',  'forewoman',  'forewomen',  'freshwoman',  'freshwomen',  \r\n",
    "                  'gal',  'galpal',  'gals',  'garbagewoman',  'garbagewomen',  'gentleman',  'girl', 'gurl',  'girlfriend',  'girlfriends',  'girls',  \r\n",
    "                  'goddess',  'goddesshead',  'goddesshood',  'goddessliness',  'goddessly',  'godmother',  'granddaughter',  'grandma',  'grandmas',  \r\n",
    "                  'grandmother',  'handywoman',  'handywomen',  'hangwoman',  'hangwomen',  'henchwoman',  'henchwomen',  'her',  'hers','heroine',  \r\n",
    "                  'heroines',  'herself',  'housewife',  'journeywoman',  'journeywomen',  'kinswoman',  'kinswomen',  'klanswoman',  'ladies',  \r\n",
    "                  'ladiez',  'lady',  'lady-romance',  'ladysplain',  'laydeez',  'laywoman',  'laywomen',  \"ma'am\",  'madam', 'madame' ,'madwoman',  \r\n",
    "                  'madwomen',  'maiden',  'mailwoman',  'mailwomen',  'mama',  'marchioness',  'margravine',  'markswoman',  'markswomen',  \r\n",
    "                  'marquise',  'middlewoman',  'middlewomen',  'milkwoman',  'milkwomen',  'miss',  'mistress',  'mom',  'mamma','mawmaw' ,'momma',  \r\n",
    "                  'mommies',  'mommy',  'moms',  'mother',  'mothers', 'mrs',  'ms',  'mum',  'mummy',  'mums',  'niece',  'nieces',  'noblewoman',  \r\n",
    "                  'noblewomen',  'ombudswoman',  'ombudswomen',  'policewoman',  'policewomen',  'postwoman',  'postwomen',  'priestess',  \r\n",
    "                  'priestesses',  'princess',  'princesses',  'prostitute',  'queen',  'queens',  'repairwoman',  'repairwomen',  'saleswoman',  \r\n",
    "                  'saleswomen',  'sandwoman',  'sandwomen',  'servicewoman',  'servicewomen',  'she',  \"she's\",  'showwoman',  'showwomen',  'sis',  \r\n",
    "                  'sistagrammer',  'sistas',  'sister',  'sisters',  'snowwoman',  'spacewoman',  'spacewomen',  'spokeswoman',  'spokeswomen',  \r\n",
    "                  'sportswoman',  'sportswomen',  'stateswoman',  'stateswomen',  'stepmother',  'stepsister',  'superwoman',  'superwomen',  \r\n",
    "                  'unwoman',  'viscountess',  'viscountesses',  'waitress',  'watchwoman',  'watchwomen',  'weatherwoman',  'weatherwomen',  \r\n",
    "                  'widow',  'widows',  'wife',  'wives',  'woman',  'womanhood',  'womankind',  'women',  \"women's\",  'workwoman',  'workwomen', \r\n",
    "                  'missus', 'hooker', 'whore', 'hoe', 'wench', 'harlot', 'womxn', 'womyn'])\r\n",
    "\r\n",
    "# this list of words\r\n",
    "female_bias_words = set(['adorable',  'affair',  'affection',  'affectionate',  'afraid',  'agree',  'angel',  'baby',  'banshee',  'barren',  \r\n",
    "                         'beautiful',  'beauty',  'bikini',  'birth',  'bitch', \"beyotch\", 'feminist',   'bitchfest',  'bitchy',  'body',  'bossy',  \r\n",
    "                         'breast', 'boobs',  'bride',  'bridezilla',  'bubbly', 'calm',  'care',  'caress',  'caring',  'catfight',  'catty',  \r\n",
    "                         'chatty',  'cheat',  'cheer',  'child',  'clotheshorse',  'clucky',  'co-operate',  'cold',  'collab',  'commit',  \r\n",
    "                         'commitment',  'committed',  'communal',  'compassion',  'compassionate',  'conscientious',  'considerate',  'cook',  \r\n",
    "                         'cooking',  'cooperate',  'cougar',  'cries',  'cry', 'coy', 'crazy',  'crying',  'dedicated',  'demure',  'depend',  \r\n",
    "                         'dependable',  'diligent',  'ditzy',  'divorce',  'domestic',  'drama',  'dramatic',  'dress',  'easy',  'emotion',  \r\n",
    "                         'emotiona',  'emotional',  'emotions',  'empath',  'empathetic',  'empathize',  'enthusiast',  'family',  'fear',  'feel',  \r\n",
    "                         'feeling',  'feisty',  'femaleness',  'feminazi',  'feminine',  'fishwife',  'flaky',  'flatter',  'flatterable',  'flirty',  \r\n",
    "                         'frail',  'frigid',  'frumpy',  'gentle', 'gently',  'girlhood',  'girlier',  'girliest',  'girly',  'gossip',  'gossipy',  \r\n",
    "                         'helpful', \"grace\", \"graceful\", \"gracefully\",  'honest',  'hormonal',  'houseproud',  'humourless',  'hysterical',  \r\n",
    "                         'ice queen', \"impatient\", \"patient\",  'inclusive',  'inter-dependen',  'inter-dependence',  'inter-dependent',  \r\n",
    "                         'inter-personal',  'interdependence',  'interdependent',  'interpersonal',  'irrational',  'kid',  'kind',  'kinship', \r\n",
    "                         \"judgy\", \"judgemental\",  'ladylike',  'lie',  'lippy',  'loose',  'love', \"lovely\",  'lover',  'loyal',  'maid',  'makeup',  \r\n",
    "                         'man-eater',  'man-hater',  'marriage',  'married',  'marrigeable',  'marry',  'maternal',  'maternal',  'menstrual',  \r\n",
    "                         'mistress',  'modest',  'modesty',  'moody',  'mousey',  'mumpreneur',  'mumsy',  'nag',  'naked',  'neurotic',  'new born',  \r\n",
    "                         'new-born',  'newborn',  'nurse',  'nurtur',  'nurture',  'nurtures',  'nurturing',  'obedient',  'obey',  'over-sensitive',  \r\n",
    "                         'pleasant',  'polite',  'powerless',  'pregnancy',  'pretty',  'prostitute',  'prude',  'quiet',  'relationship',  'respon',  \r\n",
    "                         'sassy',  'secretary',  'seduce',  'seductive',  'sensitiv',  'sensitive',  'sex',  'sexual',  'sexually',  'share',  \r\n",
    "                         'sharin',  'shop',  'shopping',  'shrew', \"shrewd\", 'single',  'slut',  'slutty',  'soft',  'spinster',  'submissive',  \r\n",
    "                         'supermum',  'support',  'sympath',  'sympathy',  'tactful', \"thin\", \"skinny\", \"voluptuous\", \"curvy\", \"curvaceous\", \r\n",
    "                         \"attractive\", \"slender\", \"round\", \"supple\",  'tease',  'tender', \"tenderness\",  'together',  'tomboy ',  'trollop',  'trust',  \r\n",
    "                         'understand',  'victim',  'virgin',  'vivacious',  'warm',  'weak',  'wedding',  'weight',  'whin',  'whitefemaleness',  \r\n",
    "                         'whore',  'womanlier',  'womanliest',  'womanliness',  'womanly',  'yield', 'ladylike', 'unladylike', 'feminist', 'motherhood'])\r\n",
    "\r\n",
    "# words we are doubtful about: \"model\", \"accuse\", \"teen\", \"young\", \"home\",\"question\", \"date\", \"good\", \"hair\", \"couple\", \"teenage\", \"fail\", \"struggle\",\r\n",
    "\r\n",
    "male_words = set(['hero', 'man',  'men', 'his', 'him', 'he', 'husband', 'father', 'male', 'son', 'god',\r\n",
    "                  'prince', 'king', 'mr', 'sir', 'brother', 'grandfather', 'uncle', 'nephew', 'master', 'patriarch',\r\n",
    "                 'chairman', 'chairmen', \"boy\",\"boyfriend\", \"save\", \"guy\", \"dad\"])\r\n",
    "\r\n",
    "discrimination_words = set(['race', 'caste', 'casteless', 'black', 'SC', 'ST', 'african american', 'white', 'colour', \r\n",
    "                            'color', \"brown\", \"asian\", \"native\", \"racial\", \"minority\", \"ethnic\", \"ethnicity\", \"indian\",\r\n",
    "                            'hindu', \"muslim\", \"chinese\", \"indian\"])\r\n",
    "\r\n",
    "male_bias_words = set([\"active\", \"adventurous\", \"aggression\",'aggressive', \"ambition\", \"assert\", 'assertive',\r\n",
    "                       \"athlete\", 'athletic', \"battle\", \"champion\", \"decisive\", 'head', 'dominate', 'dominant',\r\n",
    "                        \"driven\", 'confident', 'strong', 'force', 'master', 'superior', 'strength', 'bold', \r\n",
    "                       'ambitious', 'power', 'intelligent', 'greedy', 'hostile', 'uncaring', 'logic', 'logical', 'rational',\r\n",
    "                       \"fearless\",'stubborn', 'independent', 'objective', \"charismatic\"])\r\n",
    "\r\n",
    "empowerement_words = set(['chairperson', 'leader','leadership',  'chairwoman', 'minister', 'power','powerful', 'authority', \r\n",
    "                          'queen', 'manager', 'success', 'successful', 'successes', 'career', 'job',\r\n",
    "                         'CEO','CFO', 'chief', 'officer', 'employment', 'employed', 'millionaire', \r\n",
    "                          'wealth', 'wealthy', 'strong', 'strength', 'courage','achievement', 'achievements', \r\n",
    "                          'achieve', 'goal', 'ambition', 'ambitious', 'passionate','passion', 'badass', \r\n",
    "                          'confident', 'confidence', 'breakthrough', \"inspirational\", \"educated\"\r\n",
    "                         'inspiring', 'inspiration', 'inspire', 'empower', 'empowered', 'empowerement',\r\n",
    "                         'genius', 'expert', 'mastery', 'owner', 'businesswoman', 'intelligent', 'smart', \r\n",
    "                          'clever', 'wise', 'worth', 'role model', 'role-model', 'activist', \"pay\", \"work\", \r\n",
    "                          \"business\", \"win\", \"award\", \"appoint\", \"lead\", \"star\", \"boss\", \"dream\",'goddess', \r\n",
    "                          \"actor\", 'queen', \"launch\", \"worker\", \"lawyer\", \"education\", \"director\", \"protester\", \r\n",
    "                          \"protest\", \"governor\", \"survive\", \"stallion\", \"doctor\", \"voice\", \"perfect\", \"author\",\r\n",
    "                          \"mayor\", \"founder\", \"abortion\", \"rise\", \"1st\", \"winner\", \"artist\", \"graduate\", \r\n",
    "                          \"employee\", \"earning\", \"survivor\", \"scientist\", \"equality\", \"equal\", \"deputy\", \r\n",
    "                          \"entrepreneur\", \"survive\", \"parent\"])\r\n",
    "\r\n",
    "politics_words = set([\"trump\", \"biden\", \"kamala\", \"harris\", \"joe\", \"vote\", \"election\", \"president\", \"elect\", \r\n",
    "                      \"state\", \"government\", \"obama\", \"office\", \"campaign\", \"melania\", \"vice\", \"govt\", \"donald\", \r\n",
    "                      \"voter\", \"congress\", \"candidate\", \"political\", \"breonna\", \"taylor\", \"pelosi\", \"democratic\",\r\n",
    "                      \"politic\", \"democrats\", \"republican\", \"ivanka\", \"republicans\", \"hillary\", \"clinton\", \"susan\", \r\n",
    "                      \"collins\", \"warren\", \"rep\", \"rally\", \"debate\", \"senate\", \"washington\", \"speech\", \"presidential\",\r\n",
    "                      \"boris\", \"mandela\"])\r\n",
    "\r\n",
    "violence_words = set([\"find\", \"allegedly\", \"fire\", \"life\", 'violent', 'violence', 'crime', 'rape','rapist', 'raped', 'murder','kill', 'killed','killer',\r\n",
    "                     'murdered', 'murderer', 'attack', 'alleged', 'criminal', 'stab', 'knife', 'gun', 'guns', 'knives',\r\n",
    "                     'blood', 'bloodshed', 'court', 'rage', 'outrage', 'rob', 'steal', 'robber', 'stealer', 'beater', \"beaten\",\r\n",
    "                     'domestic violence', 'aggression', 'aggressor', 'war', 'battle', 'abduction', 'assault', 'assaulted',\r\n",
    "                     'drug', 'abuse', 'child abuse', 'prison', 'fraud', 'human traffic', 'homicide', 'organised crime',\r\n",
    "                     'organized crime', 'genocide', 'fight', 'manslaughter', 'terrorist', 'weapon', 'smuggl', 'shoplift',\r\n",
    "                     'vandalism', 'crime', 'theft', 'penalty', 'prison sentence', 'detained', 'guilty', 'trial',\r\n",
    "                     'defense', 'defend', 'armed', 'jail', 'illegal', 'accomplice',\r\n",
    "                     'alcohol', 'allegation', 'arson', 'bail', 'battery', 'dead', 'death', 'deadly', 'corrupt', 'killer', \r\n",
    "                     'sex crime', 'wanted', \"arrest\", \"police\", \"die\", \"charge\", \"suspect\", \"shoot\", \"sentence\", \"cop\", \"hit\", \r\n",
    "                     \"break\", \"beat\", \"judge\", \"kidnap\", \"law\", \"corruption\", \"gang\", \"suicide\", \"critically injured\", 'harassment',\r\n",
    "                     \"run\", \"crash\", \"hospital\", 'security', \"report\", \"risk\", \"fall\", \"burn\", \"escape\", \"threaten\", \"slam\",\r\n",
    "                     \"gangrape\", \"harass\", \"brutally\", \"drown\", \"justice\", \"hate\", \"racist\", \"allege\", \"lawsuit\", \"injure\", \"racism\", \r\n",
    "                      \"thug\", \"suffer\", \"injury\", \"horror\", \"killing\", \"robbery\", \"plead\", \"wound\", \"kidnapping\", \"convict\", \"shooting\"])\r\n",
    "\r\n",
    "female_bias = female_words.union(female_bias_words)\r\n",
    "male_bias = male_words.union(male_bias_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "# all_hl_words =[]\r\n",
    "# all_marked_words = {}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# all_marked_words =  female_words.union(female_bias_words).union(male_words).union(discrimination_words).union(male_bias_words).union(empowerement_words).union(violence_words)\r\n",
    "# #all_marked_words = ast.literal_eval(all_marked_words)\r\n",
    "# #list(female_bias)\r\n",
    "# all_marked_words = list(all_marked_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# def addedges(list):\r\n",
    "#     edges = []\r\n",
    "#     for i in combinations(list,2):\r\n",
    "#         edges.append(i)\r\n",
    "#     G.add_edges_from(edges) \r\n",
    "#     print('added edge')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "# def usefulwordassociations(list):\r\n",
    "#     new_list = [x for x in list if x in all_marked_words]\r\n",
    "#     return new_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "def clean_hl_words(all_hl_words):\r\n",
    "    for index,value in enumerate(all_hl_words):\r\n",
    "        if len(value) == 1:\r\n",
    "            all_hl_words.remove(value)\r\n",
    "\r\n",
    "    for index,value in enumerate(all_hl_words):\r\n",
    "        if value.isalpha() == False:\r\n",
    "            all_hl_words.remove(value)\r\n",
    "    return all_hl_words       "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "# def cleanwordassociations(list):\r\n",
    "#     new_list = [x for x in list if x.isalpha == False]\r\n",
    "#     new_list = [x for x in new_list if len(x)>1]\r\n",
    "#     return new_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "#G = nx.Graph()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "def createfullnetwork(df, column=\"clean_h1_verbs_adjs\"):\r\n",
    "    G = nx.MultiGraph()\r\n",
    "    all_hl_words = []\r\n",
    "    for i in range(len(df)):\r\n",
    "        word_associations = []\r\n",
    "        all_hl_words += clean_hl_words(df[column][i])\r\n",
    "        \r\n",
    "        #all_hl_words = clean_hl_words(all_hl_words)\r\n",
    "        #word_associations = cleanwordassociations(ast.literal_eval(df['clean_hl_words'][i]))\r\n",
    "        \r\n",
    "        word_associations = df[column][i]\r\n",
    "        \r\n",
    "        #print(type(word_associations))\r\n",
    "        #addedges(word_associations)\r\n",
    "        \r\n",
    "        edges = []\r\n",
    "        for i in combinations(word_associations,2):\r\n",
    "            edges.append(i)\r\n",
    "        G.add_edges_from(edges) \r\n",
    "        \r\n",
    "    #print('added edge')\r\n",
    "    G.add_nodes_from(all_hl_words)\r\n",
    "    return G"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "def createWeightedGraph(g_test):\r\n",
    "    G = nx.Graph()\r\n",
    "    for u,v,data in g_test.edges(data=True):\r\n",
    "        w = data['weight'] if 'weight' in data else 1.0\r\n",
    "        if G.has_edge(u,v):\r\n",
    "            G[u][v]['weight'] += int(w)\r\n",
    "        else:\r\n",
    "            G.add_edge(u, v, weight=w)\r\n",
    "    return G"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "def getNodeFreq(g, freq):\r\n",
    "    nx.set_node_attributes(g, 0, \"frequency\")\r\n",
    "#     nx.set_node_attributes(g, 0, \"perc_freq\")\r\n",
    "    for u in g.nodes(data=True):\r\n",
    "        node = u[0]\r\n",
    "        if len(freq[freq['word']== str(u[0])])>0:\r\n",
    "            g.nodes[str(node)]['frequency'] = int(freq[freq['word']== str(u[0])]['frequency'].values[0])\r\n",
    "#             g.nodes[str(node)]['perc_freq'] = int(freq[freq['word']== str(u[0])]['perc_freq'].values[0]*100000)\r\n",
    "    return g"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "def get_node_themes(g):\r\n",
    "    nx.set_node_attributes(g, \"\", \"theme\")\r\n",
    "    for u in g.nodes(data=True):\r\n",
    "        node = u[0]\r\n",
    "        if node in female_bias:\r\n",
    "             g.nodes[str(node)]['theme'] = \"female_bias\"\r\n",
    "        elif node in male_bias:\r\n",
    "             g.nodes[str(node)]['theme'] = \"male_bias\"\r\n",
    "        elif node in empowerement_words:\r\n",
    "             g.nodes[str(node)]['theme'] = \"empowerment\"\r\n",
    "        elif node in violence_words:\r\n",
    "             g.nodes[str(node)]['theme'] = \"violence\"\r\n",
    "        elif node in politics_words:\r\n",
    "             g.nodes[str(node)]['theme'] = \"politics\"\r\n",
    "        elif node in discrimination_words:\r\n",
    "             g.nodes[str(node)]['theme'] = \"race\"\r\n",
    "        else:\r\n",
    "            g.nodes[str(node)]['theme'] = \"other\"\r\n",
    "            \r\n",
    "    return g"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "def get_edge_themes(g):\r\n",
    "    nx.set_edge_attributes(g, \"\", \"theme\")\r\n",
    "    for u,v,data in g.edges(data=True):\r\n",
    "    #     print(data['theme'])\r\n",
    "    #     print(g[u][v]['weight'])\r\n",
    "        if u in female_bias:\r\n",
    "             g[u][v]['theme'] = \"female_bias\"\r\n",
    "        elif u in male_bias:\r\n",
    "             g[u][v]['theme'] = \"male_bias\"\r\n",
    "        elif u in empowerement_words:\r\n",
    "             g[u][v]['theme'] = \"empowerment\"\r\n",
    "        elif u in violence_words:\r\n",
    "             g[u][v]['theme'] = \"violence\"\r\n",
    "        elif u in politics_words:\r\n",
    "             g[u][v]['theme'] = \"politics\"\r\n",
    "        elif u in discrimination_words:\r\n",
    "             g[u][v]['theme'] = \"race\"\r\n",
    "        else:\r\n",
    "            g[u][v]['theme'] = \"other\"\r\n",
    "    return g"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "def getFinalNetwork(df, freq, edge_filter=2, node_filter=2):\r\n",
    "    # create network\r\n",
    "    g = createfullnetwork(df)\r\n",
    "    # get edge weights\r\n",
    "    g = createWeightedGraph(g)\r\n",
    "    # get node themes\r\n",
    "    g = get_node_themes(g)\r\n",
    "    # get edge themes\r\n",
    "    g = get_edge_themes(g)\r\n",
    "    # get node frequency\r\n",
    "    g = getNodeFreq(g, freq)\r\n",
    "    print(len(g.nodes), \"nodes\", \"and\", len(g.edges), \"edges before reducing\") \r\n",
    "    # remove irrelevant noise\r\n",
    "    ## add irrelevant words here\r\n",
    "#     words = ['say', 'new', 'time', \"south\", \"africa\", \"india\", \"england\", \"united states\", \"american\", \"americans\", \r\n",
    "#              \"UK\", \"get\", \"video\", \"tell\", \"cape\", \"makesefakese\", \"not\", \"no\", \"ask\", \"november\", \"trump\", \"biden\", \r\n",
    "#              \"pandemic\", \"theill\", \"covid\"]\r\n",
    "    \r\n",
    "\r\n",
    "    \r\n",
    "    remove_edges = [(u,v) for u, v, data in g.edges(data=True) if data[\"weight\"] < edge_filter]\r\n",
    "    g.remove_edges_from(remove_edges)\r\n",
    "\r\n",
    "    deg = g.degree()\r\n",
    "    \r\n",
    "    nx.set_node_attributes(g, 0, \"degree\")\r\n",
    "    for node in g.nodes():\r\n",
    "        # we added this one before, just increase the weight by one\r\n",
    "        g.nodes[node]['degree'] = deg[node]\r\n",
    "            \r\n",
    "    remove_nodes = [node for node,degree in dict(g.degree()).items() if degree < node_filter]\r\n",
    "#     remove_nodes = []\r\n",
    "#     remove_nodes = [node[0] for node in g.nodes(data=True) if g.nodes[node[0]][\"perc_freq\"] <= node_filter]\r\n",
    "#     remove_nodes.extend(words)\r\n",
    "    g.remove_nodes_from(remove_nodes)\r\n",
    "    print(len(g.nodes), \"nodes\", \"and\", len(g.edges), \"edges after reducing\")     \r\n",
    "    \r\n",
    "    return g"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "# for node in g_IN.nodes():\r\n",
    "# #     print(g_IN.nodes[node])\r\n",
    "#     print(g_IN.degree()[node])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "def get_country_data(freq, df, country):\r\n",
    "    country_freqs = freq[freq['country']==country].reset_index()\r\n",
    "    country_df = df[df['country']==country].reset_index()\r\n",
    "    return country_df, country_freqs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "def saveNetwork(g, country, net_type=\"all\"):\r\n",
    "    connections = json_graph.node_link_data(g)\r\n",
    "    connections_json = json.dumps(connections, indent = 2) \r\n",
    "    with open(f\"../data/processed/word_connections_{country}_{net_type}.json\", \"w\") as outfile: \r\n",
    "        outfile.write(connections_json) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "# freq = pd.read_csv(\"../data/processed/countries_freq_rapi.csv\")\r\n",
    "\r\n",
    "# country = \"India\" #South Africa, UK, USA, India\r\n",
    "# # words = list(freq[freq['country']!=country].word.values)\r\n",
    "# country_freqs = freq[freq['country']!=country].reset_index()\r\n",
    "# country_df = df[df['country']==country].reset_index()\r\n",
    "ef = 3\r\n",
    "nf = 10\r\n",
    "\r\n",
    "g_IN = getFinalNetwork(df = get_country_data(freq=freq, df=df, country = \"India\")[0], freq = get_country_data(freq=freq, df=df, country = \"India\")[1], edge_filter=ef, node_filter=nf)\r\n",
    "# saveNetwork(g_IN, \"IN\", net_type=\"verbs_adjs\")\r\n",
    "g_USA = getFinalNetwork(df = get_country_data(freq=freq, df=df, country = \"USA\")[0], freq = get_country_data(freq=freq, df=df, country = \"USA\")[1], edge_filter=ef, node_filter=nf)\r\n",
    "# saveNetwork(g_USA, \"USA\", net_type=\"verbs_adjs\")\r\n",
    "g_UK = getFinalNetwork(get_country_data(freq, df, \"UK\")[0], get_country_data(freq, df, \"UK\")[1], edge_filter=ef, node_filter=nf)\r\n",
    "# saveNetwork(g_UK, \"UK\", net_type=\"verbs_adjs\")\r\n",
    "g_SA = getFinalNetwork(get_country_data(freq, df, \"South Africa\")[0], get_country_data(freq, df, \"South Africa\")[1], edge_filter=ef, node_filter=nf)\r\n",
    "# saveNetwork(g_SA, \"SA\", net_type=\"verbs_adjs\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7958 nodes and 78200 edges before reducing\n",
      "229 nodes and 3717 edges after reducing\n",
      "7177 nodes and 73899 edges before reducing\n",
      "234 nodes and 3817 edges after reducing\n",
      "7299 nodes and 81944 edges before reducing\n",
      "256 nodes and 4472 edges after reducing\n",
      "2292 nodes and 8647 edges before reducing\n",
      "11 nodes and 35 edges after reducing\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "saveNetwork(g_IN, \"IN\", net_type=\"verbs_adjs\")\r\n",
    "saveNetwork(g_USA, \"USA\", net_type=\"verbs_adjs\")\r\n",
    "saveNetwork(g_UK, \"UK\", net_type=\"verbs_adjs\")\r\n",
    "saveNetwork(g_SA, \"SA\", net_type=\"verbs_adjs\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "# pd.read_csv(\"../data/processed/countries_freq.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "freq = pd.read_csv(\"../data/processed/countries_freq.csv\")\r\n",
    "# country = \"India\" #South Africa, UK, USA, India\r\n",
    "# # words = list(freq[freq['country']!=country].word.values)\r\n",
    "# country_freqs = freq[freq['country']!=country].reset_index()\r\n",
    "# country_df = df[df['country']==country].reset_index()\r\n",
    "g_IN = getFinalNetwork(df = get_country_data(freq=freq, df=df, country = \"India\")[0], freq = get_country_data(freq=freq, df=df, country = \"India\")[1], edge_filter=10, node_filter=200)\r\n",
    "saveNetwork(g_IN, \"IN_small\")\r\n",
    "g_USA = getFinalNetwork(df = get_country_data(freq=freq, df=df, country = \"USA\")[0], freq = get_country_data(freq=freq, df=df, country = \"USA\")[1], edge_filter=10, node_filter=200)\r\n",
    "saveNetwork(g_USA, \"USA_small\")\r\n",
    "g_UK = getFinalNetwork(get_country_data(freq, df, \"UK\")[0], get_country_data(freq, df, \"UK\")[1], edge_filter=10, node_filter=200)\r\n",
    "saveNetwork(g_UK, \"UK_small\")\r\n",
    "g_SA = getFinalNetwork(get_country_data(freq, df, \"South Africa\")[0], get_country_data(freq, df, \"South Africa\")[1], edge_filter=10, node_filter=200)\r\n",
    "saveNetwork(g_SA, \"SA_small\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13181 nodes and 215872 edges before reducing\n",
      "91 nodes and 175 edges after reducing\n",
      "15094 nodes and 294534 edges before reducing\n",
      "92 nodes and 157 edges after reducing\n",
      "16674 nodes and 282702 edges before reducing\n",
      "87 nodes and 142 edges after reducing\n",
      "10476 nodes and 104227 edges before reducing\n",
      "83 nodes and 83 edges after reducing\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "grey_nodes_IN = [node[0] for node in g_IN.nodes(data=True) if g_IN.nodes[node[0]][\"theme\"] == \"other\"]\r\n",
    "grey_nodes_USA = [node[0] for node in g_USA.nodes(data=True) if g_USA.nodes[node[0]][\"theme\"] == \"other\"]\r\n",
    "grey_nodes_UK = [node[0] for node in g_UK.nodes(data=True) if g_UK.nodes[node[0]][\"theme\"] == \"other\"]\r\n",
    "grey_nodes_SA = [node[0] for node in g_SA.nodes(data=True) if g_SA.nodes[node[0]][\"theme\"] == \"other\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "g_SA.nodes['rape']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'theme': 'violence', 'frequency': 250, 'perc_freq': 946}"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "freq[freq['word']== \"rape\"]['perc_freq'].values[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.009463244757362405"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "0.0005*100000"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "freq[freq['country']==\"USA\"]['perc_freq']*100000"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10415    1431.440078\n",
       "10416    1363.009165\n",
       "10417    1312.527344\n",
       "10418    1020.854601\n",
       "10419     701.136402\n",
       "            ...     \n",
       "29855       1.121818\n",
       "29856       1.121818\n",
       "29857       1.121818\n",
       "29858       1.121818\n",
       "29859       1.121818\n",
       "Name: perc_freq, Length: 19445, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "len([node[0] for node in g_USA.nodes(data=True) if g_USA.nodes[node[0]][\"perc_freq\"] <= 0.0005])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'perc_freq'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-865dbd65cf79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mg_USA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg_USA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"perc_freq\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-91-865dbd65cf79>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mg_USA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg_USA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"perc_freq\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'perc_freq'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len([(u,v) for u, v, data in g_IN.edges(data=True) if data[\"weight\"] <= 4])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "remove_edges = [(u,v) for u, v, data in g.edges(data=True) if data[\"weight\"] <= 3]\r\n",
    "remove_nodes = [node[0] for node in g.nodes(data=True) if g.nodes[node[0]][\"frequency\"] <= 5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "source": [
    "# g_SA = getNodeFreq(g_SA, country_freqs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "source": [
    "# g_SA.edges['man', \"rape\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "source": [
    "# g_SA.nodes['man']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# len([node[0]  for node in g_SA.nodes(data=True) if g_SA.nodes[node[0]][\"frequency\"] <= 5])\r\n",
    "len([(u,v) for u, v, data in g_IN.edges(data=True) if data[\"weight\"] <= 5])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1331"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# remove_edges = [(u,v) for u, v, data in g_IN.edges(data=True) if data[\"weight\"] <= 3]\r\n",
    "remove_nodes = [node[0] for node in g_IN.nodes(data=True) if g_IN.nodes[node[0]][\"frequency\"] <= 10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "g_SA.remove_edges_from(remove_edges)\r\n",
    "g_SA.remove_nodes_from(remove_nodes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "len(remove_edges)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "209850"
      ]
     },
     "metadata": {},
     "execution_count": 220
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "len(g_IN.edges)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "g = createfullnetwork(df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "34311\n",
      "1173780\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "frequency = []\r\n",
    "nx.set_node_attributes(G, frequency, \"frequency\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for u in g_weighted.nodes(data=True):\r\n",
    "    node = u[0]\r\n",
    "    if len(freq[freq['word']== str(u[0])])>0:\r\n",
    "        g_weighted.nodes[str(node)]['frequency'] = int(freq[freq['word']== str(u[0])]['frequency'].values[0])\r\n",
    "   # print(u[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "source": [
    "g_weighted.nodes['die']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'frequency': 937}"
      ]
     },
     "metadata": {},
     "execution_count": 208
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "edges_test = nx.to_pandas_edgelist(G)\r\n",
    "edges_test[edges_test['weight']>1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         source    target  weight\n",
       "0         crown         v       2\n",
       "6         crown        di       2\n",
       "7         crown     diana       3\n",
       "65           le        di       2\n",
       "66           le     diana       2\n",
       "...         ...       ...     ...\n",
       "12603    supply   problem       2\n",
       "12606   problem    reckon       2\n",
       "12868   monthly  archives       4\n",
       "12871   monthly   archive       3\n",
       "13092  archives       tag       2\n",
       "\n",
       "[795 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>crown</td>\n",
       "      <td>v</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>crown</td>\n",
       "      <td>di</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>crown</td>\n",
       "      <td>diana</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>le</td>\n",
       "      <td>di</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>le</td>\n",
       "      <td>diana</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12603</th>\n",
       "      <td>supply</td>\n",
       "      <td>problem</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12606</th>\n",
       "      <td>problem</td>\n",
       "      <td>reckon</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12868</th>\n",
       "      <td>monthly</td>\n",
       "      <td>archives</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12871</th>\n",
       "      <td>monthly</td>\n",
       "      <td>archive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13092</th>\n",
       "      <td>archives</td>\n",
       "      <td>tag</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Node attributes to add -\r\n",
    "1. which dictionary it belongs to\r\n",
    "2. frequency of node\r\n",
    "3. frequency of edge"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "connections = json_graph.node_link_data(g_weighted)\r\n",
    "connections_json = json.dumps(connections, indent = 2) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "connections"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "with open(\"../data/processed/word_connections.json\", \"w\") as outfile: \r\n",
    "    outfile.write(connections_json) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "g.number_of_edges('diana', 'crown')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "len(g_filtered.nodes)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32592"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "df['useful_word_associations'] = np.empty((len(df), 0)).tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# import time\r\n",
    "# time\r\n",
    "# def calcfilterednetwork(df):\r\n",
    "#     all_hl_words = []\r\n",
    "#     all_useful_hl_words = []\r\n",
    "#     for i in range(len(df)):\r\n",
    "#         all_hl_words += getusefulnodes(ast.literal_eval(df['clean_hl_words'][i]))\r\n",
    "#         df['useful_word_associations'].iloc[i] = usefulwordassociations(ast.literal_eval(df['clean_hl_words'][i]))\r\n",
    "        \r\n",
    "#         if len(df['useful_word_associations'][i])>=1:\r\n",
    "#             addedges(df['useful_word_associations'][i])\r\n",
    "            \r\n",
    "            \r\n",
    "#     G.add_nodes_from(all_hl_words)\r\n",
    "    \r\n",
    "#     print(len(all_hl_words))  \r\n",
    "#     return G"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "#g_filtered = calcfilterednetwork(df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Themes addition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "def read_json_file(filename):\r\n",
    "    with open(filename) as f:\r\n",
    "        js_graph = json.load(f)\r\n",
    "    return json_graph.node_link_graph(js_graph)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "g = read_json_file(\"../data/processed/word_connections_4.json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "theme = \"\"\r\n",
    "nx.set_node_attributes(g, theme, \"theme\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "def get_node_themes(g):\r\n",
    "    for u in g.nodes(data=True):\r\n",
    "        node = u[0]\r\n",
    "        if node in female_bias:\r\n",
    "             g.nodes[str(node)]['theme'] = \"female_bias\"\r\n",
    "        elif node in male_bias:\r\n",
    "             g.nodes[str(node)]['theme'] = \"male_bias\"\r\n",
    "        elif node in empowerement_words:\r\n",
    "             g.nodes[str(node)]['theme'] = \"empowerment\"\r\n",
    "        elif node in violence_words:\r\n",
    "             g.nodes[str(node)]['theme'] = \"violence\"\r\n",
    "        elif node in politics_words:\r\n",
    "             g.nodes[str(node)]['theme'] = \"politics\"\r\n",
    "        elif node in discrimination_words:\r\n",
    "             g.nodes[str(node)]['theme'] = \"race\"\r\n",
    "        else:\r\n",
    "            g.nodes[str(node)]['theme'] = \"other\"\r\n",
    "            \r\n",
    "    return g\r\n",
    "\r\n",
    "get_node_themes(g)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x1ad8a9116a0>"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "g.nodes['police']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'frequency': 954, 'theme': 'violence'}"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "g.remove_node(\"say\")\r\n",
    "g.remove_node(\"new\")\r\n",
    "g.remove_node(\"time\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "nx.set_edge_attributes(g, theme, \"theme\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "def get_edge_themes(g):\r\n",
    "    for u,v,data in g.edges(data=True):\r\n",
    "    #     print(data['theme'])\r\n",
    "    #     print(g[u][v]['weight'])\r\n",
    "        if u in female_bias:\r\n",
    "             g[u][v]['theme'] = \"female_bias\"\r\n",
    "        elif u in male_bias:\r\n",
    "             g[u][v]['theme'] = \"male_bias\"\r\n",
    "        elif u in empowerement_words:\r\n",
    "             g[u][v]['theme'] = \"empowerment\"\r\n",
    "        elif u in violence_words:\r\n",
    "             g[u][v]['theme'] = \"violence\"\r\n",
    "        elif u in politics_words:\r\n",
    "             g[u][v]['theme'] = \"politics\"\r\n",
    "        elif u in discrimination_words:\r\n",
    "             g[u][v]['theme'] = \"race\"\r\n",
    "        else:\r\n",
    "            g[u][v]['theme'] = \"other\"\r\n",
    "            \r\n",
    "get_edge_themes(g)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "g.edges[\"joe\", \"biden\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'weight': 173.0, 'theme': 'politics'}"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "connections = json_graph.node_link_data(g)\n",
    "connections_json = json.dumps(connections, indent = 2) \n",
    "with open(\"../data/processed/word_connections_4_themes.json\", \"w\") as outfile: \n",
    "    outfile.write(connections_json) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "remove = [(u,v) for u, v, data in g.edges(data=True) if data[\"weight\"] <= 5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "len(remove)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "83723"
      ]
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "len(g.edges)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "93642"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "g.remove_edges_from(remove)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "len(g.edges)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9919"
      ]
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "connections = json_graph.node_link_data(g)\n",
    "connections_json = json.dumps(connections, indent = 2) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "with open(\"../data/processed/word_connections_4_themes_filtered.json\", \"w\") as outfile: \n",
    "    outfile.write(connections_json) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Countries Networks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "source": [
    "g = read_json_file(\"../data/processed/word_connections_4.json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "freq = pd.read_csv(\"../data/processed/countries_freq.csv\")\n",
    "country = \"South Africa\"\n",
    "words = list(freq[freq['country']!=country].word.values)\n",
    "country_freqs = freq[freq['country']!=country]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "source": [
    "len(g.nodes)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2032"
      ]
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# g.remove_nodes_from(words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# len(g.nodes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "nx.set_node_attributes(g, 0, \"frequency\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "g.nodes['die']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'frequency': 0}"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# get specific country frequencies\n",
    "for u in g.nodes(data=True):\n",
    "    node = u[0]\n",
    "    if len(country_freqs[country_freqs['word']== str(u[0])])>0:\n",
    "        g.nodes[str(node)]['frequency'] = int(freq[freq['word']== str(u[0])]['frequency'].values[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "g.nodes['die']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'frequency': 143}"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}