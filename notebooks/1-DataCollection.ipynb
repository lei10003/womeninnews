{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsrooms = pd.read_csv('../data/raw/newsrooms.csv')\n",
    "newsrooms.columns = ['site', 'monthly_visits', 'country', 'country_of_pub']\n",
    "newsrooms.dropna(inplace=True)\n",
    "newsrooms = newsrooms.drop_duplicates(subset = 'site')\n",
    "newsrooms = newsrooms.sort_values(by=['country_of_pub', 'monthly_visits'], ascending=False).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dfs with sites\n",
    "UK = newsrooms[newsrooms['country_of_pub']=='UK'].reset_index().drop('index', axis=1)\n",
    "USA = newsrooms[newsrooms['country_of_pub']=='USA'].reset_index().drop('index', axis=1)\n",
    "India = newsrooms[newsrooms['country_of_pub']=='India'].reset_index().drop('index', axis=1)\n",
    "SA = newsrooms[newsrooms['country_of_pub']=='South Africa'].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import re\n",
    "from random import randint #avoid throttling by not sending too many requests one after the other\n",
    "from warnings import warn\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(url):\n",
    "    '''Send request and sleep if a connection error comes up.'''\n",
    "    # send request\n",
    "    try:\n",
    "        sleep(90)\n",
    "        r1 = requests.get(url)\n",
    "    except :\n",
    "        print(\"Encountered an error, F*** YOU GOOGLE, sleeping . . .\")\n",
    "        sleep(120)\n",
    "        try:\n",
    "            r1 = requests.get(url)\n",
    "        except :\n",
    "            print(\"Encountered an error, F*** YOU GOOGLE, sleeping . . .\")\n",
    "            sleep(240)\n",
    "            try:\n",
    "                r1 = requests.get(url)\n",
    "            except :\n",
    "                print(\"Encountered an error, F*** YOU GOOGLE, sleeping . . .\")\n",
    "                sleep(300)\n",
    "                r1 = requests.get(url)\n",
    "    # get html content\n",
    "    coverpage = r1.content\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "    \n",
    "    return soup1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "month_ranges = [[[1, 1, year], [31, 1, year]], \n",
    "                [[1, 2, year], [28, 2, year]],\n",
    "                [[1, 3, year], [31, 3, year]],\n",
    "                [[1, 4, year], [30, 4, year]],\n",
    "                [[1, 5, year], [31, 5, year]],\n",
    "                [[1, 6, year], [30, 6, year]],\n",
    "                [[1, 7, year], [31, 7, year]],\n",
    "                [[1, 8, year], [31, 8, year]],\n",
    "                [[1, 9, year], [30, 9, year]],\n",
    "                [[1, 10, year], [31, 10, year]],\n",
    "                [[1, 11, year], [30, 11, year]],\n",
    "                [[1, 12, year], [31, 12, year]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://www.google.com/search?q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:drudgereport.com&tbs=sbd:1,cdr:1,cd_min:1/1/2020,cd_max:1/31/2020&tbm=nws&sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&ei=O9mNYJ-hE_LR0PEP6dqaoAU&start=0&sa=N&ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&biw=2512&bih=1298&dpr=1.5'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import html\n",
    "import html.parser    \n",
    "import xml.sax.saxutils as saxutils\n",
    "import urllib.parse\n",
    "from urllib.parse import urlencode\n",
    "# from w3lib.html import replace_entities\n",
    "html.unescape(\"https://www.google.com/search?q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:drudgereport.com&amp;tbs=sbd:1,cdr:1,cd_min:1/1/2020,cd_max:1/31/2020&amp;tbm=nws&amp;sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&amp;ei=O9mNYJ-hE_LR0PEP6dqaoAU&amp;start=0&amp;sa=N&amp;ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&amp;biw=2512&amp;bih=1298&amp;dpr=1.5\")\n",
    "# html.unescape(\"++site:drudgereport.com&amp;tbs=sbd:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.google.com/search?q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:drudgereport.com&tbs=sbd:1,cdr:1,cd_min:1/1/2020,cd_max:1/31/2020&tbm=nws&sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&ei=O9mNYJ-hE_LR0PEP6dqaoAU&start=0&sa=N&ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&biw=2512&bih=1298&dpr=1.5'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"https://www.google.com/search?q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:drudgereport.com&amp;tbs=sbd:1,cdr:1,cd_min:1/1/2020,cd_max:1/31/2020&amp;tbm=nws&amp;sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&amp;ei=O9mNYJ-hE_LR0PEP6dqaoAU&amp;start=0&amp;sa=N&amp;ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&amp;biw=2512&amp;bih=1298&amp;dpr=1.5\"\n",
    "t.replace(r\"&amp;\", r\"&\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.google.com/search?q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:drudgereport.com&tbs=sbd:1,cdr:1,cd_min:1/1/2020,cd_max:1/31/2020&tbm=nws&sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&ei=O9mNYJ-hE_LR0PEP6dqaoAU&start=0&sa=N&ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&biw=2512&bih=1298&dpr=1.5'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unescape(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.google.com/search?q=women | woman | girl | girls | female | females | lady | ladies | she | her  site:drudgereport.com&amp;tbs=sbd:1,cdr:1,cd_min:1/1/2020,cd_max:1/31/2020&amp;tbm=nws&amp;sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&amp;ei=O9mNYJ-hE_LR0PEP6dqaoAU&amp;start=0&amp;sa=N&amp;ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&amp;biw=2512&amp;bih=1298&amp;dpr=1.5'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.parse.unquote_plus(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 day ago'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl = soup.find_all('div', class_='ZINbbc xpd O9g5cc uUPGi')\n",
    "hl[1].span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies = {\n",
    "    \"http\": 'http://140.227.66.105:58888', \n",
    "    \"https\": 'http://140.227.66.105:58888'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "import requests\n",
    "requests.utils.get_environ_proxies(\"www.nytimes.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.google.com/search?q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:nytimes.com&tbs=sbd:1&tbm=nws&tbas=0&source=lnt&sa=X&ved=0ahUKEwiEhOSI6bLwAhVHDewKHXQuCcoQpwUIKA&biw=982&bih=698&dpr=1.25\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "#             \"q\":\"women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:ksl.com\",\n",
    "            \"q\":\"women | woman | girl | girls | female | females | lady | ladies | she | her++site:ksl.com\",\n",
    "            \"tbs\":\"sbd:1,cdr:1,cd_min:1/1/2020,cd_max:1/31/2020\",\n",
    "            \"tbm\":\"nws\",\n",
    "            \"sxsrf\":\"ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324\",\n",
    "            \"ei\":\"O9mNYJ-hE_LR0PEP6dqaoAU\",\n",
    "            \"start\":10,\n",
    "            \"sa\":\"N\",\n",
    "            \"ved\":\"0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB\",\n",
    "            \"biw\":2512,\n",
    "            \"bih\":1298,\n",
    "            \"dpr\":1.5\n",
    "}\n",
    "\n",
    "params_str = urllib.parse.urlencode(params, safe=':+')\n",
    "\n",
    "r = requests.get(\"https://www.google.com/search?\", params=params_str)\n",
    "\n",
    "# q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:ksl.com&\n",
    "# tbs=sbd:1,cdr:1,cd_min:2/1/2020,cd_max:2/28/2020&\n",
    "# tbm=nws&\n",
    "# sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&\n",
    "# ei=O9mNYJ-hE_LR0PEP6dqaoAU&\n",
    "# start=10&\n",
    "# sa=N&\n",
    "# ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&\n",
    "# biw=2512&\n",
    "# bih=1298&\n",
    "# dpr=1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverpage = r.content\n",
    "soup1 = BeautifulSoup(coverpage, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1.find_all('div', class_='ZINbbc xpd O9g5cc uUPGi')[5].span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unescape(s):\n",
    "#     s = s.replace(\"&lt;\", \"<\")\n",
    "#     s = s.replace(\"&gt;\", \">\")\n",
    "#     # this has to be last:\n",
    "#     s = s.replace(\"&amp;\", \"&\")\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "\n",
    "# base_url = 'https://www.google.com/search?'\n",
    "# query = 'q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:ksl.com&tbs=sbd:1,cdr:1,cd_min:1/1/2020,cd_max:1/31/2020&tbm=nws&sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&ei=O9mNYJ-hE_LR0PEP6dqaoAU&start=10&sa=N&ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&biw=2512&bih=1298&dpr=1.5'\n",
    "# url = base_url + query\n",
    "\n",
    "# response = urlopen(url)\n",
    "# data = response.read()\n",
    "# # response data valid\n",
    "\n",
    "# print(response.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import Request, urlopen\n",
    "\n",
    "# req = Request(url, headers={'User-Agent': 'Chrome'})\n",
    "# webpage = urlopen(req).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup1 = BeautifulSoup(webpage, 'html5lib')\n",
    "# soup1.find_all('div', class_='ZINbbc xpd O9g5cc uUPGi')[3].span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paramaters(soup1, urls, headlines, subtitles, times, scrape_dates, websites):\n",
    "\n",
    "    coverpage_news = soup1.find_all('div', class_='ZINbbc xpd O9g5cc uUPGi')\n",
    "    if len(coverpage_news)>0:\n",
    "        # get info for each of 10 google results\n",
    "        for i in range(len(coverpage_news)):\n",
    "            # url\n",
    "            url = coverpage_news[i].a['href'][7:]\n",
    "            urls.append(url)\n",
    "            # headline\n",
    "            headline = coverpage_news[i].a.text\n",
    "            headlines.append(headline)\n",
    "            # subtitle\n",
    "            subtitle = coverpage_news[i].text.split('�')[1].strip().strip('\"').strip('.')\n",
    "            subtitles.append(subtitle)\n",
    "            # time\n",
    "            time = coverpage_news[i].span.text\n",
    "            times.append(time)\n",
    "            #print(time)\n",
    "            # scrape date\n",
    "            scrape_date = datetime.datetime.now().strftime(\"%d/%m/%Y\")\n",
    "            scrape_dates.append(scrape_date)\n",
    "            # website\n",
    "            websites.append(site)\n",
    "        # keep track\n",
    "        print(str(start + 10) + \" results of site \" + str(site) + \" scraped successfully!\")\n",
    "    #else:\n",
    "     #   break\n",
    "\n",
    "    return urls, headlines, subtitles, times, scrape_dates, websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\sahit\\\\PythonProjects\\\\Data Visualization\\\\womeninnews\\\\notebooks'"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/raw2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n",
      "Collecting headlines for drudgereport.com from [1, 1, 2020] to [31, 1, 2020]\n",
      "Scraping page 1.0 of drudgereport.com google news results, for [1, 1, 2020]  to  [31, 1, 2020]\n",
      "https://www.google.com/search?q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:drudgereport.com&tbs=sbd:1,cdr:1,cd_min:1/1/2020,cd_max:1/31/2020&tbm=nws&sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&ei=O9mNYJ-hE_LR0PEP6dqaoAU&start=0&sa=N&ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&biw=2512&bih=1298&dpr=1.5\n",
      "unusual traffic error\n",
      "Scraping page 2.0 of drudgereport.com google news results, for [1, 1, 2020]  to  [31, 1, 2020]\n",
      "https://www.google.com/search?q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:drudgereport.com&tbs=sbd:1,cdr:1,cd_min:1/1/2020,cd_max:1/31/2020&tbm=nws&sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&ei=O9mNYJ-hE_LR0PEP6dqaoAU&start=10&sa=N&ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&biw=2512&bih=1298&dpr=1.5\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-0b0388f30573>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;31m# check whether google is trying to redirect to captcha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"Our systems have detected unusual traffic from your computer network\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m                 \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unusual traffic error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0msoup1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop\n",
    "%time\n",
    "from time import sleep\n",
    "from random import randint\n",
    "# Country: \n",
    "query = ['women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her']\n",
    "# sites = list(USA.site.unique().flatten())[10:12]\n",
    "sites = list(USA.site.unique().flatten())\n",
    "# starting_result = range(0, 310, 10)\n",
    "#starting_result = [0, 10, 20]\n",
    "starting_result = [0, 10, 20]\n",
    "\n",
    "\n",
    "for site in sites[2:10]:\n",
    "    \n",
    "    urls = []\n",
    "    headlines = []\n",
    "    subtitles = []\n",
    "    times = []\n",
    "    scrape_dates = []\n",
    "    websites = []\n",
    "\n",
    "    for i in range(len(month_ranges[0:1])):\n",
    "        start_date = month_ranges[i][0]\n",
    "        start_day = month_ranges[i][0][0]\n",
    "        start_month = month_ranges[i][0][1]\n",
    "        start_year = month_ranges[i][0][2]\n",
    "\n",
    "        end_day = month_ranges[i][1][0]\n",
    "        end_month = month_ranges[i][1][1]\n",
    "        end_year = month_ranges[i][1][2]\n",
    "        end_date = month_ranges[i][1]\n",
    "    \n",
    "        print(\"Collecting headlines for\", site, \"from\", start_date, \"to\", end_date)\n",
    "        \n",
    "        for start in starting_result:\n",
    "            # keep track\n",
    "#             print(\"Scraping page\", str((start+10)/10), \"of\", site, \"google news results.\")\n",
    "            print(\"Scraping page\", str((start+10)/10), \"of\", site, \"google news results, for\", start_date, \" to \", end_date)\n",
    "            # change url\n",
    "            url = html.unescape(f'https://www.google.com/search?q={query[0]}++site:{site}&tbs=sbd:1,cdr:1,cd_min:{start_month}/{start_day}/{start_year},cd_max:{end_month}/{end_day}/{end_year}&tbm=nws&sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&ei=O9mNYJ-hE_LR0PEP6dqaoAU&start={start}&sa=N&ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&biw=2512&bih=1298&dpr=1.5')\n",
    "            print(str(url))\n",
    "            # sleep\n",
    "            sleep(randint(100,120))\n",
    "            soup1 = send_request(url)\n",
    "            # check whether google is trying to redirect to captcha\n",
    "            if \"Our systems have detected unusual traffic from your computer network\" in str(soup1):\n",
    "                sleep(120)\n",
    "                print(\"unusual traffic error\")\n",
    "                soup1 = send_request(url)\n",
    "            # continue\n",
    "            urls, headlines, subtitles, times, scrape_dates, websites = extract_paramaters(soup1, urls, headlines, subtitles, times, scrape_dates, websites)\n",
    "                \n",
    "        # dictionary of results\n",
    "        headlines_dict = {\"url\": urls, \"headline\": headlines,  \"subtitle\": subtitles, \"time\": times, \"scrape_date\": scrape_dates, \"site\": websites}\n",
    "        # df of results\n",
    "        headlines_df = pd.DataFrame.from_dict(headlines_dict)\n",
    "        number_of_articles = len(headlines_df)\n",
    "        # save results to csv\n",
    "        headlines_df.to_csv(f'../data/raw2/{number_of_articles}_{site}_month{i}_articles.csv')\n",
    "        # print final statement\n",
    "        print(\"Saved data from \" + str(number_of_articles) + \" articles of \"  + str(site) + \" to .csv!\")\n",
    "        \n",
    "    print(\"Scrape complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_dict = {\"url\": urls, \"headline\": headlines,  \"subtitle\": subtitles, \"time\": times, \"scrape_date\": scrape_dates, \"site\": websites}\n",
    "# df of results\n",
    "headlines_df = pd.DataFrame.from_dict(headlines_dict)\n",
    "number_of_articles = len(headlines_df)\n",
    "# save results to csv\n",
    "headlines_df.to_csv(f'../data/{number_of_articles}_{site}_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n",
      "HTTP Error 429: Too Many Requests\n"
     ]
    }
   ],
   "source": [
    "from GoogleNews import GoogleNews\n",
    "# from newspaper import Article\n",
    "import pandas as pd\n",
    "\n",
    "googlenews=GoogleNews(start='02/01/2020',end='03/31/2020')\n",
    "googlenews.search('women|woman|girl|girls|female|females|lady|ladies|she|her site:nbcnews.com')\n",
    "\n",
    "for i in range(1,30):\n",
    "    #sleep(randint(90,100))\n",
    "    try:\n",
    "        googlenews.getpage(i)\n",
    "    except (ConnectionError, OSError, TimeoutError) as e:\n",
    "        print(\"Encountered an error, F*** YOU GOOGLE, sleeping . . .\")\n",
    "        sleep(90)\n",
    "        googlenews.getpage(i)\n",
    "\n",
    "    except \"HTTP Error 429: Too Many Requests\" as e:\n",
    "        print(\"Encountered too many requests error, F*** YOU GOOGLE, sleeping . . .\")\n",
    "        sleep(randint(1,10))\n",
    "        googlenews.getpage(i)\n",
    "    except \"NoneType' object is not iterable\" as e:\n",
    "        print(\"This month does not have 30 pages worth of data\")\n",
    "    result=googlenews.result()\n",
    "    df=pd.DataFrame(result)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['women|woman|girl|girls|female|females|lady|ladies|she|her']\n",
    "# sites = list(USA.site.unique().flatten())[10:12]\n",
    "sites = list(USA.site.unique().flatten())\n",
    "# starting_result = range(0, 310, 10)\n",
    "starting_result = [0, 10, 20]\n",
    "\n",
    "\n",
    "for site in sites[1:3]:\n",
    "\n",
    "    for i in range(len(month_ranges[0:2])):\n",
    "        start_date = month_ranges[i][0]\n",
    "        start_day = month_ranges[i][0][0]\n",
    "        start_month = month_ranges[i][0][1]\n",
    "        start_year = month_ranges[i][0][2]\n",
    "\n",
    "        end_day = month_ranges[i][1][0]\n",
    "        end_month = month_ranges[i][1][1]\n",
    "        end_year = month_ranges[i][1][2]\n",
    "        end_date = month_ranges[i][1]\n",
    "    \n",
    "        print(\"Collecting headlines for\", site, \"from\", start_date, \"to\", end_date)\n",
    "        \n",
    "        googlenews=GoogleNews(start=f'{start_month}/{start_day}/{start_year}', end=f'{end_month}/{end_day}/{end_year}')\n",
    "        googlenews.search(f'{query[0]} site:{site}')\n",
    "\n",
    "        for i in range(1,30):\n",
    "            sleep(randint(1,10))\n",
    "#             print(\"page\", i, \"scraped!\")\n",
    "            googlenews.getpage(i)\n",
    "            result=googlenews.result()\n",
    "            df=pd.DataFrame(result)\n",
    "            \n",
    "        df.to_csv(f'../data/raw_2/{site}_{start_date}_{end_date}.csv')\n",
    "        # print final statement\n",
    "        number_of_articles = len(df)\n",
    "        print(\"Saved data from \" + str(number_of_articles) + \" articles of \"  + str(site) + \" to .csv!\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Her Dream of Becoming a Doctor Turned Into a Nightmare, and a Movie'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.theblaze.com/news/leftist-woman-spits-in-face-of-cop&sa=U&ved=2ahUKEwj_2pujuo7tAhUMI6wKHaZdAtUQxfQBMAl6BAgCEAE&usg=AOvVaw2TSZgMP4nTF5XNFsvmKzCh'"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverpage_news[i].a['href'][7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverpage_news = soup1.find_all('div', class_='ZINbbc xpd O9g5cc uUPGi')\n",
    "# coverpage_news = soup1.find_all('div', class_='kCrYT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = random.choice(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_dict = {\"url\": [url], \"headline\": [headline],  \"subtitle\": [subtitle], \"time\": [time], \"scrape_date\": [scrape_date], \"site\":[site]}\n",
    "headlines_df = pd.DataFrame.from_dict(headlines_dict)\n",
    "number_of_articles = len(headlines_df)\n",
    "headlines_df.to_csv(f'../data/{number_of_articles}_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19-year-old woman ends life in UP''s Bulandshahr, blames three men in suicide noteOutlook India\n",
      "https://www.outlookindia.com/newsscroll/19yearold-woman-ends-life-in-ups-bulandshahr-blames-three-men-in-suicide-note/1977017&sa=U&ved=2ahUKEwiM16CdxY7tAhVDXawKHd_cBnU4ChDF9AEwB3oECAAQAQ&usg=AOvVaw1n2cj0aRpqzPcYPDnrIMth\n",
      "2 giorni fa\n",
      "Bulandshahr (UP), Nov 16 (PTI) A 19-year-old woman allegedly died by suicide in a ... She left behind a suicide note blaming three people for her death. ... and statement) of the CrPC, the girl completely denied the incident\n",
      "19/11/2020\n"
     ]
    }
   ],
   "source": [
    "print(headline)\n",
    "print(url)\n",
    "print(time)\n",
    "print(subtitle)\n",
    "print(scrape_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup1.prettify())\n",
    "i = 7\n",
    "url = coverpage_news[i].a['href'][7:]\n",
    "headline = coverpage_news[i].a.text\n",
    "time = coverpage_news[i].span.text\n",
    "subtitle = coverpage_news[i].text.split('�')[1].strip().strip('\"').strip('.')\n",
    "scrape_date = datetime.datetime.now().strftime(\"%d/%m/%Y\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0c377a3a05ba1d31fd00218651f72b0a27b3c45e2b83cef10397a5da0ee866dc9",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}