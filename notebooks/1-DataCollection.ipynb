{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsrooms = pd.read_csv('../data/raw/newsrooms.csv')\n",
    "newsrooms.columns = ['site', 'monthly_visits', 'country', 'country_of_pub']\n",
    "newsrooms.dropna(inplace=True)\n",
    "newsrooms = newsrooms.drop_duplicates(subset = 'site')\n",
    "newsrooms = newsrooms.sort_values(by=['country_of_pub', 'monthly_visits'], ascending=False).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dfs with sites\n",
    "UK = newsrooms[newsrooms['country_of_pub']=='UK'].reset_index().drop('index', axis=1)\n",
    "USA = newsrooms[newsrooms['country_of_pub']=='USA'].reset_index().drop('index', axis=1)\n",
    "India = newsrooms[newsrooms['country_of_pub']=='India'].reset_index().drop('index', axis=1)\n",
    "SA = newsrooms[newsrooms['country_of_pub']=='South Africa'].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import re\n",
    "from random import randint #avoid throttling by not sending too many requests one after the other\n",
    "from warnings import warn\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "import numpy as np\n",
    "import datetime\n",
    "import html\n",
    "import html.parser    \n",
    "import xml.sax.saxutils as saxutils\n",
    "import urllib.parse\n",
    "from urllib.parse import urlencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(url):\n",
    "    '''Send request and sleep if a connection error comes up.'''\n",
    "    # send request\n",
    "    try:\n",
    "        sleep(90)\n",
    "        r1 = requests.get(url)\n",
    "    except :\n",
    "        print(\"Encountered an error, F*** YOU GOOGLE, sleeping . . .\")\n",
    "        sleep(120)\n",
    "        try:\n",
    "            r1 = requests.get(url)\n",
    "        except :\n",
    "            print(\"Encountered an error, F*** YOU GOOGLE, sleeping . . .\")\n",
    "            sleep(240)\n",
    "            try:\n",
    "                r1 = requests.get(url)\n",
    "            except :\n",
    "                print(\"Encountered an error, F*** YOU GOOGLE, sleeping . . .\")\n",
    "                sleep(300)\n",
    "                r1 = requests.get(url)\n",
    "    # get html content\n",
    "    coverpage = r1.content\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "    \n",
    "    return soup1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "month_ranges = [[[1, 1, year], [31, 1, year]], \n",
    "                [[1, 2, year], [28, 2, year]],\n",
    "                [[1, 3, year], [31, 3, year]],\n",
    "                [[1, 4, year], [30, 4, year]],\n",
    "                [[1, 5, year], [31, 5, year]],\n",
    "                [[1, 6, year], [30, 6, year]],\n",
    "                [[1, 7, year], [31, 7, year]],\n",
    "                [[1, 8, year], [31, 8, year]],\n",
    "                [[1, 9, year], [30, 9, year]],\n",
    "                [[1, 10, year], [31, 10, year]],\n",
    "                [[1, 11, year], [30, 11, year]],\n",
    "                [[1, 12, year], [31, 12, year]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "#             \"q\":\"women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:ksl.com\",\n",
    "            \"q\":\"women | woman | girl | girls | female | females | lady | ladies | she | her++site:ksl.com\",\n",
    "            \"tbs\":\"sbd:1,cdr:1,cd_min:1/1/2020,cd_max:1/31/2020\",\n",
    "            \"tbm\":\"nws\",\n",
    "            \"sxsrf\":\"ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324\",\n",
    "            \"ei\":\"O9mNYJ-hE_LR0PEP6dqaoAU\",\n",
    "            \"start\":10,\n",
    "            \"sa\":\"N\",\n",
    "            \"ved\":\"0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB\",\n",
    "            \"biw\":2512,\n",
    "            \"bih\":1298,\n",
    "            \"dpr\":1.5\n",
    "}\n",
    "\n",
    "params_str = urllib.parse.urlencode(params, safe=':+')\n",
    "\n",
    "r = requests.get(\"https://www.google.com/search?\", params=params_str)\n",
    "\n",
    "# q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:ksl.com&\n",
    "# tbs=sbd:1,cdr:1,cd_min:2/1/2020,cd_max:2/28/2020&\n",
    "# tbm=nws&\n",
    "# sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&\n",
    "# ei=O9mNYJ-hE_LR0PEP6dqaoAU&\n",
    "# start=10&\n",
    "# sa=N&\n",
    "# ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&\n",
    "# biw=2512&\n",
    "# bih=1298&\n",
    "# dpr=1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unescape(s):\n",
    "#     s = s.replace(\"&lt;\", \"<\")\n",
    "#     s = s.replace(\"&gt;\", \">\")\n",
    "#     # this has to be last:\n",
    "#     s = s.replace(\"&amp;\", \"&\")\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paramaters(soup1, urls, headlines, subtitles, times, scrape_dates, websites):\n",
    "\n",
    "    coverpage_news = soup1.find_all('div', class_='ZINbbc xpd O9g5cc uUPGi')\n",
    "    if len(coverpage_news)>0:\n",
    "        # get info for each of 10 google results\n",
    "        for i in range(len(coverpage_news)):\n",
    "            # url\n",
    "            url = coverpage_news[i].a['href'][7:]\n",
    "            urls.append(url)\n",
    "            # headline\n",
    "            headline = coverpage_news[i].a.text\n",
    "            headlines.append(headline)\n",
    "            # subtitle\n",
    "            subtitle = coverpage_news[i].text.split('ï¿½')[1].strip().strip('\"').strip('.')\n",
    "            subtitles.append(subtitle)\n",
    "            # time\n",
    "            time = coverpage_news[i].span.text\n",
    "            times.append(time)\n",
    "            #print(time)\n",
    "            # scrape date\n",
    "            scrape_date = datetime.datetime.now().strftime(\"%d/%m/%Y\")\n",
    "            scrape_dates.append(scrape_date)\n",
    "            # website\n",
    "            websites.append(site)\n",
    "        # keep track\n",
    "        print(str(start + 10) + \" results of site \" + str(site) + \" scraped successfully!\")\n",
    "    #else:\n",
    "     #   break\n",
    "\n",
    "    return urls, headlines, subtitles, times, scrape_dates, websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[1, 10, 2020], [31, 10, 2020]]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "month_ranges[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'drudgereport.com'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "sites = list(USA.site.unique().flatten())\n",
    "sites[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 0 ns\n",
      "Scraping page 1.0 of d google news results, for [1, 10, 2020]  to  [31, 12, 2020]\n",
      "https://www.google.com/search?q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:d&tbs=sbd:1,cdr:1,cd_min:10/1/2020,cd_max:12/31/2020&tbm=nws&sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&ei=O9mNYJ-hE_LR0PEP6dqaoAU&start=0&sa=N&ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&biw=2512&bih=1298&dpr=1.5\n",
      "Scraping page 2.0 of d google news results, for [1, 10, 2020]  to  [31, 12, 2020]\n",
      "https://www.google.com/search?q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:d&tbs=sbd:1,cdr:1,cd_min:10/1/2020,cd_max:12/31/2020&tbm=nws&sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&ei=O9mNYJ-hE_LR0PEP6dqaoAU&start=10&sa=N&ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&biw=2512&bih=1298&dpr=1.5\n",
      "Scraping page 3.0 of d google news results, for [1, 10, 2020]  to  [31, 12, 2020]\n",
      "https://www.google.com/search?q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:d&tbs=sbd:1,cdr:1,cd_min:10/1/2020,cd_max:12/31/2020&tbm=nws&sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&ei=O9mNYJ-hE_LR0PEP6dqaoAU&start=20&sa=N&ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&biw=2512&bih=1298&dpr=1.5\n",
      "Saved data from 0 articles of d to .csv!\n",
      "Scraping page 1.0 of r google news results, for [1, 10, 2020]  to  [31, 12, 2020]\n",
      "https://www.google.com/search?q=women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her++site:r&tbs=sbd:1,cdr:1,cd_min:10/1/2020,cd_max:12/31/2020&tbm=nws&sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&ei=O9mNYJ-hE_LR0PEP6dqaoAU&start=0&sa=N&ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&biw=2512&bih=1298&dpr=1.5\n"
     ]
    }
   ],
   "source": [
    "# loop\n",
    "%time\n",
    "from time import sleep\n",
    "from random import randint\n",
    "# Country: \n",
    "query = ['women+%7C+woman+%7C+girl+%7C+girls+%7C+female+%7C+females+%7C+lady+%7C+ladies+%7C+she+%7C+her']\n",
    "# sites = list(USA.site.unique().flatten())[10:12]\n",
    "sites = list(USA.site.unique().flatten())\n",
    "# starting_result = range(0, 310, 10)\n",
    "#starting_result = [0, 10, 20]\n",
    "starting_result = [0, 10, 20]\n",
    "\n",
    "\n",
    "for site in sites[0]:\n",
    "    print(site)\n",
    "    \n",
    "    urls = []\n",
    "    headlines = []\n",
    "    subtitles = []\n",
    "    times = []\n",
    "    scrape_dates = []\n",
    "    websites = []\n",
    "\n",
    "    #for i in range(len(month_ranges[10:12])):\n",
    "    start_date = month_ranges[9][0]\n",
    "    start_day = month_ranges[9][0][0]\n",
    "    start_month = month_ranges[9][0][1]\n",
    "    start_year = month_ranges[9][0][2]\n",
    "\n",
    "    end_day = month_ranges[11][1][0]\n",
    "    end_month = month_ranges[11][1][1]\n",
    "    end_year = month_ranges[11][1][2]\n",
    "    end_date = month_ranges[11][1]\n",
    "\n",
    "   # print(\"Collecting headlines for\", site, \"from\", start_date, \"to\", end_date)\n",
    "    \n",
    "    for start in starting_result:\n",
    "        # keep track\n",
    "#             print(\"Scraping page\", str((start+10)/10), \"of\", site, \"google news results.\")\n",
    "        print(\"Scraping page\", str((start+10)/10), \"of\", site, \"google news results, for\", start_date, \" to \", end_date)\n",
    "        # change url\n",
    "        url = html.unescape(f'https://www.google.com/search?q={query[0]}++site:{site}&tbs=sbd:1,cdr:1,cd_min:{start_month}/{start_day}/{start_year},cd_max:{end_month}/{end_day}/{end_year}&tbm=nws&sxsrf=ALeKk01s2qiIETbDTDSuhPCKn0BzZOKJDg:1619908923324&ei=O9mNYJ-hE_LR0PEP6dqaoAU&start={start}&sa=N&ved=0ahUKEwjfqsX_xqnwAhXyKDQIHWmtBlQQ8tMDCIYB&biw=2512&bih=1298&dpr=1.5')\n",
    "        #print(str(url))\n",
    "        # sleep\n",
    "        sleep(randint(100,120))\n",
    "        soup1 = send_request(url)\n",
    "        # check whether google is trying to redirect to captcha\n",
    "        if \"Our systems have detected unusual traffic from your computer network\" in str(soup1):\n",
    "            sleep(120)\n",
    "            print(\"unusual traffic error\")\n",
    "            soup1 = send_request(url)\n",
    "        # continue\n",
    "        urls, headlines, subtitles, times, scrape_dates, websites = extract_paramaters(soup1, urls, headlines, subtitles, times, scrape_dates, websites)\n",
    "            \n",
    "    # dictionary of results\n",
    "    headlines_dict = {\"url\": urls, \"headline\": headlines,  \"subtitle\": subtitles, \"time\": times, \"scrape_date\": scrape_dates, \"site\": websites}\n",
    "    # df of results\n",
    "    headlines_df = pd.DataFrame.from_dict(headlines_dict)\n",
    "    number_of_articles = len(headlines_df)\n",
    "    # save results to csv\n",
    "    headlines_df.to_csv(f'../data/raw2/{number_of_articles}_{site}_articles.csv')\n",
    "    # print final statement\n",
    "    print(\"Saved data from \" + str(number_of_articles) + \" articles of \"  + str(site) + \" to .csv!\")\n",
    "    \n",
    "print(\"Scrape complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_dict = {\"url\": urls, \"headline\": headlines,  \"subtitle\": subtitles, \"time\": times, \"scrape_date\": scrape_dates, \"site\": websites}\n",
    "# df of results\n",
    "headlines_df = pd.DataFrame.from_dict(headlines_dict)\n",
    "number_of_articles = len(headlines_df)\n",
    "# save results to csv\n",
    "headlines_df.to_csv(f'../data/{number_of_articles}_{site}_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "from GoogleNews import GoogleNews\n",
    "# from newspaper import Article\n",
    "import pandas as pd\n",
    "\n",
    "googlenews=GoogleNews(start='10/01/2020',end='12/31/2020')\n",
    "googlenews.search('site:nbcnews.com')\n",
    "\n",
    "for i in range(1,30):\n",
    "    #sleep(randint(90,100))\n",
    "    try:\n",
    "        googlenews.getpage(i)\n",
    "    except (ConnectionError, OSError, TimeoutError) as e:\n",
    "        print(\"Encountered an error, F*** YOU GOOGLE, sleeping . . .\")\n",
    "        sleep(90)\n",
    "        googlenews.getpage(i)\n",
    "\n",
    "    except \"HTTP Error 429: Too Many Requests\" as e:\n",
    "        print(\"Encountered too many requests error, F*** YOU GOOGLE, sleeping . . .\")\n",
    "        sleep(randint(1,10))\n",
    "        googlenews.getpage(i)\n",
    "    except \"NoneType' object is not iterable\" as e:\n",
    "        print(\"This month does not have 30 pages worth of data\")\n",
    "    result=googlenews.result()\n",
    "    df=pd.DataFrame(result)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(268, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['women|woman|girl|girls|female|females|lady|ladies|she|her']\n",
    "# sites = list(USA.site.unique().flatten())[10:12]\n",
    "sites = list(USA.site.unique().flatten())\n",
    "# starting_result = range(0, 310, 10)\n",
    "starting_result = [0, 10, 20]\n",
    "\n",
    "\n",
    "for site in sites[1:3]:\n",
    "\n",
    "    for i in range(len(month_ranges[0:2])):\n",
    "        start_date = month_ranges[i][0]\n",
    "        start_day = month_ranges[i][0][0]\n",
    "        start_month = month_ranges[i][0][1]\n",
    "        start_year = month_ranges[i][0][2]\n",
    "\n",
    "        end_day = month_ranges[i][1][0]\n",
    "        end_month = month_ranges[i][1][1]\n",
    "        end_year = month_ranges[i][1][2]\n",
    "        end_date = month_ranges[i][1]\n",
    "    \n",
    "        print(\"Collecting headlines for\", site, \"from\", start_date, \"to\", end_date)\n",
    "        \n",
    "        googlenews=GoogleNews(start=f'{start_month}/{start_day}/{start_year}', end=f'{end_month}/{end_day}/{end_year}')\n",
    "        googlenews.search(f'{query[0]} site:{site}')\n",
    "\n",
    "        for i in range(1,30):\n",
    "            sleep(randint(1,10))\n",
    "#             print(\"page\", i, \"scraped!\")\n",
    "            googlenews.getpage(i)\n",
    "            result=googlenews.result()\n",
    "            df=pd.DataFrame(result)\n",
    "            \n",
    "        df.to_csv(f'../data/raw_2/{site}_{start_date}_{end_date}.csv')\n",
    "        # print final statement\n",
    "        number_of_articles = len(df)\n",
    "        print(\"Saved data from \" + str(number_of_articles) + \" articles of \"  + str(site) + \" to .csv!\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Her Dream of Becoming a Doctor Turned Into a Nightmare, and a Movie'"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.title[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.theblaze.com/news/leftist-woman-spits-in-face-of-cop&sa=U&ved=2ahUKEwj_2pujuo7tAhUMI6wKHaZdAtUQxfQBMAl6BAgCEAE&usg=AOvVaw2TSZgMP4nTF5XNFsvmKzCh'"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverpage_news[i].a['href'][7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverpage_news = soup1.find_all('div', class_='ZINbbc xpd O9g5cc uUPGi')\n",
    "# coverpage_news = soup1.find_all('div', class_='kCrYT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = random.choice(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_dict = {\"url\": [url], \"headline\": [headline],  \"subtitle\": [subtitle], \"time\": [time], \"scrape_date\": [scrape_date], \"site\":[site]}\n",
    "headlines_df = pd.DataFrame.from_dict(headlines_dict)\n",
    "number_of_articles = len(headlines_df)\n",
    "headlines_df.to_csv(f'../data/{number_of_articles}_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19-year-old woman ends life in UP''s Bulandshahr, blames three men in suicide noteOutlook India\n",
      "https://www.outlookindia.com/newsscroll/19yearold-woman-ends-life-in-ups-bulandshahr-blames-three-men-in-suicide-note/1977017&sa=U&ved=2ahUKEwiM16CdxY7tAhVDXawKHd_cBnU4ChDF9AEwB3oECAAQAQ&usg=AOvVaw1n2cj0aRpqzPcYPDnrIMth\n",
      "2 giorni fa\n",
      "Bulandshahr (UP), Nov 16 (PTI) A 19-year-old woman allegedly died by suicide in a ... She left behind a suicide note blaming three people for her death. ... and statement) of the CrPC, the girl completely denied the incident\n",
      "19/11/2020\n"
     ]
    }
   ],
   "source": [
    "print(headline)\n",
    "print(url)\n",
    "print(time)\n",
    "print(subtitle)\n",
    "print(scrape_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup1.prettify())\n",
    "i = 7\n",
    "url = coverpage_news[i].a['href'][7:]\n",
    "headline = coverpage_news[i].a.text\n",
    "time = coverpage_news[i].span.text\n",
    "subtitle = coverpage_news[i].text.split('ï¿½')[1].strip().strip('\"').strip('.')\n",
    "scrape_date = datetime.datetime.now().strftime(\"%d/%m/%Y\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0c377a3a05ba1d31fd00218651f72b0a27b3c45e2b83cef10397a5da0ee866dc9",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}