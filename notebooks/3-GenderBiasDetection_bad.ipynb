{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Leonardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Leonardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    # 1. Remove non-letters\n",
    "    sentence_text = re.sub(r'[^\\w\\s]','', sentence)\n",
    "    # 2. Remove all numbers\n",
    "    sentence_text = re.sub(r'[0-9]+', '', sentence_text)\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = sentence_text.lower().split()\n",
    "    # 4. Stemming\n",
    "    words = [stemmer.stem(w) for w in words] \n",
    "    # 5. Lemmatizing\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # 6. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the max length sentence\n",
    "def find_max_length_sentence(sentence):\n",
    "    max_length = 0\n",
    "    for i in sentence:\n",
    "#         length = len(sentence_to_wordlist(i))\n",
    "        length = len(i)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/processed/headlines_cl_sent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = df['clean_headline']\n",
    "seq_length = find_max_length_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word vectors: 3863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.models import model_from_json\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# # Model reconstruction from JSON file\n",
    "# with open('../models/biasly_model_architecture.json', 'r') as f:\n",
    "#     loaded_model = model_from_json(f.read())\n",
    "\n",
    "# # Load weights into the new model\n",
    "# loaded_model.load_weights('../models/biasly_model_weights.h5')\n",
    "\n",
    "# load model\n",
    "wv_model = Word2Vec.load('../models/model.bin')\n",
    "word_vectors = wv_model.wv\n",
    "words = list(wv_model.wv.vocab)\n",
    "# Calling init_sims will make the model will be better for memory\n",
    "# if we don't want to train the model over and over again\n",
    "wv_model.init_sims(replace=True)\n",
    "\n",
    "#n_words = print(len(words))\n",
    "\n",
    "print(\"Number of word vectors: {}\".format(len(word_vectors.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_dim = 100\n",
    "num_words = len(word_vectors.vocab)\n",
    "vocab = Counter(words)\n",
    "\n",
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(num_words-1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentences():\n",
    "    #print(\"Please input a sentence: \")\n",
    "    pred_text = sentence_to_wordlist(input())\n",
    "#     pred_text = input()\n",
    "    pred_sequences = [word_index.get(t, 0) for t in pred_text]\n",
    "    pred_data = pad_sequences([pred_sequences], maxlen=seq_length, padding=\"post\", truncating=\"post\")\n",
    "    prediction = loaded_model.predict(pred_data)\n",
    "    return \"LSTM Model thinks the sentence is %.2f%% biased.\"%(prediction[0][0] * 100)\n",
    "    #    return \"%.2f%%\"%(prediction[0][0] *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " All boys are kind\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6306776]]\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beautiful',\n",
       " 'die',\n",
       " 'catastrophic',\n",
       " 'injury',\n",
       " 'fall',\n",
       " 'tree',\n",
       " 'branch',\n",
       " 'kill',\n",
       " 'family',\n",
       " 'walk']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "df['clean_hl_words'] = df['clean_hl_words'].apply(lambda row: ast.literal_eval(row))\n",
    "df['clean_hl_words'][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['beautiful', 'girl', 'die', 'catastrophic', 'injury', 'fall', 'tree', 'branch', 'kill', 'family', 'walk']\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_headline'][1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two lists  of words that are used when a man or woman is present, based on Danielle Sucher's https://github.com/DanielleSucher/Jailbreak-the-Patriarchy\n",
    "male_words=set(['guy','spokesman','chairman',\"men's\",'men','him',\"he's\",'his','boy','boyfriend','boyfriends','boys','brother','brothers','dad','dads','dude','father','fathers','fiance','gentleman','gentlemen','god','grandfather','grandpa','grandson','groom','he','himself','husband','husbands','king','male','man','mr','nephew','nephews','priest','prince','son','sons','uncle','uncles','waiter','widower','widowers'])\n",
    "female_words=set(['heroine','spokeswoman','chairwoman',\"women's\",'actress','women',\"she's\",'her','aunt','aunts','bride','daughter','daughters','female','fiancee','girl','girlfriend','girlfriends','girls','goddess','granddaughter','grandma','grandmother','herself','ladies','lady','lady','mom','moms','mother','mothers','mrs','ms','niece','nieces','priestess','princess','queens','she','sister','sisters','waitress','widow','widows','wife','wives','woman', 'beautiful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_the_sentence(sentence_words):\n",
    "    mw_length=len(male_words.intersection(sentence_words))\n",
    "    fw_length=len(female_words.intersection(sentence_words))\n",
    "\n",
    "    if mw_length>0 and fw_length==0:\n",
    "        gender='male'\n",
    "    elif mw_length==0 and fw_length>0: \n",
    "        gender='female'\n",
    "    elif mw_length>0 and fw_length>0: \n",
    "        gender='both'\n",
    "    else:\n",
    "        gender='none'\n",
    "    return gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = set([w.lower() for w in df['clean_hl_words'][1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beautiful',\n",
       " 'branch',\n",
       " 'catastrophic',\n",
       " 'die',\n",
       " 'fall',\n",
       " 'family',\n",
       " 'injury',\n",
       " 'kill',\n",
       " 'tree',\n",
       " 'walk'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_the_sentence(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
