{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all files \n",
    "files = sorted(glob.glob(\"../data/raw/*_articles.csv\"))\n",
    "\n",
    "file_names = []\n",
    "\n",
    "for file_path in files:\n",
    "    file_name = re.split(\"_\", os.path.basename(file_path))[1].split('.')[0]\n",
    "    \n",
    "    hd = pd.read_csv(file_path)\n",
    "    file_names.append(hd)\n",
    "    \n",
    "df = pd.concat(file_names).reset_index().drop([\"Unnamed: 0\", \"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata file\n",
    "newsrooms = pd.read_csv('../data/raw/newsrooms.csv')\n",
    "newsrooms.columns = ['site', 'monthly_visits', 'country', 'country_of_pub']\n",
    "newsrooms.dropna(inplace=True)\n",
    "newsrooms = newsrooms.drop_duplicates(subset = 'site')\n",
    "newsrooms = newsrooms.sort_values(by=['country_of_pub', 'monthly_visits'], ascending=False).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, newsrooms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has 7814 headlines from South Africa 6779 headlines from India 11210 headlines from the UK and 22376 headlines from the U.S.\n"
     ]
    }
   ],
   "source": [
    "print(\"This dataset has\", len(df[df['country_of_pub']=='South Africa']), \"headlines from South Africa\", len(df[df['country_of_pub']=='India']), \"headlines from India\",\n",
    "      len(df[df['country_of_pub']=='UK']), \"headlines from the UK and\", len(df[df['country_of_pub']=='USA']), \"headlines from the U.S.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset covers 29 news outlets from South Africa 24 news outlets from India 43 news outlets from the UK and 90 news outlets from the U.S.\n"
     ]
    }
   ],
   "source": [
    "print(\"This dataset covers\", len(df.site[df['country_of_pub']=='South Africa'].unique()), \"news outlets from South Africa\", len(df.site[df['country_of_pub']=='India'].unique()), \n",
    "      \"news outlets from India\", len(df.site[df['country_of_pub']=='UK'].unique()), \"news outlets from the UK and\", len(df.site[df['country_of_pub']=='USA'].unique()), \"news outlets from the U.S.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoticons_happy = set([\n",
    "#     ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "#     ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "#     '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "#     'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "#     '<3'\n",
    "#     ])\n",
    "\n",
    "# # Sad Emoticons\n",
    "# emoticons_sad = set([\n",
    "#     ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "#     ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "#     ':c', ':{', '>:\\\\', ';('\n",
    "#     ])\n",
    "\n",
    "# #Emoji patterns\n",
    "# emoji_pattern = re.compile(\"[\"\n",
    "#          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#          u\"\\U00002702-\\U000027B0\"\n",
    "#          u\"\\U000024C2-\\U0001F251\"\n",
    "#          \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# #combine sad and happy emoticons\n",
    "# emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_tweets(tweet):\n",
    " \n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     word_tokens = word_tokenize(tweet)\n",
    "#     #after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
    "#     tweet = re.sub(r':', '', tweet)\n",
    "#     tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "#     tweet = tweet.replace(\"'s\", '')\n",
    "#     #replace consecutive non-ASCII characters with a space\n",
    "#     tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "#     #remove emojis from tweet\n",
    "#     tweet = emoji_pattern.sub(r'', tweet)\n",
    "#     tweet = tweet.lower()\n",
    "#     #filter using NLTK library append it to a string\n",
    "#     filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "#     filtered_tweet = []\n",
    "#     #looping through conditions\n",
    "#     for w in word_tokens:\n",
    "#     #check tokens against stop words , emoticons and punctuations\n",
    "#         if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "#             filtered_tweet.append(w)\n",
    "#     return ' '.join(filtered_tweet)\n",
    "#     #print(word_tokens)\n",
    "#     #print(filtered_sentence)return tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# def remove_pattern(input_txt, pattern):\n",
    "#     r = re.findall(pattern, input_txt)\n",
    "#     for i in r:\n",
    "#         input_txt = re.sub(i, '', input_txt)        \n",
    "#     return input_txt\n",
    "# def vec_tweets(tweets):\n",
    "#     #remove twitter Return handles (RT @xxx:)\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\") \n",
    "    \n",
    "#     #remove twitter handles (@xxx)\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"@[\\w]*\")\n",
    "    \n",
    "#     #remove URL links (httpxxx)\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")\n",
    "\n",
    "#     #remove special characters, numbers, punctuations (except for #)\n",
    "#     tweets = np.core.defchararray.replace(tweets, \"[^a-zA-Z]\", \" \")\n",
    "    \n",
    "#     return tweets\n",
    "\n",
    "# # tweets_df['clean_text']= clean_tweets(tweets_df['clean_text'].astype('str')) #The function clean_tweets were put to use.\n",
    "# # tweets_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Choose model accordingly for contractions function\n",
    "# model = api.load(\"glove-twitter-25\")\n",
    "# # model = api.load(\"glove-twitter-100\")\n",
    "# model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# cont = Contractions(kv_model=model)\n",
    "# cont.load_models()\n",
    "\n",
    "\n",
    "# exclude words from spacy stopwords list\n",
    "deselect_stop_words = ['no', 'not']\n",
    "for w in deselect_stop_words:\n",
    "    nlp.vocab[w].is_stop = False\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    \"\"\"remove html tags from text\"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    \"\"\"remove extra whitespaces from text\"\"\"\n",
    "    text = text.strip()\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"remove accented characters from text, e.g. café\"\"\"\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# def expand_contractions(text):\n",
    "#     \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n",
    "#     text = list(cont.expand_texts([text], precise=True))[0]\n",
    "#     return text\n",
    "\n",
    "\n",
    "def text_preprocessing(text, accented_chars=True, contractions=False, \n",
    "                       convert_num=True, extra_whitespace=True, \n",
    "                       lemmatization=True, lowercase=True, punctuations=True,\n",
    "                       remove_html=True, remove_num=True, special_chars=True, \n",
    "                       stop_words=True):\n",
    "    \"\"\"preprocess text with default option set to true for all steps\"\"\"\n",
    "    if remove_html == True: #remove html tags\n",
    "        text = strip_html_tags(text)\n",
    "    if extra_whitespace == True: #remove extra whitespaces\n",
    "        text = remove_whitespace(text)\n",
    "    if accented_chars == True: #remove accented characters\n",
    "        text = remove_accented_chars(text)\n",
    "    if contractions == True: #expand contractions\n",
    "        text = expand_contractions(text)\n",
    "    if lowercase == True: #convert all characters to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "    doc = nlp(text) #tokenise text\n",
    "\n",
    "    clean_text = []\n",
    "    \n",
    "    for token in doc:\n",
    "        flag = True\n",
    "        edit = token.text\n",
    "        # remove stop words\n",
    "        if stop_words == True and token.is_stop and token.pos_ != 'NUM': \n",
    "            flag = False\n",
    "        # remove punctuations\n",
    "        if punctuations == True and token.pos_ == 'PUNCT' and flag == True: \n",
    "            flag = False\n",
    "        # remove special characters\n",
    "        if special_chars == True and token.pos_ == 'SYM' and flag == True: \n",
    "            flag = False\n",
    "        # remove numbers\n",
    "        if remove_num == True and (token.pos_ == 'NUM' or token.text.isnumeric()) \\\n",
    "        and flag == True:\n",
    "            flag = False\n",
    "        # convert number words to numeric numbers\n",
    "        if convert_num == True and token.pos_ == 'NUM' and flag == True:\n",
    "            edit = w2n.word_to_num(token.text)\n",
    "        # convert tokens to base form\n",
    "        elif lemmatization == True and token.lemma_ != \"-PRON-\" and flag == True:\n",
    "            edit = token.lemma_\n",
    "        # append tokens edited and not removed to list \n",
    "        if edit != \"\" and flag == True:\n",
    "            clean_text.append(edit)        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leonardo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['headline_no_site'] = df['headline'].apply(lambda x: ''.join(re.split('(?<=[a-z0-9])[A-Z]', x)[:-1]))\n",
    "df['clean_headline'] = df['headline_no_site'].apply(text_preprocessing)\n",
    "df['subtitle'][df['subtitle'].isna() == True] = \"\"\n",
    "df['clean_subtitle'] = df['subtitle'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/headlines_cl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl = pd.read_csv('../data/processed/headlines_cl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leonardo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['clean_headline'] = df['headline'].apply(clean_tweets).apply(vec_tweets)\n",
    "df['subtitle'][df['subtitle'].isna() == True] = \"\"\n",
    "df['clean_subtitle'] = df['subtitle'].apply(clean_tweets).apply(vec_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate time to english\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "df['clean_subtitle'] = translator.translate('9 ore fa', src='it', dest='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9 hours ago'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#importing and initialising the VADER analyser\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Storing the scores in list of dictionaries\n",
    "scores = []\n",
    "# Declare variables for scores\n",
    "compound_list = []\n",
    "positive_list = []\n",
    "negative_list = []\n",
    "neutral_list = []\n",
    "for i in range(tweets_df['clean_text'].shape[0]):\n",
    "#print(analyser.polarity_scores(sentiments_pd['Tweet'][i]))\n",
    "    compound = analyzer.polarity_scores(tweets_df['clean_text'][i])[\"compound\"]\n",
    "    pos = analyzer.polarity_scores(tweets_df['clean_text'][i])[\"pos\"]\n",
    "    neu = analyzer.polarity_scores(tweets_df['clean_text'][i])[\"neu\"]\n",
    "    neg = analyzer.polarity_scores(tweets_df['clean_text'][i])[\"neg\"]\n",
    "    \n",
    "    scores.append({\"compound_s\": compound,\n",
    "                       \"positive_s\": pos,\n",
    "                       \"negative_s\": neg,\n",
    "                       \"neutral_s\": neu\n",
    "                  })\n",
    "\n",
    "#Appending the scores into the dataframe for further analysis \n",
    "sentiments_score = pd.DataFrame.from_dict(scores)\n",
    "tweets_df_sent = tweets_df.join(sentiments_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "plt.hist(tweets_df_sent['compound_s'].values, bins=40)\n",
    "plt.title('Histogram of Compound Sentiment')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.xlabel('Compound Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
