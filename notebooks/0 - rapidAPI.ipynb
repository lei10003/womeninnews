{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0c377a3a05ba1d31fd00218651f72b0a27b3c45e2b83cef10397a5da0ee866dc9",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "from datetime import date\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsrooms = pd.read_csv('../data/raw/newsrooms.csv')\n",
    "newsrooms.columns = ['site', 'monthly_visits', 'country', 'country_of_pub']\n",
    "newsrooms.dropna(inplace=True)\n",
    "newsrooms = newsrooms.drop_duplicates(subset = 'site')\n",
    "newsrooms = newsrooms.sort_values(by=['country_of_pub', 'monthly_visits'], ascending=False).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK = newsrooms[newsrooms['country_of_pub']=='UK'].reset_index().drop('index', axis=1)\n",
    "USA = newsrooms[newsrooms['country_of_pub']=='USA'].reset_index().drop('index', axis=1)\n",
    "India = newsrooms[newsrooms['country_of_pub']=='India'].reset_index().drop('index', axis=1)\n",
    "SA = newsrooms[newsrooms['country_of_pub']=='South Africa'].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites = list(SA.site.unique().flatten()) + list(USA.site.unique().flatten()) + list(UK.site.unique().flatten()) + list(India.site.unique().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "metadata": {},
     "execution_count": 186
    }
   ],
   "source": [
    "len(all_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "metadata": {},
     "execution_count": 137
    }
   ],
   "source": [
    "len(UK) + len(USA) + len(India) + len(SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "\n",
    "def get_two_month_ranges(year):\n",
    "    month_ranges = [[[1, 1, year], [31, 1, year]], \n",
    "                    [[1, 2, year], [28, 2, year]],\n",
    "                    [[1, 3, year], [31, 3, year]],\n",
    "                    [[1, 4, year], [30, 4, year]],\n",
    "                    [[1, 5, year], [31, 5, year]],\n",
    "                    [[1, 6, year], [30, 6, year]],\n",
    "                    [[1, 7, year], [31, 7, year]],\n",
    "                    [[1, 8, year], [31, 8, year]],\n",
    "                    [[1, 9, year], [30, 9, year]],\n",
    "                    [[1, 10, year], [31, 10, year]],\n",
    "                    [[1, 11, year], [30, 11, year]],\n",
    "                    [[1, 12, year], [31, 12, year]]]\n",
    "\n",
    "    two_month_ranges = []\n",
    "    for i in range(len(month_ranges)-1):\n",
    "        if i%2==0:\n",
    "            two_month_ranges.append((month_ranges[i][0], month_ranges[i+1][1]))\n",
    "\n",
    "    return two_month_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_ranges = []\n",
    "for i in range(2019,2020):\n",
    "    month_ranges = month_ranges + get_two_month_ranges(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[([1, 1, 2019], [28, 2, 2019]),\n",
       " ([1, 3, 2019], [30, 4, 2019]),\n",
       " ([1, 5, 2019], [30, 6, 2019]),\n",
       " ([1, 7, 2019], [31, 8, 2019]),\n",
       " ([1, 9, 2019], [31, 10, 2019]),\n",
       " ([1, 11, 2019], [31, 12, 2019])]"
      ]
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "month_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 87 articles of thedailybeast.com to .csv!\n",
      "1280 requests fired\n",
      "Scraping realclearpolitics.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of realclearpolitics.com to .csv!\n",
      "1281 requests fired\n",
      "Scraping cbssports.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 13 articles of cbssports.com to .csv!\n",
      "1282 requests fired\n",
      "Scraping yahoo.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of yahoo.com to .csv!\n",
      "1283 requests fired\n",
      "Scraping lifehacker.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 2 articles of lifehacker.com to .csv!\n",
      "1284 requests fired\n",
      "Scraping buzzfeed.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 81 articles of buzzfeed.com to .csv!\n",
      "1285 requests fired\n",
      "Scraping nationalgeographic.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 1 articles of nationalgeographic.com to .csv!\n",
      "1286 requests fired\n",
      "Scraping reuters.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 71 articles of reuters.com to .csv!\n",
      "1287 requests fired\n",
      "Scraping chron.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 71 articles of chron.com to .csv!\n",
      "1288 requests fired\n",
      "Scraping theblaze.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 7 articles of theblaze.com to .csv!\n",
      "1289 requests fired\n",
      "Scraping eonline.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of eonline.com to .csv!\n",
      "1290 requests fired\n",
      "Scraping marketwatch.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 47 articles of marketwatch.com to .csv!\n",
      "1291 requests fired\n",
      "Scraping dailykos.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of dailykos.com to .csv!\n",
      "1292 requests fired\n",
      "Scraping hollywoodreporter.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of hollywoodreporter.com to .csv!\n",
      "1293 requests fired\n",
      "Scraping time.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 23 articles of time.com to .csv!\n",
      "1294 requests fired\n",
      "Scraping salon.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 25 articles of salon.com to .csv!\n",
      "1295 requests fired\n",
      "Scraping newsmax.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of newsmax.com to .csv!\n",
      "1296 requests fired\n",
      "Scraping gawker.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of gawker.com to .csv!\n",
      "1297 requests fired\n",
      "Scraping theatlantic.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 9 articles of theatlantic.com to .csv!\n",
      "1298 requests fired\n",
      "Scraping nbcsports.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 13 articles of nbcsports.com to .csv!\n",
      "1299 requests fired\n",
      "Scraping chicagotribune.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of chicagotribune.com to .csv!\n",
      "1300 requests fired\n",
      "Scraping msnbc.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 1 articles of msnbc.com to .csv!\n",
      "1301 requests fired\n",
      "Scraping dailycaller.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 12 articles of dailycaller.com to .csv!\n",
      "1302 requests fired\n",
      "Scraping littlethings.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 2 articles of littlethings.com to .csv!\n",
      "1303 requests fired\n",
      "Scraping zerohedge.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of zerohedge.com to .csv!\n",
      "1304 requests fired\n",
      "Scraping usmagazine.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of usmagazine.com to .csv!\n",
      "1305 requests fired\n",
      "Scraping vox.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 12 articles of vox.com to .csv!\n",
      "1306 requests fired\n",
      "Scraping thehill.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 83 articles of thehill.com to .csv!\n",
      "1307 requests fired\n",
      "Scraping ew.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 18 articles of ew.com to .csv!\n",
      "1308 requests fired\n",
      "Scraping si.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 15 articles of si.com to .csv!\n",
      "1309 requests fired\n",
      "Scraping realsimple.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 2 articles of realsimple.com to .csv!\n",
      "1310 requests fired\n",
      "Scraping rt.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 16 articles of rt.com to .csv!\n",
      "1311 requests fired\n",
      "Scraping Msn.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of Msn.com to .csv!\n",
      "1312 requests fired\n",
      "Scraping cnet.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 18 articles of cnet.com to .csv!\n",
      "1313 requests fired\n",
      "Scraping espncricinfo.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 21 articles of espncricinfo.com to .csv!\n",
      "1314 requests fired\n",
      "Scraping news.google.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of news.google.com to .csv!\n",
      "1315 requests fired\n",
      "Scraping ign.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 23 articles of ign.com to .csv!\n",
      "1316 requests fired\n",
      "Scraping bloomberg.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 64 articles of bloomberg.com to .csv!\n",
      "1317 requests fired\n",
      "Scraping washingtonpost.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of washingtonpost.com to .csv!\n",
      "1318 requests fired\n",
      "Scraping rollingstone.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 37 articles of rollingstone.com to .csv!\n",
      "1319 requests fired\n",
      "Scraping huffingtonpost.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 12 articles of huffingtonpost.com to .csv!\n",
      "1320 requests fired\n",
      "Scraping msn.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of msn.com to .csv!\n",
      "1321 requests fired\n",
      "Scraping vice.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 62 articles of vice.com to .csv!\n",
      "1322 requests fired\n",
      "Scraping vogue.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 39 articles of vogue.com to .csv!\n",
      "1323 requests fired\n",
      "Scraping sports.yahoo.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 42 articles of sports.yahoo.com to .csv!\n",
      "1324 requests fired\n",
      "Scraping mashable.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 74 articles of mashable.com to .csv!\n",
      "1325 requests fired\n",
      "Scraping gamespot.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 1 articles of gamespot.com to .csv!\n",
      "1326 requests fired\n",
      "Scraping style.yahoo.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 51 articles of style.yahoo.com to .csv!\n",
      "1327 requests fired\n",
      "Scraping breitbart.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 19 articles of breitbart.com to .csv!\n",
      "1328 requests fired\n",
      "Scraping howtogeek.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of howtogeek.com to .csv!\n",
      "1329 requests fired\n",
      "Scraping cracked.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of cracked.com to .csv!\n",
      "1330 requests fired\n",
      "Scraping collegehumor.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of collegehumor.com to .csv!\n",
      "1331 requests fired\n",
      "Scraping theverge.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 2 articles of theverge.com to .csv!\n",
      "1332 requests fired\n",
      "Scraping digitaltrends.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 4 articles of digitaltrends.com to .csv!\n",
      "1333 requests fired\n",
      "Scraping health.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of health.com to .csv!\n",
      "1334 requests fired\n",
      "Scraping pcmag.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of pcmag.com to .csv!\n",
      "1335 requests fired\n",
      "Scraping foxnews.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of foxnews.com to .csv!\n",
      "1336 requests fired\n",
      "Scraping wired.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 2 articles of wired.com to .csv!\n",
      "1337 requests fired\n",
      "Scraping yahoo.net starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of yahoo.net to .csv!\n",
      "1338 requests fired\n",
      "Scraping nytimes.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of nytimes.com to .csv!\n",
      "1339 requests fired\n",
      "Scraping forbes.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of forbes.com to .csv!\n",
      "1340 requests fired\n",
      "Scraping businessinsider.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of businessinsider.com to .csv!\n",
      "1341 requests fired\n",
      "Scraping news.yahoo.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of news.yahoo.com to .csv!\n",
      "1342 requests fired\n",
      "Scraping cosmopolitan.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 32 articles of cosmopolitan.com to .csv!\n",
      "1343 requests fired\n",
      "Scraping finance.yahoo.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 54 articles of finance.yahoo.com to .csv!\n",
      "1344 requests fired\n",
      "Scraping bbc.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 12 articles of bbc.co.uk to .csv!\n",
      "1345 requests fired\n",
      "Scraping telegraph.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of telegraph.co.uk to .csv!\n",
      "1346 requests fired\n",
      "Scraping newsnow.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of newsnow.co.uk to .csv!\n",
      "1347 requests fired\n",
      "Scraping independent.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of independent.co.uk to .csv!\n",
      "1348 requests fired\n",
      "Scraping skysports.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 26 articles of skysports.com to .csv!\n",
      "1349 requests fired\n",
      "Scraping express.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of express.co.uk to .csv!\n",
      "1350 requests fired\n",
      "Scraping mirror.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of mirror.co.uk to .csv!\n",
      "1351 requests fired\n",
      "Scraping ibtimes.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 13 articles of ibtimes.co.uk to .csv!\n",
      "1352 requests fired\n",
      "Scraping thesun.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 96 articles of thesun.co.uk to .csv!\n",
      "1353 requests fired\n",
      "Scraping news.sky.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of news.sky.com to .csv!\n",
      "1354 requests fired\n",
      "Scraping metro.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of metro.co.uk to .csv!\n",
      "1355 requests fired\n",
      "Scraping liveleak.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of liveleak.com to .csv!\n",
      "1356 requests fired\n",
      "Scraping standard.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of standard.co.uk to .csv!\n",
      "1357 requests fired\n",
      "Scraping which.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of which.co.uk to .csv!\n",
      "1358 requests fired\n",
      "Scraping huffingtonpost.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 40 articles of huffingtonpost.co.uk to .csv!\n",
      "1359 requests fired\n",
      "Scraping ft.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 2 articles of ft.com to .csv!\n",
      "1360 requests fired\n",
      "Scraping thetimes.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of thetimes.co.uk to .csv!\n",
      "1361 requests fired\n",
      "Scraping techradar.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 2 articles of techradar.com to .csv!\n",
      "1362 requests fired\n",
      "Scraping digitalspy.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 28 articles of digitalspy.com to .csv!\n",
      "1363 requests fired\n",
      "Scraping dailystar.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 88 articles of dailystar.co.uk to .csv!\n",
      "1364 requests fired\n",
      "Scraping manchestereveningnews.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 82 articles of manchestereveningnews.co.uk to .csv!\n",
      "1365 requests fired\n",
      "Scraping autoexpress.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of autoexpress.co.uk to .csv!\n",
      "1366 requests fired\n",
      "Scraping timeout.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 8 articles of timeout.com to .csv!\n",
      "1367 requests fired\n",
      "Scraping bbc.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 17 articles of bbc.com to .csv!\n",
      "1368 requests fired\n",
      "Scraping liverpoolecho.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 77 articles of liverpoolecho.co.uk to .csv!\n",
      "1369 requests fired\n",
      "Scraping givemesport.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 33 articles of givemesport.com to .csv!\n",
      "1370 requests fired\n",
      "Scraping pcadvisor.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of pcadvisor.co.uk to .csv!\n",
      "1371 requests fired\n",
      "Scraping dailyrecord.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of dailyrecord.co.uk to .csv!\n",
      "1372 requests fired\n",
      "Scraping theladbible.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 78 articles of theladbible.com to .csv!\n",
      "1373 requests fired\n",
      "Scraping theregister.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of theregister.co.uk to .csv!\n",
      "1374 requests fired\n",
      "Scraping thisismoney.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 8 articles of thisismoney.co.uk to .csv!\n",
      "1375 requests fired\n",
      "Scraping chroniclelive.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 54 articles of chroniclelive.co.uk to .csv!\n",
      "1376 requests fired\n",
      "Scraping espn.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 27 articles of espn.co.uk to .csv!\n",
      "1377 requests fired\n",
      "Scraping sport.bt.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of sport.bt.com to .csv!\n",
      "1378 requests fired\n",
      "Scraping birminghammail.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 85 articles of birminghammail.co.uk to .csv!\n",
      "1379 requests fired\n",
      "Scraping walesonline.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 66 articles of walesonline.co.uk to .csv!\n",
      "1380 requests fired\n",
      "Scraping unilad.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of unilad.co.uk to .csv!\n",
      "1381 requests fired\n",
      "Scraping makeuseof.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of makeuseof.com to .csv!\n",
      "1382 requests fired\n",
      "Scraping nature.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of nature.com to .csv!\n",
      "1383 requests fired\n",
      "Scraping plymouthherald.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 36 articles of plymouthherald.co.uk to .csv!\n",
      "1384 requests fired\n",
      "Scraping theguardian.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of theguardian.com to .csv!\n",
      "1385 requests fired\n",
      "Scraping cnn.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of cnn.com to .csv!\n",
      "1386 requests fired\n",
      "Scraping dailymail.co.uk starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of dailymail.co.uk to .csv!\n",
      "1387 requests fired\n",
      "Scraping edition.cnn.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 11 articles of edition.cnn.com to .csv!\n",
      "1388 requests fired\n",
      "Scraping telegraphindia.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 69 articles of telegraphindia.com to .csv!\n",
      "1389 requests fired\n",
      "Scraping aajtak.in starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of aajtak.in to .csv!\n",
      "1390 requests fired\n",
      "Scraping indiatimes.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of indiatimes.com to .csv!\n",
      "1391 requests fired\n",
      "Scraping ndtv.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of ndtv.com to .csv!\n",
      "1392 requests fired\n",
      "Scraping news18.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of news18.com to .csv!\n",
      "1393 requests fired\n",
      "Scraping timesofindia.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of timesofindia.com to .csv!\n",
      "1394 requests fired\n",
      "Scraping india.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of india.com to .csv!\n",
      "1395 requests fired\n",
      "Scraping hindustantimes.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of hindustantimes.com to .csv!\n",
      "1396 requests fired\n",
      "Scraping indianexpress.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of indianexpress.com to .csv!\n",
      "1397 requests fired\n",
      "Scraping indiatoday.in starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of indiatoday.in to .csv!\n",
      "1398 requests fired\n",
      "Scraping rediff.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of rediff.com to .csv!\n",
      "1399 requests fired\n",
      "Scraping asianetnews.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 26 articles of asianetnews.com to .csv!\n",
      "1400 requests fired\n",
      "Scraping livemint.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 30 articles of livemint.com to .csv!\n",
      "1401 requests fired\n",
      "Scraping timesnownews.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of timesnownews.com to .csv!\n",
      "1402 requests fired\n",
      "Scraping oneindia.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 50 articles of oneindia.com to .csv!\n",
      "1403 requests fired\n",
      "Scraping thehindu.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of thehindu.com to .csv!\n",
      "1404 requests fired\n",
      "Scraping prokerala.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 3 articles of prokerala.com to .csv!\n",
      "1405 requests fired\n",
      "Scraping dnaindia.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of dnaindia.com to .csv!\n",
      "1406 requests fired\n",
      "Scraping mykhel.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 20 articles of mykhel.com to .csv!\n",
      "1407 requests fired\n",
      "Scraping firstpost.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of firstpost.com to .csv!\n",
      "1408 requests fired\n",
      "Scraping deccanherald.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 40 articles of deccanherald.com to .csv!\n",
      "1409 requests fired\n",
      "Scraping opindia.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 37 articles of opindia.com to .csv!\n",
      "1410 requests fired\n",
      "Scraping scroll.in starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of scroll.in to .csv!\n",
      "1411 requests fired\n",
      "Scraping newindianexpress.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of newindianexpress.com to .csv!\n",
      "1412 requests fired\n",
      "Scraping thenewsminute.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of thenewsminute.com to .csv!\n",
      "1413 requests fired\n",
      "Scraping thewire.in starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 29 articles of thewire.in to .csv!\n",
      "1414 requests fired\n",
      "Scraping scoopwhoop.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 66 articles of scoopwhoop.com to .csv!\n",
      "1415 requests fired\n",
      "Scraping thequint.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 59 articles of thequint.com to .csv!\n",
      "1416 requests fired\n",
      "Scraping vogue.in starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 18 articles of vogue.in to .csv!\n",
      "1417 requests fired\n",
      "Scraping outlookindia.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 73 articles of outlookindia.com to .csv!\n",
      "1418 requests fired\n",
      "Scraping mensxp.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 11 articles of mensxp.com to .csv!\n",
      "1419 requests fired\n",
      "Scraping deccanchronicle.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of deccanchronicle.com to .csv!\n",
      "1420 requests fired\n",
      "Scraping sumanasa.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of sumanasa.com to .csv!\n",
      "1421 requests fired\n",
      "Scraping swarajyamag.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 49 articles of swarajyamag.com to .csv!\n",
      "1422 requests fired\n",
      "Scraping greaterkashmir.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 0 articles of greaterkashmir.com to .csv!\n",
      "1423 requests fired\n",
      "Scraping thebetterindia.com starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 29 articles of thebetterindia.com to .csv!\n",
      "1424 requests fired\n",
      "Scraping freepressjournal.in starting from 2019-11-1 to 2019-12-31\n",
      "Saved data from 100 articles of freepressjournal.in to .csv!\n",
      "1425 requests fired\n",
      "Scraping economictimes.indiatimes.com starting from 2019-11-1 to 2019-12-31\n",
      "100%|██████████| 6/6 [48:16<00:00, 482.77s/it]Saved data from 42 articles of economictimes.indiatimes.com to .csv!\n",
      "1426 requests fired\n",
      "Wall time: 48min 16s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "headlines_dict = {}\n",
    "\n",
    "sites = all_sites\n",
    "\n",
    "url = \"https://google-news.p.rapidapi.com/v1/source_search\"\n",
    "\n",
    "for i in tqdm(month_ranges):\n",
    "    from_date = f'{i[0][2]}-{i[0][1]}-{i[0][0]}'\n",
    "    to_date = f'{i[1][2]}-{i[1][1]}-{i[1][0]}'\n",
    "    year = i[0][2]\n",
    "    bimester = i[1][1]/2\n",
    "\n",
    "    for site in sites:\n",
    "\n",
    "        print(\"Scraping\" ,site, \"starting from\", from_date, \"to\", to_date)\n",
    "\n",
    "        urls = []\n",
    "        headlines = []\n",
    "        times = []\n",
    "        scrape_dates = []\n",
    "        websites = []\n",
    "\n",
    "        # baseline news\n",
    "\n",
    "        #querystring = { \"lang\":\"en\",\"from\":from_date,\"to\":to_date,\"source\":site}\n",
    "\n",
    "        # women's news\n",
    "\n",
    "        querystring = {\"q\": \"women | woman | girl  | female  | lady | ladies | she | her | herself | aunt | grandmother | mother | sister | daughter | wife | mom | mum | girlfriend | mrs | niece \" , \"lang\":\"en\",\"from\":from_date,\"to\":to_date,\"source\":site}\n",
    "\n",
    "        headers = {\n",
    "            # Paid API\n",
    "            'x-rapidapi-key': \"bd5010b7eemsh00ab5684fa6a7efp15b791jsnd00e2d098829\",\n",
    "\n",
    "            # Free API\n",
    "            #'x-rapidapi-key': \"3e81132ebdmshafd93b150db164ep13c18fjsn834550268d59\",\n",
    "            'x-rapidapi-host': \"google-news.p.rapidapi.com\"\n",
    "            }\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring).json()\n",
    "\n",
    "        request_count += 1\n",
    "\n",
    "        for j in range(len(response['articles'])):\n",
    "            headlines.append(response['articles'][j]['title'])\n",
    "            times.append(response['articles'][j]['published'])\n",
    "            urls.append(response['articles'][j]['link'])\n",
    "            websites.append(response['articles'][j]['source']['href'])\n",
    "\n",
    "        headlines_dict = {\"url\": urls, \"headline\": headlines, \"time\": times, \"scrape_date\": date.today().strftime(\"%d/%m/%Y\"), \"site\": site}\n",
    "\n",
    "        headlines_df = pd.DataFrame.from_dict(headlines_dict)\n",
    "        number_of_articles = len(headlines_df)\n",
    "        # save results to csv\n",
    "\n",
    "        headlines_df.to_csv(f'../data/raw_temporal/{number_of_articles}_{site}_{bimester}_{year}_articles.csv')\n",
    "        # print final statement\n",
    "        print(\"Saved data from \" + str(number_of_articles) + \" articles of \"  + str(site) + \" to .csv!\")\n",
    "        print(request_count, \"requests fired\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'status': 'error',\n",
       " 'error_code': 'UnsupportedParameter',\n",
       " 'message': \"Found unsupported parameter(s)['source']\"}"
      ]
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "response"
   ]
  }
 ]
}