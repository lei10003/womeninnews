{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import ast\n",
    "import networkx as nx; \n",
    "from networkx.readwrite import json_graph;\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "import time\n",
    "nlp = spacy.load('en_core_web_sm') #you can use other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/headlines_cl_sent.csv')\n",
    "freq = pd.read_csv('../data/processed/words_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47554, 19)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset = ['clean_hl_words'], inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloody\n",
      "phone\n",
      "thief\n",
      "m\n",
      "move\n"
     ]
    }
   ],
   "source": [
    "for idx in nlp(' '.join(word for word in ast.literal_eval(df['clean_hl_words'][100]))):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# exclude nouns from word list\n",
    "# excluded_tags = {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"ADP\", \"PROPN\"}\n",
    "included_tags = {\"VERB\", \"ADJ\", \"ADV\"}\n",
    "\n",
    "df[\"clean_h1_verbs_adjs\"] = df['clean_hl_words'].apply(lambda x: [str(idx) for idx in nlp(' '.join(word for word in ast.literal_eval(x))) if idx.pos_ in included_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porter = PorterStemmer()\n",
    "# lancaster=LancasterStemmer()\n",
    "\n",
    "# print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\"))\n",
    "# for word in female_bias_words:\n",
    "#     print(\"{0:20}{1:20}{2:20}\".format(word,porter.stem(word),lancaster.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources\n",
    "# http://gender-decoder.katmatfield.com/about\n",
    "# http://gender-decoder.katmatfield.com/static/documents/Gaucher-Friesen-Kay-JPSP-Gendered-Wording-in-Job-ads.pdf\n",
    "# https://github.com/DanielleSucher/Jailbreak-the-Patriarchy\n",
    "\n",
    "# this list of words is used to check for female-gendered language\n",
    "female_words=set(['actress',  'actresses',  'airwomen',  'alderwoman',  'alderwomen',  'anchorwoman',  'anchorwomen',  \n",
    "                  'archduchess',  'archduchesses',  'assemblywoman',  'assemblywomen',  'aunt',  'aunts',  'baroness',  \n",
    "                  'baronesses',  'baronetess',  'baronetesses',  'bogeywoman',  'bogeywomen',  'bondswoman',  'bondswomen',  \n",
    "                  'bride',  'businesswoman',  'businesswomen',  'camerawoman',  'camerawomen',  'cavewoman',  'cavewomen',  \n",
    "                  'chairwoman',  'chairwomen',  'clergywoman',  'clergywomen',  'comtesse',  'congresswoman',  'congresswomen',  \n",
    "                  'councilwoman',  'councilwomen',  'countess',  'countesses',  'countrywoman',  'countrywomen',  'craftswoman',  \n",
    "                  'craftswomen',  'damsel',  'daughter',  'daughters',  'deaconess',  'deaconesses',  'diva',  'donna’, ‘housewife',  \n",
    "                  'doorwoman',  'doorwomen',  'duchess',  'duchesses',  'empress',  'empresses',  'fem',  'female',  'females',  \n",
    "                  'fiancee',  'firewoman',  'firewomen',  'fisherwoman',  'fisherwomen',  'forewoman',  'forewomen',  'freshwoman',  \n",
    "                  'freshwomen',  'gal',  'galpal',  'gals',  'garbagewoman',  'garbagewomen',  'gentleman',  'girl',  'girlfriend',  \n",
    "                  'girlfriends',  'girls',  'goddess',  'goddesshead',  'goddesshood',  'goddessliness',  'goddessly',  'godmother',  \n",
    "                  'granddaughter',  'grandma',  'grandmas',  'grandmother',  'handywoman',  'handywomen',  'hangwoman',  'hangwomen',  \n",
    "                  'henchwoman',  'henchwomen',  'her',  'heroine',  'heroines',  'herself',  'housewife',  'journeywoman',  'journeywomen',  \n",
    "                  'kinswoman',  'kinswomen',  'klanswoman',  'ladies',  'ladiez',  'lady',  'lady-romance',  'ladysplain',  'laydeez',  \n",
    "                  'laywoman',  'laywomen',  \"ma'am\",  'madam',  'madwoman',  'madwomen',  'maiden',  'mailwoman',  'mailwomen',  'mama',  \n",
    "                  'marchioness',  'margravine',  'markswoman',  'markswomen',  'marquise',  'middlewoman',  'middlewomen',  'milkwoman',  \n",
    "                  'milkwomen',  'miss',  'mistress',  'mom',  'momma',  'mommies',  'mommy',  'moms',  'mother',  'mothersmrs',  'ms',  \n",
    "                  'mum',  'mummy',  'mums',  'niece',  'nieces',  'noblewoman',  'noblewomen',  'ombudswoman',  'ombudswomen',  'policewoman',  \n",
    "                  'policewomen',  'postwoman',  'postwomen',  'priestess',  'priestesses',  'princess',  'princesses',  'prostitute',  'queen',  \n",
    "                  'queens',  'repairwoman',  'repairwomen',  'saleswoman',  'saleswomen',  'sandwoman',  'sandwomen',  'servicewoman',  \n",
    "                  'servicewomen',  'she',  \"she's\",  'showwoman',  'showwomen',  'sis',  'sistagrammer',  'sistas',  'sister',  'sisters',  \n",
    "                  'snowwoman',  'spacewoman',  'spacewomen',  'spokeswoman',  'spokeswomen',  'sportswoman',  'sportswomen',  'stateswoman',  \n",
    "                  'stateswomen',  'stepmother',  'stepsister',  'superwoman',  'superwomen',  'unwoman',  'viscountess',  'viscountesses',  \n",
    "                  'waitress',  'watchwoman',  'watchwomen',  'weatherwoman',  'weatherwomen',  'widow',  'widows',  'wife',  'wives',  \n",
    "                  'woman',  'womanhood',  'womankind',  'women',  \"women's\",  'workwoman',  'workwomen'])\n",
    "\n",
    "# this list of words\n",
    "female_bias_words = set(['adorable',  'affair',  'affection',  'affectionate',  'afraid',  'agree',  'angel',  'baby',  'banshee',  'barren',  \n",
    "                         'beautiful',  'beauty',  'bikini',  'birth',  'bitch',  'bitchfest',  'bitchy',  'body',  'bossy',  'breast',  'bride',  \n",
    "                         'bridezilla',  'bubbly',  'care',  'caress',  'caring',  'catfight',  'catty',  'chatty',  'cheat',  'cheer',  'child',  \n",
    "                         'clotheshorse',  'clucky',  'co-operate',  'cold',  'collab',  'commit',  'commitment',  'committed',  'communal',  \n",
    "                         'compassion',  'compassionate',  'conscientious',  'considerate',  'cook',  'cooking',  'cooperate',  'cougar',  \n",
    "                         'cries',  'cry',  'crying',  'dedicated',  'demure',  'depend',  'dependable',  'diligent',  'ditzy',  'divorce',  \n",
    "                         'domestic',  'drama',  'dramatic',  'dress',  'easy',  'emotion',  'emotiona',  'emotional',  'emotions',  'empath',  \n",
    "                         'empathetic',  'empathize',  'enthusias',  'family',  'fear',  'feel',  'feeling',  'feisty',  'femaleness',  'feminazi',  \n",
    "                         'feminine',  'fishwife',  'flaky',  'flatter',  'flatterable',  'flirty',  'frail',  'frigid',  'frumpy',  'gentle',  \n",
    "                         'girlhood',  'girlier',  'girliest',  'girly',  'gossip',  'gossipy',  'helpful',  'honest',  'hormonal',  'houseproud',  \n",
    "                         'humourless',  'hysterical',  'ice queen',  'inclusive',  'inter-dependen',  'inter-dependence',  'inter-dependent',  \n",
    "                         'inter-personal',  'interdependence',  'interdependent',  'interpersonal',  'irrational',  'kid',  'kind',  'kinship',  \n",
    "                         'ladylike',  'lie',  'lippy',  'loose',  'love',  'lover',  'loyal',  'maid',  'makeup',  'man-eater',  'man-hater',  \n",
    "                         'marriage',  'married',  'marrigeable',  'marry',  'maternal',  'maternal ',  'menstrual ',  'mistress',  'modest',  \n",
    "                         'modesty',  'moody',  'mousey',  'mumpreneur',  'mumsy',  'nag',  'naked',  'neurotic',  'new born',  'new-born',  \n",
    "                         'newborn',  'nurse',  'nurtur',  'nurture',  'nurtures',  'nurturing',  'obedient',  'obey',  'over-sensitive',  \n",
    "                         'pleasant',  'polite',  'powerless',  'pregnancy',  'pretty',  'prostitute',  'prude',  'quiet',  'relationship',  \n",
    "                         'respon',  'sassy',  'secretary',  'seduce',  'seductive',  'sensitiv',  'sensitive',  'sex',  'sexual',  'sexually',  \n",
    "                         'share',  'sharin',  'shop',  'shopping',  'shrew',  'single',  'slut',  'slutty',  'soft',  'spinster',  'submissive',  \n",
    "                         'supermum',  'support',  'sympath',  'sympathy',  'tactful',  'tease',  'tender',  'together',  'tomboy ',  'trollop',  \n",
    "                         'trust',  'understand',  'victim',  'virgin',  'vivacious',  'warm',  'weak',  'wedding',  'weight',  'whin',  \n",
    "                         'whitefemaleness',  'whore',  'womanlier',  'womanliest',  'womanliness',  'womanly',  'yield'])\n",
    "\n",
    "# words we are doubtful about: \"model\", \"accuse\", \"teen\", \"young\", \"home\",\"question\", \"date\", \"good\", \"hair\", \"couple\", \"teenage\", \"fail\", \"struggle\",\n",
    "\n",
    "male_words = set(['hero', 'man',  'men', 'his', 'him', 'he', 'husband', 'father', 'male', 'son', 'god',\n",
    "                  'prince', 'king', 'mr', 'sir', 'brother', 'grandfather', 'uncle', 'nephew', 'master', 'patriarch',\n",
    "                 'chairman', 'chairmen', \"boy\",\"boyfriend\", \"save\", \"guy\", \"dad\"])\n",
    "\n",
    "discrimination_words = set(['race', 'caste', 'casteless', 'black', 'SC', 'ST', 'african american', 'white', 'colour', \n",
    "                            'color', \"brown\", \"asian\", \"native\", \"racial\", \"minority\", \"ethnic\", \"ethnicity\", \"indian\",\n",
    "                            'hindu', \"muslim\", \"chinese\", \"indian\"])\n",
    "\n",
    "male_bias_words = set([\"active\", \"adventurous\", \"aggression\",'aggressive', \"ambition\", \"assert\", 'assertive',\n",
    "                       \"athlete\", 'athletic', \"battle\", \"champion\", \"decisive\", 'head', 'dominate', 'dominant',\n",
    "                        \"driven\", 'confident', 'strong', 'force', 'master', 'superior', 'strength', 'bold', \n",
    "                       'ambitious', 'power', 'intelligent', 'greedy', 'hostile', 'uncaring', 'logic', 'logical', 'rational',\n",
    "                       \"fearless\",'stubborn', 'independent', 'objective', \"charismatic\"])\n",
    "\n",
    "empowerement_words = set(['chairperson', 'leader','leadership',  'chairwoman', 'minister', 'power','powerful', 'authority', \n",
    "                          'queen', 'manager', 'success', 'successful', 'successes', 'career', 'job',\n",
    "                         'CEO','CFO', 'chief', 'officer', 'employment', 'employed', 'millionaire', \n",
    "                          'wealth', 'wealthy', 'strong', 'strength', 'courage','achievement', 'achievements', \n",
    "                          'achieve', 'goal', 'ambition', 'ambitious', 'passionate','passion', 'badass', \n",
    "                          'confident', 'confidence', 'breakthrough', \"inspirational\", \"educated\"\n",
    "                         'inspiring', 'inspiration', 'inspire', 'empower', 'empowered', 'empowerement',\n",
    "                         'genius', 'expert', 'mastery', 'owner', 'businesswoman', 'intelligent', 'smart', \n",
    "                          'clever', 'wise', 'worth', 'role model', 'role-model', 'activist', \"pay\", \"work\", \n",
    "                          \"business\", \"win\", \"award\", \"appoint\", \"lead\", \"star\", \"boss\", \"dream\",'goddess', \n",
    "                          \"actor\", 'queen', \"launch\", \"worker\", \"lawyer\", \"education\", \"director\", \"protester\", \n",
    "                          \"protest\", \"governor\", \"survive\", \"stallion\", \"doctor\", \"voice\", \"perfect\", \"author\",\n",
    "                          \"mayor\", \"founder\", \"abortion\", \"rise\", \"1st\", \"winner\", \"artist\", \"graduate\", \n",
    "                          \"employee\", \"earning\", \"survivor\", \"scientist\", \"equality\", \"equal\", \"deputy\", \n",
    "                          \"entrepreneur\", \"survive\", \"parent\"])\n",
    "\n",
    "politics_words = set([\"trump\", \"biden\", \"kamala\", \"harris\", \"joe\", \"vote\", \"election\", \"president\", \"elect\", \n",
    "                      \"state\", \"government\", \"obama\", \"office\", \"campaign\", \"melania\", \"vice\", \"govt\", \"donald\", \n",
    "                      \"voter\", \"congress\", \"candidate\", \"political\", \"breonna\", \"taylor\", \"pelosi\", \"democratic\",\n",
    "                      \"politic\", \"democrats\", \"republican\", \"ivanka\", \"republicans\", \"hillary\", \"clinton\", \"susan\", \n",
    "                      \"collins\", \"warren\", \"rep\", \"rally\", \"debate\", \"senate\", \"washington\", \"speech\", \"presidential\",\n",
    "                      \"boris\", \"mandela\"])\n",
    "\n",
    "violence_words = set([\"find\", \"allegedly\", \"fire\", \"life\", 'violent', 'violence', 'crime', 'rape','rapist', 'raped', 'murder','kill', 'killed','killer',\n",
    "                     'murdered', 'murderer', 'attack', 'alleged', 'criminal', 'stab', 'knife', 'gun', 'guns', 'knives',\n",
    "                     'blood', 'bloodshed', 'court', 'rage', 'outrage', 'rob', 'steal', 'robber', 'stealer', 'beater', \"beaten\",\n",
    "                     'domestic violence', 'aggression', 'aggressor', 'war', 'battle', 'abduction', 'assault', 'assaulted',\n",
    "                     'drug', 'abuse', 'child abuse', 'prison', 'fraud', 'human traffic', 'homicide', 'organised crime',\n",
    "                     'organized crime', 'genocide', 'fight', 'manslaughter', 'terrorist', 'weapon', 'smuggl', 'shoplift',\n",
    "                     'vandalism', 'crime', 'theft', 'penalty', 'prison sentence', 'detained', 'guilty', 'trial',\n",
    "                     'defense', 'defend', 'armed', 'jail', 'illegal', 'accomplice',\n",
    "                     'alcohol', 'allegation', 'arson', 'bail', 'battery', 'dead', 'death', 'deadly', 'corrupt', 'killer', \n",
    "                     'sex crime', 'wanted', \"arrest\", \"police\", \"die\", \"charge\", \"suspect\", \"shoot\", \"sentence\", \"cop\", \"hit\", \n",
    "                     \"break\", \"beat\", \"judge\", \"kidnap\", \"law\", \"corruption\", \"gang\", \"suicide\", \"critically injured\", 'harassment',\n",
    "                     \"run\", \"crash\", \"hospital\", 'security', \"report\", \"risk\", \"fall\", \"burn\", \"escape\", \"threaten\", \"slam\",\n",
    "                     \"gangrape\", \"harass\", \"brutally\", \"drown\", \"justice\", \"hate\", \"racist\", \"allege\", \"lawsuit\", \"injure\", \"racism\", \n",
    "                      \"thug\", \"suffer\", \"injury\", \"horror\", \"killing\", \"robbery\", \"plead\", \"wound\", \"kidnapping\", \"convict\", \"shooting\"])\n",
    "\n",
    "female_bias = female_words.union(female_bias_words)\n",
    "male_bias = male_words.union(male_bias_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_hl_words =[]\n",
    "# all_marked_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_marked_words =  female_words.union(female_bias_words).union(male_words).union(discrimination_words).union(male_bias_words).union(empowerement_words).union(violence_words)\n",
    "# #all_marked_words = ast.literal_eval(all_marked_words)\n",
    "# #list(female_bias)\n",
    "# all_marked_words = list(all_marked_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def addedges(list):\n",
    "#     edges = []\n",
    "#     for i in combinations(list,2):\n",
    "#         edges.append(i)\n",
    "#     G.add_edges_from(edges) \n",
    "#     print('added edge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def usefulwordassociations(list):\n",
    "#     new_list = [x for x in list if x in all_marked_words]\n",
    "#     return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hl_words(all_hl_words):\n",
    "    for index,value in enumerate(all_hl_words):\n",
    "        if len(value) == 1:\n",
    "            all_hl_words.remove(value)\n",
    "\n",
    "    for index,value in enumerate(all_hl_words):\n",
    "        if value.isalpha() == False:\n",
    "            all_hl_words.remove(value)\n",
    "    return all_hl_words       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleanwordassociations(list):\n",
    "#     new_list = [x for x in list if x.isalpha == False]\n",
    "#     new_list = [x for x in new_list if len(x)>1]\n",
    "#     return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createfullnetwork(df, column=\"clean_h1_verbs_adjs\"):\n",
    "    G = nx.MultiGraph()\n",
    "    all_hl_words = []\n",
    "    for i in range(len(df)):\n",
    "        word_associations = []\n",
    "        all_hl_words += clean_hl_words(df[column][i])\n",
    "        \n",
    "        #all_hl_words = clean_hl_words(all_hl_words)\n",
    "        #word_associations = cleanwordassociations(ast.literal_eval(df['clean_hl_words'][i]))\n",
    "        \n",
    "        word_associations = df[column][i]\n",
    "        \n",
    "        #print(type(word_associations))\n",
    "        #addedges(word_associations)\n",
    "        \n",
    "        edges = []\n",
    "        for i in combinations(word_associations,2):\n",
    "            edges.append(i)\n",
    "        G.add_edges_from(edges) \n",
    "        \n",
    "    #print('added edge')\n",
    "    G.add_nodes_from(all_hl_words)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWeightedGraph(g_test):\n",
    "    G = nx.Graph()\n",
    "    for u,v,data in g_test.edges(data=True):\n",
    "        w = data['weight'] if 'weight' in data else 1.0\n",
    "        if G.has_edge(u,v):\n",
    "            G[u][v]['weight'] += int(w)\n",
    "        else:\n",
    "            G.add_edge(u, v, weight=w)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNodeFreq(g, freq):\n",
    "    nx.set_node_attributes(g, 0, \"frequency\")\n",
    "    nx.set_node_attributes(g, 0, \"perc_freq\")\n",
    "    for u in g.nodes(data=True):\n",
    "        node = u[0]\n",
    "        if len(freq[freq['word']== str(u[0])])>0:\n",
    "            g.nodes[str(node)]['frequency'] = int(freq[freq['word']== str(u[0])]['frequency'].values[0])\n",
    "            g.nodes[str(node)]['perc_freq'] = int(freq[freq['word']== str(u[0])]['perc_freq'].values[0]*100000)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_themes(g):\n",
    "    nx.set_node_attributes(g, \"\", \"theme\")\n",
    "    for u in g.nodes(data=True):\n",
    "        node = u[0]\n",
    "        if node in female_bias:\n",
    "             g.nodes[str(node)]['theme'] = \"female_bias\"\n",
    "        elif node in male_bias:\n",
    "             g.nodes[str(node)]['theme'] = \"male_bias\"\n",
    "        elif node in empowerement_words:\n",
    "             g.nodes[str(node)]['theme'] = \"empowerment\"\n",
    "        elif node in violence_words:\n",
    "             g.nodes[str(node)]['theme'] = \"violence\"\n",
    "        elif node in politics_words:\n",
    "             g.nodes[str(node)]['theme'] = \"politics\"\n",
    "        elif node in discrimination_words:\n",
    "             g.nodes[str(node)]['theme'] = \"race\"\n",
    "        else:\n",
    "            g.nodes[str(node)]['theme'] = \"other\"\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_themes(g):\n",
    "    nx.set_edge_attributes(g, \"\", \"theme\")\n",
    "    for u,v,data in g.edges(data=True):\n",
    "    #     print(data['theme'])\n",
    "    #     print(g[u][v]['weight'])\n",
    "        if u in female_bias:\n",
    "             g[u][v]['theme'] = \"female_bias\"\n",
    "        elif u in male_bias:\n",
    "             g[u][v]['theme'] = \"male_bias\"\n",
    "        elif u in empowerement_words:\n",
    "             g[u][v]['theme'] = \"empowerment\"\n",
    "        elif u in violence_words:\n",
    "             g[u][v]['theme'] = \"violence\"\n",
    "        elif u in politics_words:\n",
    "             g[u][v]['theme'] = \"politics\"\n",
    "        elif u in discrimination_words:\n",
    "             g[u][v]['theme'] = \"race\"\n",
    "        else:\n",
    "            g[u][v]['theme'] = \"other\"\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFinalNetwork(df, freq, edge_filter=2, node_filter=2):\n",
    "    # create network\n",
    "    g = createfullnetwork(df)\n",
    "    # get edge weights\n",
    "    g = createWeightedGraph(g)\n",
    "    # get node themes\n",
    "    g = get_node_themes(g)\n",
    "    # get edge themes\n",
    "    g = get_edge_themes(g)\n",
    "    # get node frequency\n",
    "    g = getNodeFreq(g, freq)\n",
    "    print(len(g.nodes), \"nodes\", \"and\", len(g.edges), \"edges before reducing\") \n",
    "    # remove irrelevant noise\n",
    "    ## add irrelevant words here\n",
    "    words = ['say', 'new', 'time', \"south\", \"africa\", \"india\", \"england\", \"united states\", \"american\", \"americans\", \n",
    "             \"UK\", \"get\", \"video\", \"tell\", \"cape\", \"makesefakese\", \"not\", \"no\", \"ask\", \"november\", \"trump\", \"biden\", \n",
    "             \"pandemic\", \"theill\", \"covid\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    remove_edges = [(u,v) for u, v, data in g.edges(data=True) if data[\"weight\"] < edge_filter]\n",
    "    g.remove_edges_from(remove_edges)\n",
    "\n",
    "    deg = g.degree()\n",
    "    remove_nodes = [node for node,degree in dict(g.degree()).items() if degree < node_filter]\n",
    "#     remove_nodes = []\n",
    "#     remove_nodes = [node[0] for node in g.nodes(data=True) if g.nodes[node[0]][\"perc_freq\"] <= node_filter]\n",
    "    remove_nodes.extend(words)\n",
    "    g.remove_nodes_from(remove_nodes)\n",
    "    print(len(g.nodes), \"nodes\", \"and\", len(g.edges), \"edges after reducing\")     \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_data(freq, df, country):\n",
    "    country_freqs = freq[freq['country']==country].reset_index()\n",
    "    country_df = df[df['country']==country].reset_index()\n",
    "    return country_df, country_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveNetwork(g, country, net_type=\"all\"):\n",
    "    connections = json_graph.node_link_data(g)\n",
    "    connections_json = json.dumps(connections, indent = 2) \n",
    "    with open(f\"../data/processed/word_connections_{country}_{net_type}.json\", \"w\") as outfile: \n",
    "        outfile.write(connections_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2543 nodes and 9920 edges before reducing\n",
      "271 nodes and 644 edges after reducing\n",
      "3344 nodes and 15308 edges before reducing\n",
      "353 nodes and 920 edges after reducing\n",
      "3488 nodes and 14239 edges before reducing\n",
      "306 nodes and 843 edges after reducing\n",
      "1762 nodes and 4799 edges before reducing\n",
      "171 nodes and 313 edges after reducing\n"
     ]
    }
   ],
   "source": [
    "freq = pd.read_csv(\"../data/processed/countries_freq.csv\")\n",
    "# country = \"India\" #South Africa, UK, USA, India\n",
    "# # words = list(freq[freq['country']!=country].word.values)\n",
    "# country_freqs = freq[freq['country']!=country].reset_index()\n",
    "# country_df = df[df['country']==country].reset_index()\n",
    "ef = 2\n",
    "nf = 2\n",
    "\n",
    "g_IN = getFinalNetwork(df = get_country_data(freq=freq, df=df, country = \"India\")[0], freq = get_country_data(freq=freq, df=df, country = \"India\")[1], edge_filter=ef, node_filter=nf)\n",
    "# saveNetwork(g_IN, \"IN\", net_type=\"verbs_adjs\")\n",
    "g_USA = getFinalNetwork(df = get_country_data(freq=freq, df=df, country = \"USA\")[0], freq = get_country_data(freq=freq, df=df, country = \"USA\")[1], edge_filter=ef, node_filter=nf)\n",
    "# saveNetwork(g_USA, \"USA\", net_type=\"verbs_adjs\")\n",
    "g_UK = getFinalNetwork(get_country_data(freq, df, \"UK\")[0], get_country_data(freq, df, \"UK\")[1], edge_filter=ef, node_filter=nf)\n",
    "# saveNetwork(g_UK, \"UK\", net_type=\"verbs_adjs\")\n",
    "g_SA = getFinalNetwork(get_country_data(freq, df, \"South Africa\")[0], get_country_data(freq, df, \"South Africa\")[1], edge_filter=ef, node_filter=nf)\n",
    "# saveNetwork(g_SA, \"SA\", net_type=\"verbs_adjs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveNetwork(g_IN, \"IN\", net_type=\"verbs_adjs\")\n",
    "saveNetwork(g_USA, \"USA\", net_type=\"verbs_adjs\")\n",
    "saveNetwork(g_UK, \"UK\", net_type=\"verbs_adjs\")\n",
    "saveNetwork(g_SA, \"SA\", net_type=\"verbs_adjs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13181 nodes and 215872 edges before reducing\n",
      "91 nodes and 175 edges after reducing\n",
      "15094 nodes and 294534 edges before reducing\n",
      "92 nodes and 157 edges after reducing\n",
      "16674 nodes and 282702 edges before reducing\n",
      "87 nodes and 142 edges after reducing\n",
      "10476 nodes and 104227 edges before reducing\n",
      "83 nodes and 83 edges after reducing\n"
     ]
    }
   ],
   "source": [
    "freq = pd.read_csv(\"../data/processed/countries_freq.csv\")\n",
    "# country = \"India\" #South Africa, UK, USA, India\n",
    "# # words = list(freq[freq['country']!=country].word.values)\n",
    "# country_freqs = freq[freq['country']!=country].reset_index()\n",
    "# country_df = df[df['country']==country].reset_index()\n",
    "g_IN = getFinalNetwork(df = get_country_data(freq=freq, df=df, country = \"India\")[0], freq = get_country_data(freq=freq, df=df, country = \"India\")[1], edge_filter=10, node_filter=200)\n",
    "saveNetwork(g_IN, \"IN_small\")\n",
    "g_USA = getFinalNetwork(df = get_country_data(freq=freq, df=df, country = \"USA\")[0], freq = get_country_data(freq=freq, df=df, country = \"USA\")[1], edge_filter=10, node_filter=200)\n",
    "saveNetwork(g_USA, \"USA_small\")\n",
    "g_UK = getFinalNetwork(get_country_data(freq, df, \"UK\")[0], get_country_data(freq, df, \"UK\")[1], edge_filter=10, node_filter=200)\n",
    "saveNetwork(g_UK, \"UK_small\")\n",
    "g_SA = getFinalNetwork(get_country_data(freq, df, \"South Africa\")[0], get_country_data(freq, df, \"South Africa\")[1], edge_filter=10, node_filter=200)\n",
    "saveNetwork(g_SA, \"SA_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_nodes_IN = [node[0] for node in g_IN.nodes(data=True) if g_IN.nodes[node[0]][\"theme\"] == \"other\"]\n",
    "grey_nodes_USA = [node[0] for node in g_USA.nodes(data=True) if g_USA.nodes[node[0]][\"theme\"] == \"other\"]\n",
    "grey_nodes_UK = [node[0] for node in g_UK.nodes(data=True) if g_UK.nodes[node[0]][\"theme\"] == \"other\"]\n",
    "grey_nodes_SA = [node[0] for node in g_SA.nodes(data=True) if g_SA.nodes[node[0]][\"theme\"] == \"other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reveal',\n",
       " 'real',\n",
       " 'indian',\n",
       " 'covid',\n",
       " 'treatment',\n",
       " 'security',\n",
       " 'come',\n",
       " 'claim',\n",
       " 'study',\n",
       " 'hand',\n",
       " 'question',\n",
       " 'diwali',\n",
       " 'post',\n",
       " 'report',\n",
       " 'hathras',\n",
       " 'not',\n",
       " 'kerala',\n",
       " 'act',\n",
       " 'state',\n",
       " 'government',\n",
       " 'eat',\n",
       " 'right',\n",
       " 'food',\n",
       " 'school',\n",
       " 'grow',\n",
       " 'bengaluru',\n",
       " 'share',\n",
       " 'tip',\n",
       " 'great',\n",
       " 'risk',\n",
       " 'long',\n",
       " 'set',\n",
       " 'way',\n",
       " 'tell',\n",
       " 'official',\n",
       " 'face',\n",
       " 'day',\n",
       " 'obama',\n",
       " 'leave',\n",
       " 'office',\n",
       " 'go',\n",
       " 'troll',\n",
       " 'online',\n",
       " 'date',\n",
       " 'app',\n",
       " 'give',\n",
       " 'struggle',\n",
       " 'like',\n",
       " 'fall',\n",
       " 'single',\n",
       " 'stay',\n",
       " 'style',\n",
       " 'room',\n",
       " 'pregnancy',\n",
       " 'shilpa',\n",
       " 'shetty',\n",
       " 'happy',\n",
       " 'hour',\n",
       " 'singh',\n",
       " 'character',\n",
       " 'look',\n",
       " 'rajput',\n",
       " 'kapoor',\n",
       " 'hair',\n",
       " 'health',\n",
       " 'sell',\n",
       " 'late',\n",
       " 'malaika',\n",
       " 'arora',\n",
       " 'help',\n",
       " 'loss',\n",
       " 'mask',\n",
       " 'deepika',\n",
       " 'padukone',\n",
       " 'makeup',\n",
       " 'hang',\n",
       " 'eye',\n",
       " 'want',\n",
       " 'fact',\n",
       " 'taapsee',\n",
       " 'pannu',\n",
       " 'film',\n",
       " 'ape',\n",
       " 'priyanka',\n",
       " 'chopra',\n",
       " 'world',\n",
       " 'moment',\n",
       " 'try',\n",
       " 'fashion',\n",
       " 'story',\n",
       " 'talk',\n",
       " 'special',\n",
       " 'sonam',\n",
       " 'dog',\n",
       " 'drop',\n",
       " 'dress',\n",
       " 'week',\n",
       " 'adorable',\n",
       " 'keep',\n",
       " 'khan',\n",
       " 'quit',\n",
       " 'friend',\n",
       " 'amid',\n",
       " 'pandemic',\n",
       " 'crush',\n",
       " 'kumar',\n",
       " 'campaign',\n",
       " 'throw',\n",
       " 'party',\n",
       " 'year',\n",
       " 'melania',\n",
       " 'end',\n",
       " 'australia',\n",
       " 'thing',\n",
       " 'birth',\n",
       " 'toilet',\n",
       " 'catch',\n",
       " 'couple',\n",
       " 'constable',\n",
       " 'china',\n",
       " 'test',\n",
       " 'coronavirus',\n",
       " 'attempt',\n",
       " 'well',\n",
       " 'think',\n",
       " 'age',\n",
       " 'burn',\n",
       " 'alive',\n",
       " 'people',\n",
       " 'cancer',\n",
       " 'spend',\n",
       " 'night',\n",
       " 'change',\n",
       " 'selfie',\n",
       " 'learn',\n",
       " 'write',\n",
       " 'raise',\n",
       " 'picture',\n",
       " 'viral',\n",
       " 'netflix',\n",
       " 'sexually',\n",
       " 'person',\n",
       " 'doctor',\n",
       " 'parent',\n",
       " 'hold',\n",
       " 'update',\n",
       " 'november',\n",
       " 'make',\n",
       " 'plan',\n",
       " 'vice',\n",
       " 'disha',\n",
       " 'twitter',\n",
       " 'fan',\n",
       " 'stop',\n",
       " 'move',\n",
       " 'mumbai',\n",
       " 'probe',\n",
       " 'diana',\n",
       " 'big',\n",
       " 'watch',\n",
       " 'ali',\n",
       " 'read',\n",
       " 'minor',\n",
       " 'hindu',\n",
       " 'muslim',\n",
       " 'bengal',\n",
       " 'member',\n",
       " 'ask',\n",
       " 'kolkata',\n",
       " 'student',\n",
       " 'start',\n",
       " 'trend',\n",
       " 'old',\n",
       " 'crorepati',\n",
       " 'take',\n",
       " 'cut',\n",
       " 'news',\n",
       " 'social',\n",
       " 'medium',\n",
       " 'air',\n",
       " 'instagram',\n",
       " 'escape',\n",
       " 'know',\n",
       " 'sushmita',\n",
       " 'sen',\n",
       " 'open',\n",
       " 'threaten',\n",
       " 'pilot',\n",
       " 'saxena',\n",
       " 'village',\n",
       " 'rule',\n",
       " 'board',\n",
       " 'discover',\n",
       " 'meet',\n",
       " 'show',\n",
       " 'vs',\n",
       " 'name',\n",
       " 'pass',\n",
       " 'relationship',\n",
       " 'kashmir',\n",
       " 'delhi',\n",
       " 'support',\n",
       " 'self',\n",
       " 'passenger',\n",
       " 'flight',\n",
       " 'rise',\n",
       " 'bihar',\n",
       " 'pakistan',\n",
       " 'gandhi',\n",
       " 'safety',\n",
       " 'uttar',\n",
       " 'pradesh',\n",
       " 'modi',\n",
       " 'case',\n",
       " 'medical',\n",
       " 'injure',\n",
       " 'bear',\n",
       " 'play',\n",
       " 'slam',\n",
       " 't20',\n",
       " 'challenge',\n",
       " 'pak',\n",
       " 'inside',\n",
       " 'tribute',\n",
       " 'bid',\n",
       " 'airport',\n",
       " 'debut',\n",
       " 'release',\n",
       " 'trailer',\n",
       " 'role',\n",
       " 'reason',\n",
       " 'movie',\n",
       " 'away',\n",
       " 'today',\n",
       " 'announce',\n",
       " 'group',\n",
       " 'pakistani',\n",
       " 'bollywood',\n",
       " 'tamil',\n",
       " 'team',\n",
       " 'no',\n",
       " 'call',\n",
       " 'turn',\n",
       " 'book',\n",
       " 'chakraborty',\n",
       " 'cricket',\n",
       " 'cup',\n",
       " 'ago',\n",
       " 'worker',\n",
       " 'demand',\n",
       " 'rhea',\n",
       " 'lawyer',\n",
       " 'record',\n",
       " 'wear',\n",
       " 'kangana',\n",
       " 'rajasthan',\n",
       " 'period',\n",
       " 'need',\n",
       " 'allow',\n",
       " 'pic',\n",
       " 'divorce',\n",
       " 'neha',\n",
       " 'govt',\n",
       " 'donald',\n",
       " 'bigg',\n",
       " 'create',\n",
       " 'host',\n",
       " 'stunning',\n",
       " 'photo',\n",
       " 'rahul',\n",
       " 'let',\n",
       " 'use',\n",
       " 'bachchan',\n",
       " 'anti',\n",
       " 'human',\n",
       " 'gujarat',\n",
       " 'prove',\n",
       " 'assam',\n",
       " 'refuse',\n",
       " 'college',\n",
       " 'class',\n",
       " 'vaccine',\n",
       " 'travel',\n",
       " 'celebrate',\n",
       " 'dad',\n",
       " 'allege',\n",
       " 'issue',\n",
       " 'stand',\n",
       " 'speak',\n",
       " 'see',\n",
       " 'teach',\n",
       " 'karnataka',\n",
       " 'seek',\n",
       " 'money',\n",
       " 'sharma',\n",
       " 'education',\n",
       " 'hot',\n",
       " 'check',\n",
       " 'jihad',\n",
       " 'kashmiri',\n",
       " 'sushant',\n",
       " 'tweet',\n",
       " 'fake',\n",
       " 'account',\n",
       " 'teacher',\n",
       " 'heart',\n",
       " 'house',\n",
       " 'return',\n",
       " 'telangana',\n",
       " 'hospital',\n",
       " 'patient',\n",
       " 'lakh',\n",
       " 'ranaut',\n",
       " 'internet',\n",
       " 'explain',\n",
       " 'army',\n",
       " 'tara',\n",
       " 'maldives',\n",
       " 'birthday',\n",
       " '1st',\n",
       " 'ld',\n",
       " 'second',\n",
       " 'traffic',\n",
       " 'positive',\n",
       " 'voter',\n",
       " 'gangrape',\n",
       " 'dalit',\n",
       " 'journalist',\n",
       " 'rs',\n",
       " 'bank',\n",
       " 'yogi',\n",
       " 'protest',\n",
       " 'file',\n",
       " 'car',\n",
       " 'high',\n",
       " 'centre',\n",
       " 'month',\n",
       " 'acid',\n",
       " 'national',\n",
       " 'commission',\n",
       " 'speech',\n",
       " 'thrash',\n",
       " 'outside',\n",
       " 'haryana',\n",
       " 'live',\n",
       " 'trailblazer',\n",
       " 'send',\n",
       " 'order',\n",
       " 'gift',\n",
       " 'note',\n",
       " 'west',\n",
       " 'visit',\n",
       " 'recover',\n",
       " 'entrepreneur',\n",
       " 'poll',\n",
       " 'exam',\n",
       " 'chinese',\n",
       " 'earn',\n",
       " 'list',\n",
       " 'wish',\n",
       " 'reply',\n",
       " 'complaint',\n",
       " 'goa',\n",
       " 'chennai',\n",
       " 'pick',\n",
       " 'punjab',\n",
       " 'nadu',\n",
       " 'song',\n",
       " 'mehbooba',\n",
       " 'c19',\n",
       " 'soon',\n",
       " 'likely',\n",
       " 'walk',\n",
       " 'netizen',\n",
       " 'rescue',\n",
       " 'road',\n",
       " 'review',\n",
       " 'action',\n",
       " 'future',\n",
       " 'youth',\n",
       " 'lockdown',\n",
       " 'free',\n",
       " 'choose',\n",
       " 'safe',\n",
       " 'congress',\n",
       " 'candidate',\n",
       " 'indira',\n",
       " 'anniversary',\n",
       " 'bus',\n",
       " 'little',\n",
       " 'proud',\n",
       " 'red',\n",
       " 'international',\n",
       " 'train',\n",
       " 'railway',\n",
       " 'step',\n",
       " 'exclusive',\n",
       " 'message',\n",
       " 'gold',\n",
       " 'celebration',\n",
       " 'driver',\n",
       " 'commit',\n",
       " 'teenage',\n",
       " 'experience',\n",
       " 'pune',\n",
       " 'elderly',\n",
       " 'bring',\n",
       " 'station',\n",
       " 'quarantine',\n",
       " 'field',\n",
       " 'shah',\n",
       " 'tribal',\n",
       " 'join',\n",
       " 'treat',\n",
       " 'water',\n",
       " 'convert',\n",
       " 'street',\n",
       " 'lover',\n",
       " 'affair',\n",
       " 'shocker',\n",
       " 'bag',\n",
       " 'kareena',\n",
       " 'bhai',\n",
       " 'remember',\n",
       " 'clothe',\n",
       " 'sara',\n",
       " 'tree',\n",
       " 'accident',\n",
       " 'saree',\n",
       " 'maharashtra',\n",
       " 'dance',\n",
       " 'tie',\n",
       " 'pet',\n",
       " 'pose',\n",
       " 'hyderabad',\n",
       " 'harass',\n",
       " 'andhra',\n",
       " 'noida',\n",
       " 'molest',\n",
       " 'kanpur',\n",
       " 'brutally',\n",
       " 'gurugram',\n",
       " 'resist',\n",
       " 'near',\n",
       " 'drown',\n",
       " 'blackmail',\n",
       " 'odisha',\n",
       " 'ablaze',\n",
       " 'drive',\n",
       " 'justice',\n",
       " 'district',\n",
       " 'madhya',\n",
       " 'srinagar',\n",
       " 'local',\n",
       " 'carry',\n",
       " 'puja',\n",
       " 'enjoy',\n",
       " 'katrina',\n",
       " 'deny',\n",
       " 'alia',\n",
       " 'bhatt',\n",
       " 'km',\n",
       " 'neighbour',\n",
       " 'deliver',\n",
       " 'welcome',\n",
       " 'director',\n",
       " 'nitish',\n",
       " 'kaif',\n",
       " 'cycle',\n",
       " 'journey',\n",
       " 'toll',\n",
       " 'anushka',\n",
       " 'sari',\n",
       " 'jk']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'theme': 'violence', 'frequency': 250, 'perc_freq': 946}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_SA.nodes['rape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009463244757362405"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq[freq['word']== \"rape\"]['perc_freq'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0005*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10415    1431.440078\n",
       "10416    1363.009165\n",
       "10417    1312.527344\n",
       "10418    1020.854601\n",
       "10419     701.136402\n",
       "            ...     \n",
       "29855       1.121818\n",
       "29856       1.121818\n",
       "29857       1.121818\n",
       "29858       1.121818\n",
       "29859       1.121818\n",
       "Name: perc_freq, Length: 19445, dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq[freq['country']==\"USA\"]['perc_freq']*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'perc_freq'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-865dbd65cf79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mg_USA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg_USA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"perc_freq\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-91-865dbd65cf79>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mg_USA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg_USA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"perc_freq\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'perc_freq'"
     ]
    }
   ],
   "source": [
    "len([node[0] for node in g_USA.nodes(data=True) if g_USA.nodes[node[0]][\"perc_freq\"] <= 0.0005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([(u,v) for u, v, data in g_IN.edges(data=True) if data[\"weight\"] <= 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_edges = [(u,v) for u, v, data in g.edges(data=True) if data[\"weight\"] <= 3]\n",
    "remove_nodes = [node[0] for node in g.nodes(data=True) if g.nodes[node[0]][\"frequency\"] <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_SA = getNodeFreq(g_SA, country_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_SA.edges['man', \"rape\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_SA.nodes['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1331"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len([node[0]  for node in g_SA.nodes(data=True) if g_SA.nodes[node[0]][\"frequency\"] <= 5])\n",
    "len([(u,v) for u, v, data in g_IN.edges(data=True) if data[\"weight\"] <= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_edges = [(u,v) for u, v, data in g_IN.edges(data=True) if data[\"weight\"] <= 3]\n",
    "remove_nodes = [node[0] for node in g_IN.nodes(data=True) if g_IN.nodes[node[0]][\"frequency\"] <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_SA.remove_edges_from(remove_edges)\n",
    "g_SA.remove_nodes_from(remove_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209850"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_IN.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34311\n",
      "1173780\n"
     ]
    }
   ],
   "source": [
    "g = createfullnetwork(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34311\n",
      "1173780\n"
     ]
    }
   ],
   "source": [
    "g = createfullnetwork(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_weighted = createWeightedGraph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>say</td>\n",
       "      <td>2146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>new</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>man</td>\n",
       "      <td>1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>trump</td>\n",
       "      <td>1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>black</td>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   word  frequency\n",
       "0           0    say       2146\n",
       "1           1    new       1940\n",
       "2           2    man       1774\n",
       "3           3  trump       1573\n",
       "4           4  black       1235"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = []\n",
    "nx.set_node_attributes(G, frequency, \"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in g_weighted.nodes(data=True):\n",
    "    node = u[0]\n",
    "    if len(freq[freq['word']== str(u[0])])>0:\n",
    "        g_weighted.nodes[str(node)]['frequency'] = int(freq[freq['word']== str(u[0])]['frequency'].values[0])\n",
    "   # print(u[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frequency': 937}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_weighted.nodes['die']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>crown</td>\n",
       "      <td>v</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>crown</td>\n",
       "      <td>di</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>crown</td>\n",
       "      <td>diana</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>le</td>\n",
       "      <td>di</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>le</td>\n",
       "      <td>diana</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12603</th>\n",
       "      <td>supply</td>\n",
       "      <td>problem</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12606</th>\n",
       "      <td>problem</td>\n",
       "      <td>reckon</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12868</th>\n",
       "      <td>monthly</td>\n",
       "      <td>archives</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12871</th>\n",
       "      <td>monthly</td>\n",
       "      <td>archive</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13092</th>\n",
       "      <td>archives</td>\n",
       "      <td>tag</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>795 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         source    target  weight\n",
       "0         crown         v       2\n",
       "6         crown        di       2\n",
       "7         crown     diana       3\n",
       "65           le        di       2\n",
       "66           le     diana       2\n",
       "...         ...       ...     ...\n",
       "12603    supply   problem       2\n",
       "12606   problem    reckon       2\n",
       "12868   monthly  archives       4\n",
       "12871   monthly   archive       3\n",
       "13092  archives       tag       2\n",
       "\n",
       "[795 rows x 3 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_test = nx.to_pandas_edgelist(G)\n",
    "edges_test[edges_test['weight']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "g = createfullnetwork(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "70\n",
      "357\n"
     ]
    }
   ],
   "source": [
    "G = createfullnetwork(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_test.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node attributes to add -\n",
    "1. which dictionary it belongs to\n",
    "2. frequency of node\n",
    "3. frequency of edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = json_graph.node_link_data(g_weighted)\n",
    "connections_json = json.dumps(connections, indent = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/word_connections.json\", \"w\") as outfile: \n",
    "    outfile.write(connections_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231     ['crown', 'v', 'realt', 'le', 'foto', 'dei', '...\n",
       "738     ['crown', 'riveler', 'nuovi', 'segreti', 'anch...\n",
       "2798    ['play', 'princess', 'diana', 'netflix', 'crown']\n",
       "3885    ['fact', 'crown', 'star', 'emma', 'corrin', 'l...\n",
       "3970    ['princess', 'diana', 'camilla', 'parker', 'bo...\n",
       "Name: clean_hl_words, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[(df['headline'].str.lower().str.contains('diana')) & (df['headline'].str.lower().str.contains('crown'))].head()['clean_hl_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.number_of_edges('diana', 'crown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32592"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_filtered.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['useful_word_associations'] = np.empty((len(df), 0)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# time\n",
    "# def calcfilterednetwork(df):\n",
    "#     all_hl_words = []\n",
    "#     all_useful_hl_words = []\n",
    "#     for i in range(len(df)):\n",
    "#         all_hl_words += getusefulnodes(ast.literal_eval(df['clean_hl_words'][i]))\n",
    "#         df['useful_word_associations'].iloc[i] = usefulwordassociations(ast.literal_eval(df['clean_hl_words'][i]))\n",
    "        \n",
    "#         if len(df['useful_word_associations'][i])>=1:\n",
    "#             addedges(df['useful_word_associations'][i])\n",
    "            \n",
    "            \n",
    "#     G.add_nodes_from(all_hl_words)\n",
    "    \n",
    "#     print(len(all_hl_words))  \n",
    "#     return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_filtered = calcfilterednetwork(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Themes addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(filename):\n",
    "    with open(filename) as f:\n",
    "        js_graph = json.load(f)\n",
    "    return json_graph.node_link_graph(js_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = read_json_file(\"../data/processed/word_connections_4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme = \"\"\n",
    "nx.set_node_attributes(g, theme, \"theme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x1ad8a9116a0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_node_themes(g):\n",
    "    for u in g.nodes(data=True):\n",
    "        node = u[0]\n",
    "        if node in female_bias:\n",
    "             g.nodes[str(node)]['theme'] = \"female_bias\"\n",
    "        elif node in male_bias:\n",
    "             g.nodes[str(node)]['theme'] = \"male_bias\"\n",
    "        elif node in empowerement_words:\n",
    "             g.nodes[str(node)]['theme'] = \"empowerment\"\n",
    "        elif node in violence_words:\n",
    "             g.nodes[str(node)]['theme'] = \"violence\"\n",
    "        elif node in politics_words:\n",
    "             g.nodes[str(node)]['theme'] = \"politics\"\n",
    "        elif node in discrimination_words:\n",
    "             g.nodes[str(node)]['theme'] = \"race\"\n",
    "        else:\n",
    "            g.nodes[str(node)]['theme'] = \"other\"\n",
    "            \n",
    "    return g\n",
    "\n",
    "get_node_themes(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frequency': 954, 'theme': 'violence'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes['police']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.remove_node(\"say\")\n",
    "g.remove_node(\"new\")\n",
    "g.remove_node(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_edge_attributes(g, theme, \"theme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_themes(g):\n",
    "    for u,v,data in g.edges(data=True):\n",
    "    #     print(data['theme'])\n",
    "    #     print(g[u][v]['weight'])\n",
    "        if u in female_bias:\n",
    "             g[u][v]['theme'] = \"female_bias\"\n",
    "        elif u in male_bias:\n",
    "             g[u][v]['theme'] = \"male_bias\"\n",
    "        elif u in empowerement_words:\n",
    "             g[u][v]['theme'] = \"empowerment\"\n",
    "        elif u in violence_words:\n",
    "             g[u][v]['theme'] = \"violence\"\n",
    "        elif u in politics_words:\n",
    "             g[u][v]['theme'] = \"politics\"\n",
    "        elif u in discrimination_words:\n",
    "             g[u][v]['theme'] = \"race\"\n",
    "        else:\n",
    "            g[u][v]['theme'] = \"other\"\n",
    "            \n",
    "get_edge_themes(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': 173.0, 'theme': 'politics'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edges[\"joe\", \"biden\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = json_graph.node_link_data(g)\n",
    "connections_json = json.dumps(connections, indent = 2) \n",
    "with open(\"../data/processed/word_connections_4_themes.json\", \"w\") as outfile: \n",
    "    outfile.write(connections_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [(u,v) for u, v, data in g.edges(data=True) if data[\"weight\"] <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83723"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93642"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.remove_edges_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9919"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = json_graph.node_link_data(g)\n",
    "connections_json = json.dumps(connections, indent = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/word_connections_4_themes_filtered.json\", \"w\") as outfile: \n",
    "    outfile.write(connections_json) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countries Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = read_json_file(\"../data/processed/word_connections_4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.read_csv(\"../data/processed/countries_freq.csv\")\n",
    "country = \"South Africa\"\n",
    "words = list(freq[freq['country']!=country].word.values)\n",
    "country_freqs = freq[freq['country']!=country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2032"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.remove_nodes_from(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(g.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(g, 0, \"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frequency': 0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes['die']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get specific country frequencies\n",
    "for u in g.nodes(data=True):\n",
    "    node = u[0]\n",
    "    if len(country_freqs[country_freqs['word']== str(u[0])])>0:\n",
    "        g.nodes[str(node)]['frequency'] = int(freq[freq['word']== str(u[0])]['frequency'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frequency': 143}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.nodes['die']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
