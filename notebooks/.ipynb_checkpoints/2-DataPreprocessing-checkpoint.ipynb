{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all files \n",
    "files = sorted(glob.glob(\"../data/*_articles.csv\"))\n",
    "\n",
    "file_names = []\n",
    "\n",
    "for file_path in files:\n",
    "    file_name = re.split(\"_\", os.path.basename(file_path))[1].split('.')[0]\n",
    "    \n",
    "    hd = pd.read_csv(file_path)\n",
    "    file_names.append(hd)\n",
    "    \n",
    "df = pd.concat(file_names).reset_index().drop([\"Unnamed: 0\", \"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata file\n",
    "newsrooms = pd.read_csv('../data/newsrooms.csv')\n",
    "newsrooms.columns = ['site', 'monthly_visits', 'country', 'country_of_pub']\n",
    "newsrooms.dropna(inplace=True)\n",
    "newsrooms = newsrooms.drop_duplicates(subset = 'site')\n",
    "newsrooms = newsrooms.sort_values(by=['country_of_pub', 'monthly_visits'], ascending=False).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, newsrooms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has 7814 headlines from South Africa 5039 headlines from India 11210 headlines from the UK and 22376 headlines from the U.S.\n"
     ]
    }
   ],
   "source": [
    "print(\"This dataset has\", len(df[df['country_of_pub']=='South Africa']), \"headlines from South Africa\", len(df[df['country_of_pub']=='India']), \"headlines from India\",\n",
    "      len(df[df['country_of_pub']=='UK']), \"headlines from the UK and\", len(df[df['country_of_pub']=='USA']), \"headlines from the U.S.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset covers 29 news outlets from South Africa 18 news outlets from India 43 news outlets from the UK and 90 news outlets from the U.S.\n"
     ]
    }
   ],
   "source": [
    "print(\"This dataset covers\", len(df.site[df['country_of_pub']=='South Africa'].unique()), \"news outlets from South Africa\", len(df.site[df['country_of_pub']=='India'].unique()), \n",
    "      \"news outlets from India\", len(df.site[df['country_of_pub']=='UK'].unique()), \"news outlets from the UK and\", len(df.site[df['country_of_pub']=='USA'].unique()), \"news outlets from the U.S.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoticons_happy = set([\n",
    "#     ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "#     ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "#     '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "#     'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "#     '<3'\n",
    "#     ])\n",
    "\n",
    "# # Sad Emoticons\n",
    "# emoticons_sad = set([\n",
    "#     ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "#     ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "#     ':c', ':{', '>:\\\\', ';('\n",
    "#     ])\n",
    "\n",
    "# #Emoji patterns\n",
    "# emoji_pattern = re.compile(\"[\"\n",
    "#          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#          u\"\\U00002702-\\U000027B0\"\n",
    "#          u\"\\U000024C2-\\U0001F251\"\n",
    "#          \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# #combine sad and happy emoticons\n",
    "# emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_tweets(tweet):\n",
    " \n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     word_tokens = word_tokenize(tweet)\n",
    "#     #after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
    "#     tweet = re.sub(r':', '', tweet)\n",
    "#     tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "#     tweet = tweet.replace(\"'s\", '')\n",
    "#     #replace consecutive non-ASCII characters with a space\n",
    "#     tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "#     #remove emojis from tweet\n",
    "#     tweet = emoji_pattern.sub(r'', tweet)\n",
    "#     tweet = tweet.lower()\n",
    "#     #filter using NLTK library append it to a string\n",
    "#     filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "#     filtered_tweet = []\n",
    "#     #looping through conditions\n",
    "#     for w in word_tokens:\n",
    "#     #check tokens against stop words , emoticons and punctuations\n",
    "#         if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "#             filtered_tweet.append(w)\n",
    "#     return ' '.join(filtered_tweet)\n",
    "#     #print(word_tokens)\n",
    "#     #print(filtered_sentence)return tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# def remove_pattern(input_txt, pattern):\n",
    "#     r = re.findall(pattern, input_txt)\n",
    "#     for i in r:\n",
    "#         input_txt = re.sub(i, '', input_txt)        \n",
    "#     return input_txt\n",
    "# def vec_tweets(tweets):\n",
    "#     #remove twitter Return handles (RT @xxx:)\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\") \n",
    "    \n",
    "#     #remove twitter handles (@xxx)\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"@[\\w]*\")\n",
    "    \n",
    "#     #remove URL links (httpxxx)\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")\n",
    "\n",
    "#     #remove special characters, numbers, punctuations (except for #)\n",
    "#     tweets = np.core.defchararray.replace(tweets, \"[^a-zA-Z]\", \" \")\n",
    "    \n",
    "#     return tweets\n",
    "\n",
    "# # tweets_df['clean_text']= clean_tweets(tweets_df['clean_text'].astype('str')) #The function clean_tweets were put to use.\n",
    "# # tweets_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Choose model accordingly for contractions function\n",
    "# model = api.load(\"glove-twitter-25\")\n",
    "# # model = api.load(\"glove-twitter-100\")\n",
    "# model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# cont = Contractions(kv_model=model)\n",
    "# cont.load_models()\n",
    "\n",
    "\n",
    "# exclude words from spacy stopwords list\n",
    "deselect_stop_words = ['no', 'not', 'hi','\\n','\\n\\n', '&amp;', ' ', '.', '-', 'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
    "for w in deselect_stop_words:\n",
    "    nlp.vocab[w].is_stop = False\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    \"\"\"remove html tags from text\"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    \"\"\"remove extra whitespaces from text\"\"\"\n",
    "    text = text.strip()\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"remove accented characters from text, e.g. café\"\"\"\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# def expand_contractions(text):\n",
    "#     \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n",
    "#     text = list(cont.expand_texts([text], precise=True))[0]\n",
    "#     return text\n",
    "\n",
    "\n",
    "def text_preprocessing(text, accented_chars=True, contractions=False, \n",
    "                       convert_num=True, extra_whitespace=True, \n",
    "                       lemmatization=True, lowercase=True, punctuations=True,\n",
    "                       remove_html=True, remove_num=True, special_chars=True, \n",
    "                       stop_words=True):\n",
    "    \"\"\"preprocess text with default option set to true for all steps\"\"\"\n",
    "    if remove_html == True: #remove html tags\n",
    "        text = strip_html_tags(text)\n",
    "    if extra_whitespace == True: #remove extra whitespaces\n",
    "        text = remove_whitespace(text)\n",
    "    if accented_chars == True: #remove accented characters\n",
    "        text = remove_accented_chars(text)\n",
    "    if contractions == True: #expand contractions\n",
    "        text = expand_contractions(text)\n",
    "    if lowercase == True: #convert all characters to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "    doc = nlp(text) #tokenise text\n",
    "\n",
    "    clean_text = []\n",
    "    \n",
    "    for token in doc:\n",
    "        flag = True\n",
    "        edit = token.text\n",
    "        # remove stop words\n",
    "        if stop_words == True and token.is_stop and token.pos_ != 'NUM': \n",
    "            flag = False\n",
    "        # remove punctuations\n",
    "        if punctuations == True and token.pos_ == 'PUNCT' and flag == True: \n",
    "            flag = False\n",
    "        # remove special characters\n",
    "        if special_chars == True and token.pos_ == 'SYM' and flag == True: \n",
    "            flag = False\n",
    "        # remove numbers\n",
    "        if remove_num == True and (token.pos_ == 'NUM' or token.text.isnumeric()) \\\n",
    "        and flag == True:\n",
    "            flag = False\n",
    "        # convert number words to numeric numbers\n",
    "        if convert_num == True and token.pos_ == 'NUM' and flag == True:\n",
    "            edit = w2n.word_to_num(token.text)\n",
    "        # convert tokens to base form\n",
    "        elif lemmatization == True and token.lemma_ != \"-PRON-\" and flag == True:\n",
    "            edit = token.lemma_\n",
    "        # append tokens edited and not removed to list \n",
    "        if edit != \"\" and flag == True:\n",
    "            clean_text.append(edit)        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leonardo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df['clean_headline'] = df['headline'].apply(text_preprocessing)\n",
    "df['subtitle'][df['subtitle'].isna() == True] = \"\"\n",
    "df['clean_subtitle'] = df['subtitle'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>time</th>\n",
       "      <th>scrape_date</th>\n",
       "      <th>site</th>\n",
       "      <th>monthly_visits</th>\n",
       "      <th>country</th>\n",
       "      <th>country_of_pub</th>\n",
       "      <th>clean_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.netwerk24.com/Vermaak/Celebs/waarh...</td>\n",
       "      <td>Waarheid moet uit oor Diana-onderhoud, s� Will...</td>\n",
       "      <td>WilliamNetwerk244 dagen geleden</td>\n",
       "      <td>4 dagen geleden</td>\n",
       "      <td>23/11/2020</td>\n",
       "      <td>Netwerk24.com</td>\n",
       "      <td>14000000</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>[waarheid, moet, uit, oor, diana, onderhoud, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.netwerk24.com/huisgenoot/Bekendes/...</td>\n",
       "      <td>KYK: Die ooievaar is doenig onder plaaslike be...</td>\n",
       "      <td>Hier is van die oulikste foto's wat plaaslike ...</td>\n",
       "      <td>1 week geleden</td>\n",
       "      <td>23/11/2020</td>\n",
       "      <td>Netwerk24.com</td>\n",
       "      <td>14000000</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>[kyk, die, ooievaar, doenig, onder, plaaslike,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.netwerk24.com/ZA/Weslander/an-arti...</td>\n",
       "      <td>An artist lives her dream out on the West Coas...</td>\n",
       "      <td>I live with eyes wide open, always absorbing s...</td>\n",
       "      <td>5 dagen geleden</td>\n",
       "      <td>23/11/2020</td>\n",
       "      <td>Netwerk24.com</td>\n",
       "      <td>14000000</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>[artist, live, dream, west, coastnetwerk24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.netwerk24.com/ZA/Distrikspos/Nuus/...</td>\n",
       "      <td>Big Apple beckons for CilliersNetwerk24</td>\n",
       "      <td>An aspiring artist from Somerset West will put...</td>\n",
       "      <td>17 uur geleden</td>\n",
       "      <td>23/11/2020</td>\n",
       "      <td>Netwerk24.com</td>\n",
       "      <td>14000000</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>[big, apple, beckon, cilliersnetwerk24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.netwerk24.com/ZA/Distrikspos/Nuus/...</td>\n",
       "      <td>City honours PR councillor Bazier with Alderma...</td>\n",
       "      <td>A local proportional representative (PR) counc...</td>\n",
       "      <td>1 week geleden</td>\n",
       "      <td>23/11/2020</td>\n",
       "      <td>Netwerk24.com</td>\n",
       "      <td>14000000</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>[city, honours, pr, councillor, bazier, alderm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46434</th>\n",
       "      <td>https://www.liverpoolecho.co.uk/news/liverpool...</td>\n",
       "      <td>Girl, seven, still fighting for life in hospit...</td>\n",
       "      <td>Officers said she was rushed to hospital at th...</td>\n",
       "      <td>3 weken geleden</td>\n",
       "      <td>22/11/2020</td>\n",
       "      <td>liverpoolecho.co.uk</td>\n",
       "      <td>11300000</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "      <td>[girl, fight, life, hospital, maghull, crashli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46435</th>\n",
       "      <td>https://www.liverpoolecho.co.uk/news/liverpool...</td>\n",
       "      <td>Footballer slapped ex fiancee and broke her fi...</td>\n",
       "      <td>NHS advice says if you are at risk of domestic...</td>\n",
       "      <td>1 week geleden</td>\n",
       "      <td>22/11/2020</td>\n",
       "      <td>liverpoolecho.co.uk</td>\n",
       "      <td>11300000</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "      <td>[footballer, slap, ex, fiancee, break, finger,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46436</th>\n",
       "      <td>https://www.liverpoolecho.co.uk/news/liverpool...</td>\n",
       "      <td>Ear biting thug with bloodstained blade confro...</td>\n",
       "      <td>A knifeman fleeing from police confronted a di...</td>\n",
       "      <td>3 weken geleden</td>\n",
       "      <td>22/11/2020</td>\n",
       "      <td>liverpoolecho.co.uk</td>\n",
       "      <td>11300000</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "      <td>[ear, bite, thug, bloodstained, blade, confron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46437</th>\n",
       "      <td>https://www.liverpoolecho.co.uk/news/liverpool...</td>\n",
       "      <td>Young dad with terminal cancer shares heartbre...</td>\n",
       "      <td>He absolutely loves her to the end of the worl...</td>\n",
       "      <td>1 week geleden</td>\n",
       "      <td>22/11/2020</td>\n",
       "      <td>liverpoolecho.co.uk</td>\n",
       "      <td>11300000</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "      <td>[young, dad, terminal, cancer, share, heartbre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46438</th>\n",
       "      <td>https://www.liverpoolecho.co.uk/whats-on/food-...</td>\n",
       "      <td>McDonald's worker shares how to get free food ...</td>\n",
       "      <td>The most recent video has seen a woman share h...</td>\n",
       "      <td>1 uur geleden</td>\n",
       "      <td>22/11/2020</td>\n",
       "      <td>liverpoolecho.co.uk</td>\n",
       "      <td>11300000</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "      <td>[mcdonald, worker, share, free, food, order, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46439 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  \\\n",
       "0      https://www.netwerk24.com/Vermaak/Celebs/waarh...   \n",
       "1      https://www.netwerk24.com/huisgenoot/Bekendes/...   \n",
       "2      https://www.netwerk24.com/ZA/Weslander/an-arti...   \n",
       "3      https://www.netwerk24.com/ZA/Distrikspos/Nuus/...   \n",
       "4      https://www.netwerk24.com/ZA/Distrikspos/Nuus/...   \n",
       "...                                                  ...   \n",
       "46434  https://www.liverpoolecho.co.uk/news/liverpool...   \n",
       "46435  https://www.liverpoolecho.co.uk/news/liverpool...   \n",
       "46436  https://www.liverpoolecho.co.uk/news/liverpool...   \n",
       "46437  https://www.liverpoolecho.co.uk/news/liverpool...   \n",
       "46438  https://www.liverpoolecho.co.uk/whats-on/food-...   \n",
       "\n",
       "                                                headline  \\\n",
       "0      Waarheid moet uit oor Diana-onderhoud, s� Will...   \n",
       "1      KYK: Die ooievaar is doenig onder plaaslike be...   \n",
       "2      An artist lives her dream out on the West Coas...   \n",
       "3                Big Apple beckons for CilliersNetwerk24   \n",
       "4      City honours PR councillor Bazier with Alderma...   \n",
       "...                                                  ...   \n",
       "46434  Girl, seven, still fighting for life in hospit...   \n",
       "46435  Footballer slapped ex fiancee and broke her fi...   \n",
       "46436  Ear biting thug with bloodstained blade confro...   \n",
       "46437  Young dad with terminal cancer shares heartbre...   \n",
       "46438  McDonald's worker shares how to get free food ...   \n",
       "\n",
       "                                                subtitle             time  \\\n",
       "0                        WilliamNetwerk244 dagen geleden  4 dagen geleden   \n",
       "1      Hier is van die oulikste foto's wat plaaslike ...   1 week geleden   \n",
       "2      I live with eyes wide open, always absorbing s...  5 dagen geleden   \n",
       "3      An aspiring artist from Somerset West will put...   17 uur geleden   \n",
       "4      A local proportional representative (PR) counc...   1 week geleden   \n",
       "...                                                  ...              ...   \n",
       "46434  Officers said she was rushed to hospital at th...  3 weken geleden   \n",
       "46435  NHS advice says if you are at risk of domestic...   1 week geleden   \n",
       "46436  A knifeman fleeing from police confronted a di...  3 weken geleden   \n",
       "46437  He absolutely loves her to the end of the worl...   1 week geleden   \n",
       "46438  The most recent video has seen a woman share h...    1 uur geleden   \n",
       "\n",
       "      scrape_date                 site  monthly_visits       country  \\\n",
       "0      23/11/2020        Netwerk24.com        14000000  South Africa   \n",
       "1      23/11/2020        Netwerk24.com        14000000  South Africa   \n",
       "2      23/11/2020        Netwerk24.com        14000000  South Africa   \n",
       "3      23/11/2020        Netwerk24.com        14000000  South Africa   \n",
       "4      23/11/2020        Netwerk24.com        14000000  South Africa   \n",
       "...           ...                  ...             ...           ...   \n",
       "46434  22/11/2020  liverpoolecho.co.uk        11300000            UK   \n",
       "46435  22/11/2020  liverpoolecho.co.uk        11300000            UK   \n",
       "46436  22/11/2020  liverpoolecho.co.uk        11300000            UK   \n",
       "46437  22/11/2020  liverpoolecho.co.uk        11300000            UK   \n",
       "46438  22/11/2020  liverpoolecho.co.uk        11300000            UK   \n",
       "\n",
       "      country_of_pub                                     clean_headline  \n",
       "0       South Africa  [waarheid, moet, uit, oor, diana, onderhoud, s...  \n",
       "1       South Africa  [kyk, die, ooievaar, doenig, onder, plaaslike,...  \n",
       "2       South Africa        [artist, live, dream, west, coastnetwerk24]  \n",
       "3       South Africa            [big, apple, beckon, cilliersnetwerk24]  \n",
       "4       South Africa  [city, honours, pr, councillor, bazier, alderm...  \n",
       "...              ...                                                ...  \n",
       "46434             UK  [girl, fight, life, hospital, maghull, crashli...  \n",
       "46435             UK  [footballer, slap, ex, fiancee, break, finger,...  \n",
       "46436             UK  [ear, bite, thug, bloodstained, blade, confron...  \n",
       "46437             UK  [young, dad, terminal, cancer, share, heartbre...  \n",
       "46438             UK  [mcdonald, worker, share, free, food, order, w...  \n",
       "\n",
       "[46439 rows x 10 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leonardo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['clean_headline'] = df['headline'].apply(clean_tweets).apply(vec_tweets)\n",
    "df['subtitle'][df['subtitle'].isna() == True] = \"\"\n",
    "df['clean_subtitle'] = df['subtitle'].apply(clean_tweets).apply(vec_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate time to english\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "df['clean_subtitle'] = translator.translate('9 ore fa', src='it', dest='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9 hours ago'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#importing and initialising the VADER analyser\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Storing the scores in list of dictionaries\n",
    "scores = []\n",
    "# Declare variables for scores\n",
    "compound_list = []\n",
    "positive_list = []\n",
    "negative_list = []\n",
    "neutral_list = []\n",
    "for i in range(tweets_df['clean_text'].shape[0]):\n",
    "#print(analyser.polarity_scores(sentiments_pd['Tweet'][i]))\n",
    "    compound = analyzer.polarity_scores(tweets_df['clean_text'][i])[\"compound\"]\n",
    "    pos = analyzer.polarity_scores(tweets_df['clean_text'][i])[\"pos\"]\n",
    "    neu = analyzer.polarity_scores(tweets_df['clean_text'][i])[\"neu\"]\n",
    "    neg = analyzer.polarity_scores(tweets_df['clean_text'][i])[\"neg\"]\n",
    "    \n",
    "    scores.append({\"compound_s\": compound,\n",
    "                       \"positive_s\": pos,\n",
    "                       \"negative_s\": neg,\n",
    "                       \"neutral_s\": neu\n",
    "                  })\n",
    "\n",
    "#Appending the scores into the dataframe for further analysis \n",
    "sentiments_score = pd.DataFrame.from_dict(scores)\n",
    "tweets_df_sent = tweets_df.join(sentiments_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "plt.hist(tweets_df_sent['compound_s'].values, bins=40)\n",
    "plt.title('Histogram of Compound Sentiment')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.xlabel('Compound Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
