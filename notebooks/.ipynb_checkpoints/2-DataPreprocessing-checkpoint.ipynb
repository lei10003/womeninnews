{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all files \n",
    "files = sorted(glob.glob(\"../data/raw/*_articles.csv\"))\n",
    "\n",
    "file_names = []\n",
    "\n",
    "for file_path in files:\n",
    "    file_name = re.split(\"_\", os.path.basename(file_path))[1].split('.')[0]\n",
    "    \n",
    "    hd = pd.read_csv(file_path)\n",
    "    file_names.append(hd)\n",
    "    \n",
    "df = pd.concat(file_names).reset_index().drop([\"Unnamed: 0\", \"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata file\n",
    "newsrooms = pd.read_csv('../data/raw/newsrooms.csv')\n",
    "newsrooms.columns = ['site', 'monthly_visits', 'country', 'country_of_pub']\n",
    "newsrooms.dropna(inplace=True)\n",
    "newsrooms = newsrooms.drop_duplicates(subset = 'site')\n",
    "newsrooms = newsrooms.sort_values(by=['country_of_pub', 'monthly_visits'], ascending=False).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, newsrooms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has 7814 headlines from South Africa 5039 headlines from India 11210 headlines from the UK and 22376 headlines from the U.S.\n"
     ]
    }
   ],
   "source": [
    "print(\"This dataset has\", len(df[df['country_of_pub']=='South Africa']), \"headlines from South Africa\", len(df[df['country_of_pub']=='India']), \"headlines from India\",\n",
    "      len(df[df['country_of_pub']=='UK']), \"headlines from the UK and\", len(df[df['country_of_pub']=='USA']), \"headlines from the U.S.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset covers 29 news outlets from South Africa 18 news outlets from India 43 news outlets from the UK and 90 news outlets from the U.S.\n"
     ]
    }
   ],
   "source": [
    "print(\"This dataset covers\", len(df.site[df['country_of_pub']=='South Africa'].unique()), \"news outlets from South Africa\", len(df.site[df['country_of_pub']=='India'].unique()), \n",
    "      \"news outlets from India\", len(df.site[df['country_of_pub']=='UK'].unique()), \"news outlets from the UK and\", len(df.site[df['country_of_pub']=='USA'].unique()), \"news outlets from the U.S.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoticons_happy = set([\n",
    "#     ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "#     ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "#     '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "#     'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "#     '<3'\n",
    "#     ])\n",
    "\n",
    "# # Sad Emoticons\n",
    "# emoticons_sad = set([\n",
    "#     ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "#     ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "#     ':c', ':{', '>:\\\\', ';('\n",
    "#     ])\n",
    "\n",
    "# #Emoji patterns\n",
    "# emoji_pattern = re.compile(\"[\"\n",
    "#          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#          u\"\\U00002702-\\U000027B0\"\n",
    "#          u\"\\U000024C2-\\U0001F251\"\n",
    "#          \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# #combine sad and happy emoticons\n",
    "# emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_tweets(tweet):\n",
    " \n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     word_tokens = word_tokenize(tweet)\n",
    "#     #after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
    "#     tweet = re.sub(r':', '', tweet)\n",
    "#     tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "#     tweet = tweet.replace(\"'s\", '')\n",
    "#     #replace consecutive non-ASCII characters with a space\n",
    "#     tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "#     #remove emojis from tweet\n",
    "#     tweet = emoji_pattern.sub(r'', tweet)\n",
    "#     tweet = tweet.lower()\n",
    "#     #filter using NLTK library append it to a string\n",
    "#     filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "#     filtered_tweet = []\n",
    "#     #looping through conditions\n",
    "#     for w in word_tokens:\n",
    "#     #check tokens against stop words , emoticons and punctuations\n",
    "#         if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "#             filtered_tweet.append(w)\n",
    "#     return ' '.join(filtered_tweet)\n",
    "#     #print(word_tokens)\n",
    "#     #print(filtered_sentence)return tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# def remove_pattern(input_txt, pattern):\n",
    "#     r = re.findall(pattern, input_txt)\n",
    "#     for i in r:\n",
    "#         input_txt = re.sub(i, '', input_txt)        \n",
    "#     return input_txt\n",
    "# def vec_tweets(tweets):\n",
    "#     #remove twitter Return handles (RT @xxx:)\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"RT @[\\w]*:\") \n",
    "    \n",
    "#     #remove twitter handles (@xxx)\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"@[\\w]*\")\n",
    "    \n",
    "#     #remove URL links (httpxxx)\n",
    "#     tweets = np.vectorize(remove_pattern)(tweets, \"https?://[A-Za-z0-9./]*\")\n",
    "\n",
    "#     #remove special characters, numbers, punctuations (except for #)\n",
    "#     tweets = np.core.defchararray.replace(tweets, \"[^a-zA-Z]\", \" \")\n",
    "    \n",
    "#     return tweets\n",
    "\n",
    "# # tweets_df['clean_text']= clean_tweets(tweets_df['clean_text'].astype('str')) #The function clean_tweets were put to use.\n",
    "# # tweets_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Waarheid moet uit oor ', 'Diana-onderhoud, s� ', 'William', 'Netwerk24']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z][^A-Z]*', df['headline'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [Waarheid, moet, uit, oor, Diana-onderhoud,, s...\n",
       "1        [KYK:, Die, ooievaar, is, doenig, onder, plaas...\n",
       "2        [An, artist, lives, her, dream, out, on, the, ...\n",
       "3            [Big, Apple, beckons, for, CilliersNetwerk24]\n",
       "4        [City, honours, PR, councillor, Bazier, with, ...\n",
       "                               ...                        \n",
       "46434    [Girl,, seven,, still, fighting, for, life, in...\n",
       "46435    [Footballer, slapped, ex, fiancee, and, broke,...\n",
       "46436    [Ear, biting, thug, with, bloodstained, blade,...\n",
       "46437    [Young, dad, with, terminal, cancer, shares, h...\n",
       "46438    [McDonald's, worker, shares, how, to, get, fre...\n",
       "Name: headline, Length: 46439, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['headline'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>time</th>\n",
       "      <th>scrape_date</th>\n",
       "      <th>site</th>\n",
       "      <th>monthly_visits</th>\n",
       "      <th>country</th>\n",
       "      <th>country_of_pub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22373</th>\n",
       "      <td>https://www.capetownetc.com/news/table-view-ma...</td>\n",
       "      <td>Table View man's body discovered in girlfriend...</td>\n",
       "      <td>A loud scream was heard along Blaauwberg Road ...</td>\n",
       "      <td>6 mesi fa</td>\n",
       "      <td>24/11/2020</td>\n",
       "      <td>Capetownetc.com</td>\n",
       "      <td>5000000</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>South Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39859</th>\n",
       "      <td>https://www.telegraphindia.com/entertainment/t...</td>\n",
       "      <td>‘This film may be about Shakuntala Devi, but A...</td>\n",
       "      <td>That lady lived on her own terms, at that time...</td>\n",
       "      <td>3 mesi fa</td>\n",
       "      <td>22/11/2020</td>\n",
       "      <td>telegraphindia.com</td>\n",
       "      <td>1933000</td>\n",
       "      <td>India</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33830</th>\n",
       "      <td>https://mg.co.za/article/2020-01-23-gender-bas...</td>\n",
       "      <td>Gender-based violence is an affront to our hum...</td>\n",
       "      <td>The year 2019 was difficult one for South Afri...</td>\n",
       "      <td>10 mesi fa</td>\n",
       "      <td>24/11/2020</td>\n",
       "      <td>mg.co.za</td>\n",
       "      <td>10000000</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>South Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32694</th>\n",
       "      <td>https://www.ksl.com/article/46743405/woman-in-...</td>\n",
       "      <td>Woman in Utah accused of delivering drug money...</td>\n",
       "      <td>During a search of her car, troopers found fou...</td>\n",
       "      <td>6 mesi fa</td>\n",
       "      <td>19/11/2020</td>\n",
       "      <td>ksl.com</td>\n",
       "      <td>153646000</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16822</th>\n",
       "      <td>http://www.theladbible.com/entertainment/celeb...</td>\n",
       "      <td>Everyone Has Gone Mad About Taylor Swift's Pos...</td>\n",
       "      <td>Of course, the chances of her actually getting...</td>\n",
       "      <td>28 mesi fa</td>\n",
       "      <td>22/11/2020</td>\n",
       "      <td>theladbible.com</td>\n",
       "      <td>10200000</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39438</th>\n",
       "      <td>https://www.telegraph.co.uk/news/2020/02/06/wo...</td>\n",
       "      <td>Role models are not enough to get more girls i...</td>\n",
       "      <td>The organisation found that only two female sc...</td>\n",
       "      <td>9 maanden geleden</td>\n",
       "      <td>21/11/2020</td>\n",
       "      <td>telegraph.co.uk</td>\n",
       "      <td>169500000</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31027</th>\n",
       "      <td>https://www.ibtimes.co.uk/56-year-old-woman-ki...</td>\n",
       "      <td>56-year-old woman kills friend by pushing her ...</td>\n",
       "      <td>200 debtInternational Business Times UK2 maand...</td>\n",
       "      <td>2 maanden geleden</td>\n",
       "      <td>21/11/2020</td>\n",
       "      <td>ibtimes.co.uk</td>\n",
       "      <td>50200000</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9604</th>\n",
       "      <td>https://www.independent.co.uk/life-style/healt...</td>\n",
       "      <td>Number of women under 30 having children hits ...</td>\n",
       "      <td>The number of women having children under the ...</td>\n",
       "      <td>3 maanden geleden</td>\n",
       "      <td>21/11/2020</td>\n",
       "      <td>independent.co.uk</td>\n",
       "      <td>93900000</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21305</th>\n",
       "      <td>https://www.birminghammail.co.uk/news/showbiz-...</td>\n",
       "      <td>I'm A Celeb Ruthie's shock details about 's***...</td>\n",
       "      <td>Socialite Lady Colin Campbell has written seve...</td>\n",
       "      <td>1 giorno fa</td>\n",
       "      <td>22/11/2020</td>\n",
       "      <td>birminghammail.co.uk</td>\n",
       "      <td>8100000</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38928</th>\n",
       "      <td>https://www.skysports.com/netball/news/12415/1...</td>\n",
       "      <td>Vitality Nations Cup: England's Roses fall sho...</td>\n",
       "      <td>The head coach put her faith in the experience...</td>\n",
       "      <td>9 maanden geleden</td>\n",
       "      <td>21/11/2020</td>\n",
       "      <td>skysports.com</td>\n",
       "      <td>65800000</td>\n",
       "      <td>UK</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  \\\n",
       "22373  https://www.capetownetc.com/news/table-view-ma...   \n",
       "39859  https://www.telegraphindia.com/entertainment/t...   \n",
       "33830  https://mg.co.za/article/2020-01-23-gender-bas...   \n",
       "32694  https://www.ksl.com/article/46743405/woman-in-...   \n",
       "16822  http://www.theladbible.com/entertainment/celeb...   \n",
       "39438  https://www.telegraph.co.uk/news/2020/02/06/wo...   \n",
       "31027  https://www.ibtimes.co.uk/56-year-old-woman-ki...   \n",
       "9604   https://www.independent.co.uk/life-style/healt...   \n",
       "21305  https://www.birminghammail.co.uk/news/showbiz-...   \n",
       "38928  https://www.skysports.com/netball/news/12415/1...   \n",
       "\n",
       "                                                headline  \\\n",
       "22373  Table View man's body discovered in girlfriend...   \n",
       "39859  ‘This film may be about Shakuntala Devi, but A...   \n",
       "33830  Gender-based violence is an affront to our hum...   \n",
       "32694  Woman in Utah accused of delivering drug money...   \n",
       "16822  Everyone Has Gone Mad About Taylor Swift's Pos...   \n",
       "39438  Role models are not enough to get more girls i...   \n",
       "31027  56-year-old woman kills friend by pushing her ...   \n",
       "9604   Number of women under 30 having children hits ...   \n",
       "21305  I'm A Celeb Ruthie's shock details about 's***...   \n",
       "38928  Vitality Nations Cup: England's Roses fall sho...   \n",
       "\n",
       "                                                subtitle               time  \\\n",
       "22373  A loud scream was heard along Blaauwberg Road ...          6 mesi fa   \n",
       "39859  That lady lived on her own terms, at that time...          3 mesi fa   \n",
       "33830  The year 2019 was difficult one for South Afri...         10 mesi fa   \n",
       "32694  During a search of her car, troopers found fou...          6 mesi fa   \n",
       "16822  Of course, the chances of her actually getting...         28 mesi fa   \n",
       "39438  The organisation found that only two female sc...  9 maanden geleden   \n",
       "31027  200 debtInternational Business Times UK2 maand...  2 maanden geleden   \n",
       "9604   The number of women having children under the ...  3 maanden geleden   \n",
       "21305  Socialite Lady Colin Campbell has written seve...        1 giorno fa   \n",
       "38928  The head coach put her faith in the experience...  9 maanden geleden   \n",
       "\n",
       "      scrape_date                  site  monthly_visits       country  \\\n",
       "22373  24/11/2020       Capetownetc.com         5000000  South Africa   \n",
       "39859  22/11/2020    telegraphindia.com         1933000         India   \n",
       "33830  24/11/2020              mg.co.za        10000000  South Africa   \n",
       "32694  19/11/2020               ksl.com       153646000           USA   \n",
       "16822  22/11/2020       theladbible.com        10200000            UK   \n",
       "39438  21/11/2020       telegraph.co.uk       169500000            UK   \n",
       "31027  21/11/2020         ibtimes.co.uk        50200000            UK   \n",
       "9604   21/11/2020     independent.co.uk        93900000            UK   \n",
       "21305  22/11/2020  birminghammail.co.uk         8100000            UK   \n",
       "38928  21/11/2020         skysports.com        65800000            UK   \n",
       "\n",
       "      country_of_pub  \n",
       "22373   South Africa  \n",
       "39859             UK  \n",
       "33830   South Africa  \n",
       "32694            USA  \n",
       "16822             UK  \n",
       "39438             UK  \n",
       "31027             UK  \n",
       "9604              UK  \n",
       "21305             UK  \n",
       "38928             UK  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Choose model accordingly for contractions function\n",
    "# model = api.load(\"glove-twitter-25\")\n",
    "# # model = api.load(\"glove-twitter-100\")\n",
    "# model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# cont = Contractions(kv_model=model)\n",
    "# cont.load_models()\n",
    "\n",
    "\n",
    "# exclude words from spacy stopwords list\n",
    "deselect_stop_words = ['no', 'not']\n",
    "for w in deselect_stop_words:\n",
    "    nlp.vocab[w].is_stop = False\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    \"\"\"remove html tags from text\"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    \"\"\"remove extra whitespaces from text\"\"\"\n",
    "    text = text.strip()\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"remove accented characters from text, e.g. café\"\"\"\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# def expand_contractions(text):\n",
    "#     \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n",
    "#     text = list(cont.expand_texts([text], precise=True))[0]\n",
    "#     return text\n",
    "\n",
    "\n",
    "def text_preprocessing(text, accented_chars=True, contractions=False, \n",
    "                       convert_num=True, extra_whitespace=True, \n",
    "                       lemmatization=True, lowercase=True, punctuations=True,\n",
    "                       remove_html=True, remove_num=True, special_chars=True, \n",
    "                       stop_words=True):\n",
    "    \"\"\"preprocess text with default option set to true for all steps\"\"\"\n",
    "    if remove_html == True: #remove html tags\n",
    "        text = strip_html_tags(text)\n",
    "    if extra_whitespace == True: #remove extra whitespaces\n",
    "        text = remove_whitespace(text)\n",
    "    if accented_chars == True: #remove accented characters\n",
    "        text = remove_accented_chars(text)\n",
    "    if contractions == True: #expand contractions\n",
    "        text = expand_contractions(text)\n",
    "    if lowercase == True: #convert all characters to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "    doc = nlp(text) #tokenise text\n",
    "\n",
    "    clean_text = []\n",
    "    \n",
    "    for token in doc:\n",
    "        flag = True\n",
    "        edit = token.text\n",
    "        # remove stop words\n",
    "        if stop_words == True and token.is_stop and token.pos_ != 'NUM': \n",
    "            flag = False\n",
    "        # remove punctuations\n",
    "        if punctuations == True and token.pos_ == 'PUNCT' and flag == True: \n",
    "            flag = False\n",
    "        # remove special characters\n",
    "        if special_chars == True and token.pos_ == 'SYM' and flag == True: \n",
    "            flag = False\n",
    "        # remove numbers\n",
    "        if remove_num == True and (token.pos_ == 'NUM' or token.text.isnumeric()) \\\n",
    "        and flag == True:\n",
    "            flag = False\n",
    "        # convert number words to numeric numbers\n",
    "        if convert_num == True and token.pos_ == 'NUM' and flag == True:\n",
    "            edit = w2n.word_to_num(token.text)\n",
    "        # convert tokens to base form\n",
    "        elif lemmatization == True and token.lemma_ != \"-PRON-\" and flag == True:\n",
    "            edit = token.lemma_\n",
    "        # append tokens edited and not removed to list \n",
    "        if edit != \"\" and flag == True:\n",
    "            clean_text.append(edit)        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leonardo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['clean_headline'] = df['headline'].apply(text_preprocessing)\n",
    "df['subtitle'][df['subtitle'].isna() == True] = \"\"\n",
    "df['clean_subtitle'] = df['subtitle'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Netwerk24.com'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cl['site'][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/headlines_cl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl = pd.read_csv('../data/processed/headlines_cl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kim Ng, Baseball’s Groundbreaking Woman'"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ''.join(re.split('(?<=[a-z0-9])[A-Z]', df_cl[df_cl['site'] == \"nytimes.com\"].reset_index().headline[0])[:-1])\n",
    "# # re.split(df_cl[df_cl['site'] == \"nbcsports.com\"].reset_index().headline[10], \"((?<=\\p{Ll})\\p{Lu}|\\p{Lu}(?=\\p{Ll}))\", \" $1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl['headline_no_site'] = df_cl['headline'].apply(lambda x: ''.join(re.split('(?<=[a-z0-9])[A-Z]', x)[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sorry, Gossip Girl Won’t Be On HBO Max Right Away'"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cl['headline_no_site'][10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Millions of women don't know where their own vagina is locatedNew York Post\""
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_cl[df_cl['site'] == \"nypost.com\"].reset_index().headline[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove url lingo from sites\n",
    "# strings = '[<.com>|<.co>|<.uk>|<.in>|<.za>]'\n",
    "# df_cl['clean_site'] = df_cl['site'].apply(lambda x: x.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cl['headline_no_site'] = df_cl['headline'].apply(lambda x: x.strip(df_cl['clean_site']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl['headline_no_site'] = df_cl['headline'].apply(lambda row: row.replace(row['clean_site'],''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove site string from headlines\n",
    "df_cl['headline_no_site'] = df_cl.apply(lambda x: x.headline.strip(x['clean_site']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ture'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cl['clean_site'][4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['bt', 'announce', 'new', 'strategy', 'change', 'livesbt', 'sport']\""
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cl['clean_headline'][5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['waarheid', 'moet', 'uit', 'oor', 'diana', 'onderhoud', 's', 'williamnetwerk24']\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cl['clean_headline'][10].lower().strip(string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'re.Pattern' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-da038bb6fa1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_cl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'headline'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# p can be reused across different input strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# ''.join(df_cl['headline'][0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 're.Pattern' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "# string = \"Netwerk24\"\n",
    "# p = re.compile(''.join(df_cl['headline'][0]))  # p can be reused across different input strings\n",
    "# # ''.join(df_cl['headline'][0])\n",
    "# p.strip(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aarheid moet uit oor diana-onderhoud, s� william'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'werk'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cl['site'][0].strip(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Thoughts\n",
    "#- Need to get rid of all \"women\" words?\n",
    "#- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'Waarheid moet uit oor Diana-onderhoud, s� WilliamNetwerk24',\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.compile(''.join(df_cl['headline'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leonardo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['clean_headline'] = df['headline'].apply(clean_tweets).apply(vec_tweets)\n",
    "df['subtitle'][df['subtitle'].isna() == True] = \"\"\n",
    "df['clean_subtitle'] = df['subtitle'].apply(clean_tweets).apply(vec_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate time to english\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "df['clean_subtitle'] = translator.translate('9 ore fa', src='it', dest='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9 hours ago'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#importing and initialising the VADER analyser\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Storing the scores in list of dictionaries\n",
    "scores = []\n",
    "# Declare variables for scores\n",
    "compound_list = []\n",
    "positive_list = []\n",
    "negative_list = []\n",
    "neutral_list = []\n",
    "for i in range(tweets_df['clean_text'].shape[0]):\n",
    "#print(analyser.polarity_scores(sentiments_pd['Tweet'][i]))\n",
    "    compound = analyzer.polarity_scores(tweets_df['clean_text'][i])[\"compound\"]\n",
    "    pos = analyzer.polarity_scores(tweets_df['clean_text'][i])[\"pos\"]\n",
    "    neu = analyzer.polarity_scores(tweets_df['clean_text'][i])[\"neu\"]\n",
    "    neg = analyzer.polarity_scores(tweets_df['clean_text'][i])[\"neg\"]\n",
    "    \n",
    "    scores.append({\"compound_s\": compound,\n",
    "                       \"positive_s\": pos,\n",
    "                       \"negative_s\": neg,\n",
    "                       \"neutral_s\": neu\n",
    "                  })\n",
    "\n",
    "#Appending the scores into the dataframe for further analysis \n",
    "sentiments_score = pd.DataFrame.from_dict(scores)\n",
    "tweets_df_sent = tweets_df.join(sentiments_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "plt.hist(tweets_df_sent['compound_s'].values, bins=40)\n",
    "plt.title('Histogram of Compound Sentiment')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.xlabel('Compound Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
